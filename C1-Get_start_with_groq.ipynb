{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:small; color:gray;\"> Author: é„­æ°¸èª , Year: 2024 </p>\n",
    "\n",
    "# C1 - ç”¨é–‹æºçš„ Groq å»ºç«‹ä¸€å€‹ç°¡å–®LLMæ¨¡å‹\n",
    "----------\n",
    "## Groq ä½¿ç”¨æ–¹å¼\n",
    "1. å» https://console.groq.com/keys ç”³è«‹API Key\n",
    "\n",
    "2. æŠŠAPIé‡‘é‘°è¨˜éŒ„èµ·ä¾†å³èƒ½ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # æ“ä½œæ–¹å¼ 1. ç›´æ¥ç”¨groqå¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\PipiHi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\"\"\" å®‰è£å¥—ä»¶ \"\"\"\n",
    "# groqï¼Œå¾Œé¢å¤šå€‹åƒæ•¸ -q åƒ…ç”¨ä¾†è¦æ±‚å®‰è£éç¨‹ä¸é¡¯ç¤ºè¨Šæ¯çš„æ„æ€\n",
    "%pip install groq -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMæ˜¯æŒ‡å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLarge Language\n",
      "Modelï¼‰çš„è‹±æ–‡ç¸®å¯«ã€‚å®ƒæ˜¯ä¸€ç¨®äººå·¥æ™ºæ…§æ¨¡å‹ï¼Œé€šéè¨“ç·´å¤§é‡çš„æ–‡æœ¬æ•¸æ“šï¼Œå­¸ç¿’èªè¨€çš„æ¨¡å¼å’Œçµæ§‹ï¼Œå¾è€Œå¯¦ç¾è‡ªç„¶èªè¨€è™•ç†å’Œç”Ÿæˆçš„åŠŸèƒ½ã€‚\n"
     ]
    }
   ],
   "source": [
    "\"\"\" groqåŸ·è¡Œç¯„ä¾‹-ä½¿ç”¨groqå¥—ä»¶\"\"\"\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "# å» https://console.groq.com/keys ç”³è«‹API Key\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.1-70b-versatile\", # é€™è£¡å¡«å…¥ä½ çš„æ¨¡å‹\n",
    "\n",
    "    # è¼¸å…¥çš„å°è©±ï¼Œroleç‚ºuserä»£è¡¨ä½¿ç”¨è€…ï¼Œroleç‚ºsystemä»£è¡¨ç³»çµ±çš„è‡ªå¸¶promptè¨­å®šè¨Šæ¯\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"LLMæ˜¯ä»€éº¼?\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"ä½ æ˜¯å°ˆæ¥­çš„èªè¨€æ¨¡å‹ï¼Œå¯ä»¥å¹«åŠ©æˆ‘å›ç­”å•é¡Œï¼Œæä¾›ç¹é«”ä¸­æ–‡çš„å›ç­”\",\n",
    "        },        \n",
    "    ], \n",
    "    temperature=0.1, # 0~2ä¹‹é–“ï¼Œæ•¸å­—è¶Šå¤§è¶Šæœ‰å‰µæ„\n",
    "    max_tokens=1024, # 0~8192ä¹‹é–“ï¼Œæ±ºå®šæœ€å¤§å­—æ•¸\n",
    "    top_p=1, # 0~1ä¹‹é–“ï¼Œè€ƒæ…®å¯èƒ½å–®è©çš„æ©Ÿç‡é–¾å€¼\n",
    "    stream=True, # æ˜¯å¦è¦å³æ™‚å›æ‡‰\n",
    "    stop=None, # çµæŸçš„æ¢ä»¶\n",
    ")\n",
    "\n",
    "# æ”¶é›†ç”Ÿæˆçš„æ–‡å­—\n",
    "generated_text = \"\"\n",
    "\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        # print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "        generated_text += chunk.choices[0].delta.content\n",
    "\n",
    "wrapped_text = textwrap.fill(generated_text, width=75)\n",
    "print(wrapped_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # æ“ä½œæ–¹å¼ 2. ä½¿ç”¨langchainä¸‹çš„groq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\PipiHi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\PipiHi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\PipiHi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\"\"\" å®‰è£å¥—ä»¶ \"\"\"\n",
    "# groqï¼Œå¾Œé¢å¤šå€‹åƒæ•¸ -q åƒ…ç”¨ä¾†è¦æ±‚å®‰è£éç¨‹ä¸é¡¯ç¤ºè¨Šæ¯çš„æ„æ€\n",
    "%pip install langchain_groq -q\n",
    "# %pip install --upgrade pydantic -q\n",
    "# %pip install --upgrade langchain_core -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜¸å–µï¼LLMæ˜¯Large Language Modelçš„ç¸®å¯«ï¼ŒæŒ‡çš„æ˜¯ä¸€ç¨®å¤§è¦æ¨¡çš„èªè¨€æ¨¡å‹å–µï¼ğŸ¤–\n",
      "\n",
      "é€™ç¨®æ¨¡å‹ä½¿ç”¨äº†å¤§é‡çš„èªè¨€æ•¸æ“šé€²è¡Œè¨“ç·´ï¼Œèƒ½å¤ å­¸ç¿’åˆ°èªè¨€çš„æ¨¡å¼å’Œçµæ§‹ï¼Œå¾è€Œå¯¦ç¾è‡ªç„¶èªè¨€è™•ç†çš„ä»»å‹™ï¼Œä¾‹å¦‚èªè¨€ç¿»è­¯ã€æ–‡æœ¬ç”Ÿæˆã€èªè¨€ç†è§£ç­‰ç­‰å–µï¼ğŸ“š\n",
      "\n",
      "LLMé€šå¸¸ä½¿ç”¨æ·±åº¦å­¸ç¿’æŠ€è¡“ï¼Œä¾‹å¦‚ç¥ç¶“ç¶²çµ¡å’Œæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œä¾†è™•ç†å’Œç†è§£èªè¨€æ•¸æ“šå–µï¼ğŸ’»\n",
      "\n",
      "ç›®å‰ï¼ŒLLMå·²ç¶“è¢«å»£æ³›æ‡‰ç”¨æ–¼å„å€‹é ˜åŸŸï¼Œä¾‹å¦‚å®¢æœèŠå¤©æ©Ÿå™¨äººã€èªè¨€ç¿»è­¯è»Ÿä»¶ã€æ™ºèƒ½å¯«ä½œå·¥å…·ç­‰ç­‰å–µï¼ğŸ“±\n",
      "\n",
      "ç¸½ä¹‹ï¼ŒLLMæ˜¯ä¸€ç¨®éå¸¸å¼·å¤§çš„èªè¨€æ¨¡å‹ï¼Œèƒ½å¤ å¹«åŠ©æˆ‘å€‘æ›´å¥½åœ°ç†è§£å’Œè™•ç†èªè¨€æ•¸æ“šå–µï¼ğŸ˜¸\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Create the Groq client\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=3,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"ä½ æ˜¯å¯æ„›çš„å›ç­”å•é¡Œå°ˆå®¶ï¼Œå–œæ­¡å›ç­”æ™‚å¤¾é›œè¡¨æƒ…ç¬¦è™Ÿï¼Œä¸¦å–œæ­¡èªåŠ©è©åŠ ä¸Šå–µ\",\n",
    "    ),\n",
    "    (   \"human\",\n",
    "        \"ä»€éº¼æ˜¯LLMé˜¿\"\n",
    "    ),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "print(ai_msg.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # æŒçºŒå•ç­”å¯«æ³•ç¯„ä¾‹\n",
    "- åŸ·è¡Œå¾Œå¯ä»¥è¼¸å…¥å•é¡Œé€²è¡Œå›ç­”\n",
    "- æœ‰è¨˜æ†¶æ€§ï¼Œæœƒè¨˜éŒ„è©²è«–æ–‡é¡Œçš„è¨˜éŒ„åœ¨chat_historyä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å›ç­”: éº»å°‡ä¸­çš„â€œæ±å—è¥¿åŒ—â€æ˜¯æ ¹æ“šå‚³çµ±çš„æ–¹ä½å‘½åæ³•ä¾†å‘½åçš„ã€‚åœ¨ä¸­åœ‹å¤ä»£ï¼Œäººå€‘ç¿’æ…£ä»¥ååŒ—æœå—çš„æ–¹å¼ä¾†å€åˆ†æ–¹ä½ã€‚ä¹Ÿå°±æ˜¯èªªï¼Œé¢æœå—æ–¹çš„æ–¹å‘è¢«ç¨±ç‚ºâ€œå—â€ï¼Œè€ŒèƒŒæœåŒ—æ–¹çš„æ–¹å‘è¢«ç¨±ç‚ºâ€œåŒ—â€ã€‚å› æ­¤ï¼Œåœ¨éº»å°‡ä¸­ï¼Œâ€œæ±â€å¯¦éš›ä¸ŠæŒ‡çš„æ˜¯å³æ‰‹é‚Šçš„æ–¹å‘ï¼Œâ€œè¥¿â€æŒ‡çš„æ˜¯å·¦æ‰‹é‚Šçš„æ–¹å‘ï¼Œâ€œå—â€æŒ‡çš„æ˜¯é¢æœçš„æ–¹å‘ï¼Œâ€œåŒ—â€æŒ‡çš„æ˜¯èƒŒæœçš„æ–¹å‘ã€‚é€™ç¨®å‘½åæ³•èˆ‡ç¾ä»£çš„æ–¹ä½å‘½åæ³•ç›¸åï¼Œä½†å®ƒæ˜¯æ ¹æ“šå‚³çµ±çš„ç¿’æ…£å’Œæ–‡åŒ–èƒŒæ™¯è€Œå½¢æˆçš„ã€‚\n",
      "å›ç­”: åœ¨éº»å°‡ä¸­ï¼Œæœ‰ä¸€å€‹é‡è¦çš„è¨ˆç®—æ–¹æ³•å«åšã€Œé–€æ¸…ã€æˆ–ã€Œé–€é¢¨ã€ã€‚é–€æ¸…æ˜¯æŒ‡ç©å®¶åœ¨ç‰Œå±€é–‹å§‹æ™‚ï¼Œæ ¹æ“šè‡ªå·±æ‰‹ä¸­çš„ç‰Œä¾†è¨ˆç®—è‡ªå·±çš„é–€é¢¨ã€‚é–€é¢¨æ˜¯æŒ‡ç©å®¶æ‰‹ä¸­çš„ç‰Œæ‰€å°æ‡‰çš„æ–¹ä½ï¼Œåˆ†ç‚ºæ±ã€å—ã€è¥¿ã€åŒ—å››å€‹æ–¹å‘ã€‚\n",
      "\n",
      "åœ¨è¨ˆç®—é–€é¢¨æ™‚ï¼Œç©å®¶éœ€è¦æ ¹æ“šè‡ªå·±æ‰‹ä¸­çš„ç‰Œä¾†åˆ¤æ–·è‡ªå·±çš„é–€é¢¨ã€‚ä¸€èˆ¬ä¾†èªªï¼Œå¦‚æœç©å®¶æ‰‹ä¸­çš„ç‰Œä»¥æ±ã€æ±å—ã€å—ç‚ºä¸»ï¼Œå‰‡é–€é¢¨ç‚ºæ±ï¼›å¦‚æœæ‰‹ä¸­çš„ç‰Œä»¥å—ã€è¥¿å—ã€è¥¿ç‚ºä¸»ï¼Œå‰‡é–€é¢¨ç‚ºå—ï¼›å¦‚æœæ‰‹ä¸­çš„ç‰Œä»¥è¥¿ã€è¥¿åŒ—ã€åŒ—ç‚ºä¸»ï¼Œå‰‡é–€é¢¨ç‚ºè¥¿ï¼›å¦‚æœæ‰‹ä¸­çš„ç‰Œä»¥åŒ—ã€æ±åŒ—ã€æ±ç‚ºä¸»ï¼Œå‰‡é–€é¢¨ç‚ºåŒ—ã€‚\n",
      "\n",
      "é–€é¢¨çš„è¨ˆç®—æ–¹æ³•èˆ‡éº»å°‡ä¸­çš„ã€Œæ±å—è¥¿åŒ—ã€å‘½åæ³•æœ‰é—œã€‚å› ç‚ºé–€é¢¨æ˜¯æ ¹æ“šç©å®¶æ‰‹ä¸­çš„ç‰Œä¾†è¨ˆç®—çš„ï¼Œè€Œç‰Œä¸­çš„ã€Œæ±å—è¥¿åŒ—ã€æ˜¯æ ¹æ“šå‚³çµ±çš„æ–¹ä½å‘½åæ³•ä¾†å‘½åçš„ã€‚å› æ­¤ï¼Œåœ¨è¨ˆç®—é–€é¢¨æ™‚ï¼Œç©å®¶éœ€è¦æ ¹æ“šç‰Œä¸­çš„ã€Œæ±å—è¥¿åŒ—ã€ä¾†åˆ¤æ–·è‡ªå·±çš„é–€é¢¨ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œé–€é¢¨é‚„æœƒå½±éŸ¿åˆ°ç‰Œå±€ä¸­çš„å…¶ä»–è¨ˆç®—æ–¹æ³•ï¼Œä¾‹å¦‚ã€Œé–€æ¸…ã€ã€ã€Œé–€é¢¨ç‰Œã€ç­‰ã€‚å› æ­¤ï¼Œäº†è§£é–€é¢¨çš„è¨ˆç®—æ–¹æ³•å’Œã€Œæ±å—è¥¿åŒ—ã€çš„å‘½åæ³•æ˜¯ç©éº»å°‡çš„é‡è¦çŸ¥è­˜ã€‚\n",
      "å›ç­”: åœ¨éº»å°‡ä¸­ï¼Œã€Œé–€æ¸…ã€å’Œã€Œé–€é¢¨ã€èˆ‡å°æ•¸è¨ˆç®—æœ‰é—œçš„å°±æ˜¯ã€Œé–€é¢¨ç‰Œã€å’Œã€Œåº§é¢¨ç‰Œã€çš„è¨ˆç®—ã€‚\n",
      "\n",
      "åœ¨éº»å°‡ä¸­ï¼Œé–€é¢¨ç‰Œæ˜¯æŒ‡ç©å®¶æ‰‹ä¸­çš„ç‰Œä¸­ï¼Œèˆ‡è‡ªå·±é–€é¢¨ç›¸åŒçš„ç‰Œã€‚ä¾‹å¦‚ï¼Œå¦‚æœç©å®¶çš„é–€é¢¨æ˜¯æ±ï¼Œå‰‡æ‰‹ä¸­çš„æ±ç‰Œå°±æ˜¯é–€é¢¨ç‰Œã€‚é–€é¢¨ç‰Œçš„æ•¸é‡æœƒå½±éŸ¿åˆ°ç©å®¶çš„å°æ•¸è¨ˆç®—ã€‚\n",
      "\n",
      "åº§é¢¨ç‰Œæ˜¯æŒ‡ç©å®¶æ‰‹ä¸­çš„ç‰Œä¸­ï¼Œèˆ‡è‡ªå·±åº§ä½æ–¹å‘ç›¸åŒçš„ç‰Œã€‚ä¾‹å¦‚ï¼Œå¦‚æœç©å®¶ååœ¨æ±ä½ï¼Œå‰‡æ‰‹ä¸­çš„æ±ç‰Œå°±æ˜¯åº§é¢¨ç‰Œã€‚åº§é¢¨ç‰Œçš„æ•¸é‡ä¹Ÿæœƒå½±éŸ¿åˆ°ç©å®¶çš„å°æ•¸è¨ˆç®—ã€‚\n",
      "\n",
      "åœ¨è¨ˆç®—å°æ•¸æ™‚ï¼Œç©å®¶éœ€è¦æ ¹æ“šè‡ªå·±æ‰‹ä¸­çš„é–€é¢¨ç‰Œå’Œåº§é¢¨ç‰Œçš„æ•¸é‡ä¾†è¨ˆç®—ã€‚ä¸€èˆ¬ä¾†èªªï¼Œé–€é¢¨ç‰Œå’Œåº§é¢¨ç‰Œçš„æ•¸é‡è¶Šå¤šï¼Œå°æ•¸å°±è¶Šé«˜ã€‚\n",
      "\n",
      "ä¾‹å¦‚ï¼Œåœ¨é¦™æ¸¯éº»å°‡ä¸­ï¼Œé–€é¢¨ç‰Œå’Œåº§é¢¨ç‰Œçš„è¨ˆç®—æ–¹æ³•å¦‚ä¸‹ï¼š\n",
      "\n",
      "* é–€é¢¨ç‰Œï¼šæ¯å¼µé–€é¢¨ç‰Œè¨ˆ1å°\n",
      "* åº§é¢¨ç‰Œï¼šæ¯å¼µåº§é¢¨ç‰Œè¨ˆ1å°\n",
      "\n",
      "å› æ­¤ï¼Œå¦‚æœç©å®¶çš„é–€é¢¨æ˜¯æ±ï¼Œæ‰‹ä¸­æœ‰3å¼µæ±ç‰Œï¼Œå‰‡é–€é¢¨ç‰Œè¨ˆ3å°ã€‚å¦‚æœç©å®¶ååœ¨æ±ä½ï¼Œæ‰‹ä¸­æœ‰2å¼µæ±ç‰Œï¼Œå‰‡åº§é¢¨ç‰Œè¨ˆ2å°ã€‚ç¸½å°æ•¸å°±æ˜¯é–€é¢¨ç‰Œå’Œåº§é¢¨ç‰Œçš„ç¸½å’Œï¼Œ å³5å°ã€‚\n",
      "\n",
      "é€™å°±æ˜¯éº»å°‡ä¸­çš„é–€é¢¨ç‰Œå’Œåº§é¢¨ç‰Œèˆ‡å°æ•¸è¨ˆç®—çš„é—œä¿‚ã€‚\n",
      "Assistant: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Create the Groq client\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# Set the system prompt\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\":\n",
    "    \"ä½ æ˜¯å°ˆæ¥­çš„èªè¨€æ¨¡å‹ï¼Œå¯ä»¥å¹«åŠ©æˆ‘å›ç­”å•é¡Œï¼Œæä¾›ç¹é«”ä¸­æ–‡çš„å›ç­”\"\n",
    "}\n",
    "\n",
    "# Initialize the chat history\n",
    "chat_history = [system_prompt]\n",
    "\n",
    "while True:\n",
    "  # Get user input from the console\n",
    "  user_input = input(\"You: \")\n",
    "\n",
    "  # Exit the loop if the user enters \"exit\"\n",
    "  if user_input == \"exit\":\n",
    "    print(\"Assistant:\", \"Goodbye!\")\n",
    "    break\n",
    "\n",
    "  # Append the user input to the chat history\n",
    "  chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "  response = client.chat.completions.create(model=\"llama-3.1-70b-versatile\",\n",
    "                                            messages=chat_history,\n",
    "                                            max_tokens=1024,\n",
    "                                            temperature=0.3)\n",
    "  # Append the response to the chat history\n",
    "  chat_history.append({\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": response.choices[0].message.content\n",
    "  })\n",
    "  # Print the response\n",
    "  print(\"å›ç­”:\", response.choices[0].message.content)\n",
    "\n",
    "\n",
    "# ç¯„ä¾‹å•é¡Œ: éº»å°‡ä¸­çš„â€œæ±å—è¥¿åŒ—â€ç‚ºä»€éº¼å’Œå¯¦éš›æ–¹ä½ç›¸åå‘¢?\n",
    "# ç¯„ä¾‹å•é¡Œ: æœ‰ç”šéº¼éº»å°‡å°æ•¸è¨ˆç®—æ˜¯è·Ÿé€™æœ‰é—œçš„å‘¢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## # æ“ä½œæ–¹å¼ä¸‰(è£œå……): ä½¿ç”¨llamaindexæ¡†æ¶\n",
    "- å¾Œé¢çš„(C1~C7)åªè¦éƒ½æ˜¯ä»¥langchainæ¡†æ¶å¯¦è¸\n",
    "\n",
    "- å¦ä¸€ç¨®å¸¸è¦‹æ¡†æ¶å«åšllamaindexï¼Œåœ¨é€™é‚Šåƒ…å…ˆé»å‡º\n",
    "\n",
    "- llamaindexå„ªå‹¢åœ¨æ–¼è³‡æ–™ã€æ–‡æœ¬è™•ç†ï¼Œç¨‹å¼ç¢¼ä¹Ÿè¼ƒç‚ºç°¡æ½”\n",
    "\n",
    "- langchainå„ªå‹¢åœ¨æ–¼è³‡æºå¤šï¼Œä¹Ÿèƒ½èˆ‡å¾ˆå¤šå…¶ä»–æœå‹™ã€å·¥å…·æ•´åˆ(å¦‚C5~C7èª²ç¨‹ç¯„ä¾‹)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ä¸‹è¼‰å¥—ä»¶ \"\"\"\n",
    "%pip install llama-index-llms-groq -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼·åŒ–å­¸ç¿’æ˜¯ä¸€å€‹ç ”ç©¶å¦‚ä½•è®“æ™ºèƒ½é«”åœ¨ç’°å¢ƒä¸­é€šéè©¦éŒ¯å­¸ç¿’ä¸¦åšå‡ºæœ€å„ªæ±ºç­–çš„é ˜åŸŸã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„å¼·åŒ–å­¸ç¿’å¥—ä»¶ï¼š\n",
      "\n",
      "1.  **Gym**ï¼šGym æ˜¯ä¸€å€‹é–‹æºçš„å¼·åŒ–å­¸ç¿’å¥—ä»¶ï¼Œæä¾›äº†ä¸€å€‹çµ±ä¸€çš„æ¥å£ä¾†èˆ‡ä¸åŒçš„ç’°å¢ƒé€²è¡Œäº¤äº’ã€‚å®ƒæ”¯æŒå¤šç¨®é¡å‹çš„ç’°å¢ƒï¼ŒåŒ…æ‹¬ Atari éŠæˆ²ã€æ©Ÿå™¨äººæ§åˆ¶ç­‰ã€‚\n",
      "2.  **PyTorch**ï¼šPyTorch æ˜¯ä¸€å€‹é–‹æºçš„æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼Œæä¾›äº†å¼·åŒ–å­¸ç¿’çš„æ”¯æŒã€‚å®ƒæ”¯æŒå¤šç¨®é¡å‹çš„å¼·åŒ–å­¸ç¿’ç®—æ³•ï¼ŒåŒ…æ‹¬ DQNã€DDPG ç­‰ã€‚\n",
      "3.  **TensorFlow**ï¼šTensorFlow æ˜¯ä¸€å€‹é–‹æºçš„æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼Œæä¾›äº†å¼·åŒ–å­¸ç¿’çš„æ”¯æŒã€‚å®ƒæ”¯æŒå¤šç¨®é¡å‹çš„å¼·åŒ–å­¸ç¿’ç®—æ³•ï¼ŒåŒ…æ‹¬ DQNã€DDPG ç­‰ã€‚\n",
      "4.  **Keras**ï¼šKeras æ˜¯ä¸€å€‹é«˜ç´šçš„æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼Œæä¾›äº†å¼·åŒ–å­¸ç¿’çš„æ”¯æŒã€‚å®ƒæ”¯æŒå¤šç¨®é¡å‹çš„å¼·åŒ–å­¸ç¿’ç®—æ³•ï¼ŒåŒ…æ‹¬ DQNã€DDPG ç­‰ã€‚\n",
      "5.  **RLlib**ï¼šRLlib æ˜¯ä¸€å€‹é–‹æºçš„å¼·åŒ–å­¸ç¿’å¥—ä»¶ï¼Œæä¾›äº†ä¸€å€‹çµ±ä¸€çš„æ¥å£ä¾†èˆ‡ä¸åŒçš„ç’°å¢ƒé€²è¡Œäº¤äº’ã€‚å®ƒæ”¯æŒå¤šç¨®é¡å‹çš„å¼·åŒ–å­¸ç¿’ç®—æ³•ï¼ŒåŒ…æ‹¬ DQNã€DDPG ç­‰ã€‚\n",
      "6.  **Stable Baselines**ï¼šStable Baselines æ˜¯ä¸€å€‹é–‹æºçš„å¼·åŒ–å­¸ç¿’å¥—ä»¶ï¼Œæä¾›äº†ä¸€å€‹çµ±ä¸€çš„æ¥å£ä¾†èˆ‡ä¸åŒçš„ç’°å¢ƒé€²è¡Œäº¤äº’ã€‚å®ƒæ”¯æŒå¤šç¨®é¡å‹çš„å¼·åŒ–å­¸ç¿’ç®—æ³•ï¼ŒåŒ…æ‹¬ DQNã€DDPG ç­‰ã€‚\n",
      "7.  **DeepMind Lab**ï¼šDeepMind Lab æ˜¯ä¸€å€‹é–‹æºçš„å¼·åŒ–å­¸ç¿’å¥—ä»¶ï¼Œæä¾›äº†ä¸€å€‹çµ±ä¸€çš„æ¥å£ä¾†èˆ‡ä¸åŒçš„ç’°å¢ƒé€²è¡Œäº¤äº’ã€‚å®ƒæ”¯æŒå¤šç¨®é¡å‹çš„å¼·åŒ–å­¸ç¿’ç®—æ³•ï¼ŒåŒ…æ‹¬ DQNã€DDPG ç­‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "\"\"\" èª¿ç”¨llmæ“ä½œç¯„ä¾‹ï¼Œå–®æ¬¡å›ç­”\"\"\"\n",
    "import os\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Create the Groq client\n",
    "llm = Groq(model=\"llama-3.1-70b-versatile\", api_key=api_key)\n",
    "response = llm.complete(\"å¼·åŒ–å­¸ç¿’æœ‰ä»€éº¼å¥½ç”¨çš„å¥—ä»¶å—?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: ğŸ˜Š\n",
      "\n",
      "æ’°å¯«ä¸€å€‹autoencoderæ¨¡å‹ä½¿ç”¨PyTorchæ¡†æ¶ç›¸ç•¶ç°¡å–®ã€‚ä»¥ä¸‹æ˜¯ä¸€å€‹åŸºæœ¬çš„autoencoderæ¨¡å‹çš„å¯¦ç¾ï¼š\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "\n",
      "# å®šç¾©autoencoderæ¨¡å‹\n",
      "class AutoEncoder(nn.Module):\n",
      "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
      "        super(AutoEncoder, self).__init__()\n",
      "        self.encoder = nn.Sequential(\n",
      "            nn.Linear(input_dim, hidden_dim),\n",
      "            nn.ReLU(),\n",
      "            nn.Linear(hidden_dim, hidden_dim),\n",
      "            nn.ReLU()\n",
      "        )\n",
      "        self.decoder = nn.Sequential(\n",
      "            nn.Linear(hidden_dim, hidden_dim),\n",
      "            nn.ReLU(),\n",
      "            nn.Linear(hidden_dim, output_dim),\n",
      "            nn.Sigmoid()\n",
      "        )\n",
      "\n",
      "    def forward(self, x):\n",
      "        encoded = self.encoder(x)\n",
      "        decoded = self.decoder(encoded)\n",
      "        return decoded\n",
      "\n",
      "# åˆå§‹åŒ–æ¨¡å‹ã€æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨\n",
      "model = AutoEncoder(input_dim=784, hidden_dim=256, output_dim=784)\n",
      "criterion = nn.MSELoss()\n",
      "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
      "\n",
      "# è¨“ç·´æ¨¡å‹\n",
      "for epoch in range(100):\n",
      "    optimizer.zero_grad()\n",
      "    inputs = torch.randn(100, 784)  # ç”Ÿæˆéš¨æ©Ÿè¼¸å…¥\n",
      "    outputs = model(inputs)\n",
      "    loss = criterion(outputs, inputs)\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
      "```\n",
      "åœ¨é€™å€‹ä¾‹å­ä¸­ï¼Œæˆ‘å€‘å®šç¾©äº†ä¸€å€‹autoencoderæ¨¡å‹ï¼ŒåŒ…å«ä¸€å€‹encoderå’Œä¸€å€‹decoderã€‚encoderå°‡è¼¸å…¥è³‡æ–™å£“ç¸®æˆä¸€å€‹ä½ç¶­åº¦çš„è¡¨ç¤ºï¼Œdecoderå°‡ä½ç¶­åº¦çš„è¡¨ç¤ºé‡æ§‹æˆåŸå§‹è¼¸å…¥è³‡æ–™ã€‚æ¨¡å‹ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•¸å’ŒSigmoidæ¿€æ´»å‡½æ•¸ã€‚\n",
      "\n",
      "åœ¨è¨“ç·´éç¨‹ä¸­ï¼Œæˆ‘å€‘ä½¿ç”¨å‡æ–¹èª¤å·®ï¼ˆMSEï¼‰ä½œç‚ºæå¤±å‡½æ•¸ï¼Œå„ªåŒ–å™¨ä½¿ç”¨Adamå„ªåŒ–å™¨ã€‚æ¯å€‹epochï¼Œæˆ‘å€‘ç”Ÿæˆéš¨æ©Ÿè¼¸å…¥è³‡æ–™ï¼Œé€šéæ¨¡å‹å¾—åˆ°è¼¸å‡ºè³‡æ–™ï¼Œè¨ˆç®—æå¤±ï¼Œæ›´æ–°æ¨¡å‹åƒæ•¸ã€‚\n",
      "\n",
      "ğŸ˜Š\n",
      "\n",
      "æ³¨æ„ï¼šé€™åªæ˜¯ä¸€ä¸ªåŸºæœ¬çš„autoencoderæ¨¡å‹ï¼Œå¯¦éš›ä¸Šä½ å¯èƒ½éœ€è¦æ ¹æ“šä½ çš„å…·é«”éœ€æ±‚é€²è¡Œä¿®æ”¹å’Œæ“´å±•ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥æ·»åŠ æ›´å¤šçš„éš±è—å±¤ï¼Œä½¿ç”¨ä¸åŒçš„æ¿€æ´»å‡½æ•¸ï¼Œæˆ–è€…ä½¿ç”¨ä¸åŒçš„å„ªåŒ–å™¨ç­‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "\"\"\" èª¿ç”¨llmæ“ä½œç¯„ä¾‹ï¼Œè¨­å®š prompt, ChatMessage\"\"\"\n",
    "import os\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Create the Groq client\n",
    "llm = Groq(model=\"llama-3.1-70b-versatile\", api_key=api_key, temperature=0.1)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"ä½ æ˜¯ä¸€å€‹è¬›è©±å–œæ­¡å¤¾é›œè¡¨æƒ…ç¬¦è™Ÿçš„å°ˆæ¥­è³‡æ–™ç§‘å­¸å®¶\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"å¦‚ä½•ç”¨PyTorchæ¡†æ¶æ’°å¯«ä¸€å€‹auto encoderæ¨¡å‹?\"),\n",
    "]\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "#### å‚™è¨»: llamaindexæ“ä½œè³‡æ–™è«‹è¦‹C9ä¹‹å¾Œèª²ç¨‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
