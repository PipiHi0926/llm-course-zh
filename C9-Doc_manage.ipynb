{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:small; color:gray;\"> Author: 鄭永誠, Year: 2024 </p>\n",
    "\n",
    "# 一些文檔處理工具基於 llama_index\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基礎改念 (複習):\n",
    "- **token / tokenization** ➡️ token是LLM在處理文字時的最小單位，可以理解就像是一個個單字，留意不同LLM會用[不同詞彙表](https://huggingface.co/docs/transformers/en/tokenizer_summary)切token\n",
    "\n",
    "- **embedding** ➡️ 把這些token轉成一組高維向量，而這個向量用來表示這個句子的涵義，要留意不同模型能吃的token大小有異\n",
    "\n",
    "- **chunk** ➡️ 分塊，當資料量太大時，我們會將其切成一個一個分塊(chunks)\n",
    "\n",
    "- **parse / parsing** ➡️ 把句子解析、轉換成更好理解的格式，像是各類文檔語法結構調整、抽取訊息等，像是從html, json...轉成更好使用的格式\n",
    "\n",
    "- **extractor** ➡️ 文本在存儲之前，我們可以透過LLM先一步去從中識別和提取特定的信息，如關鍵字識別、主題建模、摘要、建立相關問題...\n",
    "\n",
    "- **pipeline** ➡️ 上面講了很多處理的流程，我們可以將其串在一起，建立所謂的資料處理流程(pipeline)，此處會講基於llamaindex的實踐方法，(下面統稱transormers)\n",
    "\n",
    "\n",
    "## 最終完整流程(Data Ingestion)可能包含\n",
    "\n",
    "- **Loaders** ➡️ 允許與外部源集成以上傳信息\n",
    "\n",
    "- **transormers** ➡️ 資料處理流程，如 parse, split to chunk, extract, embedding... 等多種流程\n",
    "\n",
    "- **Vector Stores** ➡️ 將資訊存入向量資料庫\n",
    "\n",
    "- **Retrievers** ➡️ 用於信息檢索的組件，從大規模文本數據集中檢索相關信息\n",
    "\n",
    "- **LLM Agent / Tools** ➡️ 各個處理問題的Agent、LLM模型或各種工具\n",
    "\n",
    "- **Memories** ➡️  記錄對話\n",
    "\n",
    "- **Output Parsers** ➡️ 把結果轉換成需求的格式，如json...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## ✏️ IngestionPipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Name: llama-index\n",
      "Version: 0.10.64\n",
      "Summary: Interface between LLMs and your data\n",
      "Home-page: https://llamaindex.ai\n",
      "Author: Jerry Liu\n",
      "Author-email: jerry@llamaindex.ai\n",
      "License: MIT\n",
      "Location: c:\\Users\\PipiHi\\Desktop\\KM\\llm-course-zh\\.venv\\Lib\\site-packages\n",
      "Requires: llama-index-agent-openai, llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-legacy, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 安裝llamaindex的相關需求套件 \"\"\"\n",
    "# %pip uninstall llama_index -q\n",
    "%pip install llama-index -q\n",
    "\n",
    "%pip install llama-index-core -q\n",
    "%pip install llama-index-llms-openai -q\n",
    "%pip install llama-index-llms-replicate -q\n",
    "%pip install llama-index-embeddings-huggingface -q\n",
    "\n",
    "%pip show llama_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## ✏️ SentenceSplitter - chunk\n",
    "- 當文本太長時，Tokenizee後會超過Embedding模型長度上線時，我們會將其切成多個chunk  \n",
    "\n",
    "- 又或者是文本本身就有特別段落分隔規則(如 \"\\n\\n\\n\" )，也可以直接依據此規則切chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: llama-index\n",
      "Version: 0.10.64\n",
      "Summary: Interface between LLMs and your data\n",
      "Home-page: https://llamaindex.ai\n",
      "Author: Jerry Liu\n",
      "Author-email: jerry@llamaindex.ai\n",
      "License: MIT\n",
      "Location: c:\\Users\\PipiHi\\Desktop\\KM\\llm-course-zh\\.venv\\Lib\\site-packages\n",
      "Requires: llama-index-agent-openai, llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-legacy, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 下面使用\"\"\"\n",
    "%pip show llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原本文檔：\n",
      "義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到\n",
      "挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至\n",
      "於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲\n",
      "野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪\n",
      "跟維爾康在南極匯合。\n",
      "\n",
      "這是用字元分割後的文檔：\n",
      "[part_0]：義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。\n",
      "[part_1]：你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至於對整個太平洋，和充電器的核污染。再或者說透\n",
      "[part_2]：再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有\n",
      "[part_3]：射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪跟維爾康在南極匯合。\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 注意，網路上llama_index舊版的是用 llama_index.XXXX ，新版用llama_index.core.XXXX \"\"\"\n",
    "import textwrap\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "\n",
    "# 自定義一個文件\n",
    "documents = \"義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪跟維爾康在南極匯合。\"\n",
    "\n",
    "print(\"原本文檔：\")\n",
    "print(textwrap.fill(documents, width=50))\n",
    "\n",
    "node_parser  = SentenceSplitter(\n",
    "    chunk_size=100, # 設定每個chunk的大小\n",
    "    chunk_overlap=15, # 設定chunk之間的重疊大小，可避免斷詞語意流失\n",
    "    tokenizer= None, # 設定分詞器\n",
    "    paragraph_separator=\"\\n\\n\", # 設定段落之間的分隔符號\n",
    "    separator=\" \", # 用於拆分句子的預設的分隔字元\n",
    "    secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?' # 用於分割句子的備份正規表示式\n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(\n",
    "    [Document(text=documents)], show_progress=False\n",
    ")\n",
    "print(\"\\n這是用字元分割後的文檔：\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f'[part_{i}]：{node.text}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原本文檔：\n",
      "義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到\n",
      "挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至\n",
      "於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲\n",
      "野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪\n",
      "跟維爾康在南極匯合。\n",
      "\n",
      "這是用token數量分割後的文檔：\n",
      "[part_0]：義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量\n",
      "[part_1]：的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英\n",
      "[part_2]：容易推斷出人工飼養的東條英機，他是可以捕獲野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪跟維爾康在南極匯合。\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "上面例子是用字元去做切割，\n",
    "當然你也可以直接用轉換token後長度去做切割，\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "# 自定義一個文件\n",
    "documents = \"義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪跟維爾康在南極匯合。\"\n",
    "\n",
    "print(\"原本文檔：\")\n",
    "print(textwrap.fill(documents, width=50))\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=128, # 實際上會設如1024之類\n",
    "    chunk_overlap=20,\n",
    "    separator=\" \",\n",
    "    # encodingName: \"gpt2\", # 你可以選擇使用哪種tokenize模型\n",
    ")\n",
    "\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(\n",
    "    [Document(text=documents)], show_progress=False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n這是用token數量分割後的文檔：\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f'[part_{i}]：{node.text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## ✏️ SentenceSplitter - parser\n",
    "- 用來將既有檔案轉換(解析)成可用的文句格式\n",
    "\n",
    "- llamaindex已整合許多用來處理各種文檔的parser，詳見官方   \n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/\n",
    "\n",
    "    - HTMLNodeParser\n",
    "\n",
    "    - JSONNodeParser\n",
    "\n",
    "    - MarkdownNodeParser    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分割後的文檔：\n",
      "[part_0]：<div align=\"center\">\n",
      "  <h1>🤖 大型語言模型入門教學 中文分享整理  💻</h1>\n",
      "  <p align=\"center\">\n",
      "    ✍️ <a href=\"https://hackmd.io/@pputzh5cRhi6gZI0csfiyA/H1ejIyxHR\"> 作者: 鄭永誠</a> • \n",
      "    ✉️ <a href=\"mailto:jason0304050607@gmail.com\">信箱</a> • \n",
      "    🧑‍🤝‍🧑 <a href=\"https://www.dalabx.com.tw//\"> 合作夥伴: 紫式大數據決策 </a> • \n",
      "    👫 <a href=\"https://moraleai.com/\"> 我的朋朋: Morale AI </a> \n",
      "  </p>\n",
      "</div>\n",
      "<p><br/></p>\n",
      "<p>內容簡介:\n",
      "1. 🍻 <strong>LLM基礎改念:</strong> 我會整理一些LLM的需求知識，但git上是實作資源為主不會講述太多，有興趣請密我\n",
      "2. 🛠️ <strong>LLM相關工具:</strong> 以下內容全基於python實踐，同時會分享相關資源、套件、開源API...\n",
      "3. 💬 <strong>LLM系統架構:</strong> 會帶你由淺入深，慢慢了解部屬LLM系統(多Agent)的方向和一些好用工具</p>\n",
      "<p>這個分享內容宗旨:\n",
      "1. 🧩 <strong>讓你好上手:</strong> 提供最簡單的、盡可能可複製即用的code，讓新手也能盡可能快速入門 (而且是中文XD)\n",
      "2. 🎈 <strong>讓你免費玩:</strong> 全基於開源資源，讓你能夠無痛體驗LLM的功能和操作\n",
      "3. 😊 <strong>讓你喜歡上:</strong> 盡量提供簡單有趣的小例子，附上完整註解說明，讓你也能喜歡LLM可帶來的運用</p>\n",
      "<p>範例使用版本/輔助工具:\n",
      "- Python 3.12.4\n",
      "- 語言模型主要使用 llama-3.1-70b-versatile\n",
      "- 個人主要使用 IDE: VScode\n",
      "- 搭配工具 寫程式大幫手 <a href=\"https://github.com/features/copilot\">Copilot</a>\n",
      "- 其他: 使用 <a href=\"https://code.visualstudio.com/docs/python/linting\">pylint</a> 擴充套件來管理 Python 程式碼的風格</p>\n",
      "<p>主要資料來源:\n",
      "  💻 <a href=\"https://github.com/\">資料來源1: 偉大的Github</a> \n",
      "  🤗 <a href=\"https://huggingface.co/\">資料來源2: 偉大的抱抱臉</a> \n",
      "  👨 <a href=\"https://github.com/underlines/awesome-ml/blob/master/llm-tools.md/\">巨人的肩膀: LLM相關工具大整理</a></p>\n",
      "<hr />\n",
      "<h2>課程內容</h2>\n",
      "<p><img alt=\"alt text\" src=\"images/image.png\" />\n",
      "(因分享會有搭配我的簡報才會以這個架構講述，實際上這些課程無直接連貫性!!!)</p>\n",
      "<h3>✔️ Part 0: LLM 導論 (Instruction)</h3>\n",
      "<p>(這些皆以前讀書會簡報內容，恕此處跳過)\n",
      "💡本章節讓你從0開始，讓你了解AI到底是啥、LLM基礎原理到底是怎麼運作的，\n",
      "以及了解踏入LLM這塊領域時你必須了解的相關的名詞和知識</p>\n",
      "<p>| 主題 | 簡介 | 類別 | Notebook | Resource|\n",
      "|----------|-------------|:----------:|:----------:|:----------:|\n",
      "| I1-人工智慧簡介與發展近況 | 什麼是AI? 發展學派與歷史脈絡| 基礎課程 | 見讀書會ppt||\n",
      "| I2-機器學習基本概念 |機器/深度學習最最最白話版基礎概念| 基礎課程 | 見讀書會ppt||\n",
      "| I3-LLM基礎原理 |Token, Embedding, Transformer...等基礎| 基礎課程 | 見讀書會ppt||\n",
      "| I4-LLM相關知識|提示工程、RAG與LLM框架、Fine-tuned，模型超參數| 基礎課程| 見讀書會ppt||\n",
      "| I5-實踐工具|LangFlow / Flowise 快速實踐工作流程| 延伸補充 | <a href=\"Flowise.md\">flowise</a>|<a href=\"https://www.langflow.org/\">langflow</a>|\n",
      "| I6-簡易部屬|內部使用OpenWeb UI / Anything LLM部屬| 延伸補充 | 見讀書會ppt|<a href=\"https://github.com/open-webui/open-webui\">OpenWebUI</a>|</p>\n",
      "<h3>✔️ Part 1: Python 基礎實踐與建立流程</h3>\n",
      "<p>💡本章節你將了解LLM的基礎使用和部署概念，\n",
      "從如何使用LLM、如何建立簡單介面展示、建立流程並管控這些流程等\n",
      "並補充一些RAG和fine-tuned的先備知識</p>\n",
      "<p>| 主題 | 簡介 | 類別 | Notebook |\n",
      "|----------|-------------|:----------:|:----------:|\n",
      "| C0-前置作業與基礎工具|建立虛擬環境、基礎python輔助工具| 前置準備 |<a href=\"C0-Basic_info.ipynb\">C0</a>|\n",
      "| C1-簡單使用範例|Groq操作、程式實踐基礎問答| 基礎課程 |<a href=\"C1-Get_start_with_groq.ipynb\">C1</a>|\n",
      "| C2-立刻部屬簡易系統|Gradio快速實踐系統介面、即時對話系統| 基礎課程 |<a href=\"C2-Create_llm_ui.ipynb\">C2</a> |\n",
      "| C3-已結合LLM的一些開源工具|一些AI工具如open-interpreter,Scrapegraph-ai...| 額外分享 |<a href=\"C3-Ai_tools.ipynb\">C3</a>|\n",
      "| C4-進階RAG操作|Reranker概念和效果| 延伸補充 |<a href=\"C4-Advanced_rag.ipynb\">C4</a>|\n",
      "| C5-實踐LLM服務Agent流程-1|基於Langchain架構下的LangGraph實踐| 進階課程 |<a href=\"C5-Agent_flow.ipynb\">C5</a>|\n",
      "| C6-實踐LLM服務Agent流程-2|基於Langchain架構下的LangGraph實踐| 進階課程 |<a href=\"C6-Agent_flow.ipynb\">C6</a>|\n",
      "| C7-將Agent流程進行管控|使用langsmith來管理、更清楚瞭解建立的流程| 進階課程 |<a href=\"C7-Llm_application.ipynb\">C7</a>|\n",
      "| C8-fine-tuned簡易操作範例|使用Unsloth簡易實踐qLora fine tuned(只放程式碼、不實際運行)| 進階課程 |<a href=\"C8-Finetune.md\">C8</a>|</p>\n",
      "<p><img alt=\"alt text\" src=\"images/image-8.png\" /></p>\n",
      "<h3>✔️ Part 2: Routing in RAG-Driven Applications</h3>\n",
      "<p>💡 本章節你將學會更進一步的資料處理流程，\n",
      "最終完整流程(Data Ingestion)可能包含:</p>\n",
      "<ul>\n",
      "<li>\n",
      "<p><strong>Loaders</strong> ➡️ 允許與外部源集成以上傳信息</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>transormers</strong> ➡️ 各種資料處理流程，如 parse (轉換), split to chunk(分割), extract(擷取), embedding(向量化)... 等多種流程</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Vector Stores</strong> ➡️ 將資訊存入向量資料庫</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Retrievers</strong> ➡️ 建立用於信息檢索的組件，從大規模文本數據集中檢索相關信息，即實踐RAG</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>LLM Agent / Tools</strong> ➡️ 各個處理問題的Agent、LLM模型或各種工具</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Memories</strong> ➡️  可能會有記錄對話的需求，特別當你建立的是對話式的LLM應用時</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Output Parsers</strong> ➡️ 把結果轉換成需求的格式，常見如json, md...</p>\n",
      "</li>\n",
      "</ul>\n",
      "<p>| 主題 | 簡介 | 類別 | Note|\n",
      "|----------|-------------|:----------:|:----------:|\n",
      "| C9-文本資料處理基礎|文檔spit, parse, extract等概念和建立pipeline| 基礎課程 |<a href=\"C9-Doc_manage.ipynb\">C9</a>|\n",
      "| C10-文本處理流程概念|接續C9，資料處理、Ingestion Pipeline與RAG routing| 基礎課程 ||\n",
      "| C11-向量資料庫建置與各功能|| 基礎課程 ||\n",
      "| C12-其他資料庫結構|知識圖譜與GraphRAG| 延伸補充 ||</p>\n",
      "<h2>先備知識</h2>\n",
      "<h3>1. LLM是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>大型語言模型 (Large Language Model) 的簡稱</li>\n",
      "<li>你可以把他理解成是一個模型，能根據輸入的文字生成文字回傳，就像在做文字接龍一樣</li>\n",
      "<li>背後深度學習, Tokenization, embedding, attention機制, Transformer 等相關介紹詳見以前分享\n",
      "<img alt=\"alt text\" src=\"images/image-2.png\" />\n",
      "<img alt=\"alt text\" src=\"images/image-3.png\" />\n",
      "<img alt=\"alt text\" src=\"images/image-5.png\" /></li>\n",
      "</ul>\n",
      "<h3>2. Hugging Face 🤗 是什麼?</h3>\n",
      "<ul>\n",
      "<li>你可以把他理解成AI界的Github</li>\n",
      "<li>使用者可以在上邊發表和<a href=\"https://huggingface.co/docs/transformers/model_sharing\">共享預訓練模型</a>、資料集和展示檔案</li>\n",
      "<li>許多<a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\">模型排名</a>、<a href=\"https://huggingface.co/docs/transformers/llm_tutorial\">程式範例</a>都可在上面找到</li>\n",
      "</ul>\n",
      "<h3>3. Llama 🦙 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>Llama是Meta開發的一系列大型語言模型，如llama2, llama3, llama3.1</li>\n",
      "<li>這些語言模型都是免費的!!也能自行去做模型參數微調訓練(Fine-tuned)</li>\n",
      "<li>至於其他常見付費LLM則包含<a href=\"https://openai.com/index/openai-api/\">Open AI</a>, <a href=\"https://www.anthropic.com/api\">Claude</a>系列...</li>\n",
      "</ul>\n",
      "<h3>4. Ollama 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>開源的本地端LLM平台</li>\n",
      "<li>允許用戶在自己的電腦上運行和調用多種開放原始碼的語言模型</li>\n",
      "<li>若要部屬自己公司/組織內部的LLM，可以運用其資源</li>\n",
      "<li>建議可搭配<a href=\"https://docs.openwebui.com/\">Open WebUI</a>、<a href=\"https://anythingllm.com/\">AnythingLLM</a>實踐UI操作介面和管理</li>\n",
      "</ul>\n",
      "<h3>5. LangChain 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li><a href=\"https://python.langchain.com/v0.2/docs/introduction/\">LangChain</a>是LLM框架，目的在簡化使用大型語言模型（LLMs）開發應用程序的過程</li>\n",
      "<li>你可以簡單理解成他是個工具箱，把LLM操作過程可能需要的工具、外部數據源整合起來</li>\n",
      "<li>有這個工具箱，你就能更方便的調用他撰寫python llm相關程式</li>\n",
      "<li>其他優勢包含很多延伸服務和工具也基於他被開發出來 (如後面會講到的<a href=\"https://langchain-ai.github.io/langgraph/\">LangGraph</a>)</li>\n",
      "<li>對於新手而言，他官網上也有非常大量<a href=\"https://python.langchain.com/v0.2/docs/tutorials/llm_chain/\">範例程式</a>輔助你實踐llm</li>\n",
      "<li>其他常用框架還有LlamaIndex，其更擅長處理文本(e.g.非結構資訊)、自定義知識庫、有多種索引查詢功能</li>\n",
      "</ul>\n",
      "<h3>6. 機器學習 (Machine Learning)、深度學習 (Deep Learning )、人工智慧 (Artificial Intelligence) 之間的關聯是?</h3>\n",
      "<ul>\n",
      "<li>AI (人工智慧) 泛指使電腦和機器能夠模擬人類智慧和解決問題能力的技術，做到各種人類的判斷或行為</li>\n",
      "<li>ML (機器學習)，則是一種常見的技術手法，你可以想像成找出一個  𝑓(𝑥)，可以根據輸入值生成想要的結果  </li>\n",
      "<li>至於 DL (深度學習)，則是ML領域中基於神經網路(NN)的概念，一種表現亮眼的技術類型</li>\n",
      "<li>GAI (生成式人工智慧) 則是當中一種可以創造新內容和想法的AI概念，如創造對話、故事、影像、視訊和音樂!</li>\n",
      "<li>當今(2024.6)，GAI領域、如本讀書的重點LLM，當中最主要仰賴的技術模型就是DL為主!!!</li>\n",
      "<li>見下圖，你就能很清楚知道他們之間的關係!!!!!</li>\n",
      "</ul>\n",
      "<p><img alt=\"alt text\" src=\"images/image-9.png\" /></p>\n",
      "<h3>7. Transformer 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>Transformer是一種深度學習模型架構，在多種自然語言(NLP)任務處理表現出色</li>\n",
      "<li>為當今(2024)生成式AI的浪潮重大突破的核心技術概念</li>\n",
      "</ul>\n",
      "<p><img alt=\"alt text\" src=\"images/image-6.png\" /></p>\n",
      "<ul>\n",
      "<li>ChatGPT 裡面的 \"T\"，指的就是Transformer喔!</li>\n",
      "</ul>\n",
      "<p><img alt=\"alt text\" src=\"images/image-7.png\" /></p>\n",
      "<h3>8. 我該使用什麼模型?</h3>\n",
      "<ul>\n",
      "<li>模型更新很快，且不同模型在不同面相有不同表現，我這邊不會特別推薦一定要用哪個!</li>\n",
      "<li>模型普遍理論上，參數越多效果越好，但所需資源空間也越大，如Llama 3.1 405B就代表他是405billion參數的大模型</li>\n",
      "<li>建議去看HuggingFace上的<a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\">LeaderBoard</a>，看看個模型在不同面向、語系的排名</li>\n",
      "<li>若是要用開源LLM模型，建議可先去<a href=\"https://api.together.ai/signin?redirectUrl=/playground/chat/meta-llama\">Together.ai</a>之類的免費測試不同模型效果~ 再決定用哪個</li>\n",
      "<li>這邊範例，都會是以免費、開源的模型為主!! 甚至是用一些直接免下載模型的API~</li>\n",
      "</ul>\n",
      "<h2>主要使用的課程套件/工具/架構運用摘要</h2>\n",
      "<ul>\n",
      "<li>C0: 可略過</li>\n",
      "<li>C1: Langchain, Groq, llama_index</li>\n",
      "<li>C2: Gradio</li>\n",
      "<li>C3: 可略過</li>\n",
      "<li>C4: Sentence-transformer, Reranker (with Jina)</li>\n",
      "<li>C5: Langchain, LangGraph</li>\n",
      "<li>C6: Langchain, LangGraph</li>\n",
      "<li>C7: LangSmith</li>\n",
      "<li>C8: Unsloth</li>\n",
      "<li>C9: llama_index</li>\n",
      "</ul>\n",
      "<p>接下來就請去各ipynb筆記上去了解如何操作吧 ~</p>\n",
      "<h2>補充: 愛用工具/連結推廣</h2>\n",
      "<ol>\n",
      "<li>\n",
      "<p><a href=\"https://www.langflow.org/\">Langflow</a> / <a href=\"https://github.com/FlowiseAI/Flowise\">Flowise</a>: 帶你實踐no code介面串接開發大型語言模型應用程式</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://www.coze.com/home\">COZE</a> (以前免費常用...但現在QQ): 快速部屬LLM服務、Agent、建立dicord bot、設計workflow等，介面很舒服</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://www.perplexity.ai/\">perplexity</a>: 目前(2024.7)用起來最順的LLM輔助網頁搜尋工具</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://claude.ai/new\">claude 3.5 sonnet</a>: 目前(2024.7)覺得code表現能力最棒的LLM，特別是他的<a href=\"https://www.youtube.com/watch?v=rHqk0ZGb6qo\">Artifacts</a>功能</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>寫程式一定要<a href=\"https://github.com/features/copilot\">Copilot Github</a> 或是嘗試近期(2024.7)熱門的 <a href=\"https://www.cursor.com/\">Cursor</a>，後者我雖沒在用，但都很推</p>\n",
      "</li>\n",
      "</ol>\n",
      "<h2>自學連結</h2>\n",
      "<ol>\n",
      "<li>\n",
      "<p>Hugging Face, Github；甚至是 <a href=\"https://discord.gg/g662TXV6\">GAI年會</a>、臉書社團, iT 邦幫忙, Medium 文章等資源</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://github.com/mlabonne/llm-course\">LLM roadmap學習方向 – 技術面 &amp; 應用面 (github)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>李宏毅 – 生成式導論2024 <a href=\"https://www.youtube.com/watch?v=AVIKFXLCPY8&amp;list=PLJV_el3uVTsPz6CTopeRp2L2t4aL_KgiI\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>李宏毅 – 【機器學習 2023】(生成式 AI) <a href=\"https://www.youtube.com/watch?v=yiY4nPOzJEg&amp;list=PLJV_el3uVTsOePyfmkfivYZ7Rqr2nMk3W\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>李宏毅 – 【機器學習 2022 】<a href=\"https://www.youtube.com/watch?v=7XZR0-4uS5s&amp;list=PLJV_el3uVTsPM2mM-OQzJXziCGJa8nJL8\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>陳縕儂 – 深度學習應用、人工智慧導論 <a href=\"https://www.youtube.com/watch?v=g4yONRTpbE4&amp;list=PLOAQYZPRn2V658cD6AjiBmKohfMIevWO7\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://github.com/xianshang33/llm-paper-daily\">LLM相關論文整理日更 (github)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>諸多YT上資源</p>\n",
      "<ul>\n",
      "<li>01Coder</li>\n",
      "<li>零度解說</li>\n",
      "<li>最佳拍檔</li>\n",
      "<li>王木头学科学</li>\n",
      "<li>Ph.D. Vlog</li>\n",
      "<li>AI Jason</li>\n",
      "<li>AI超元域</li>\n",
      "<li>跟李沐学AI (Mu Li)</li>\n",
      "<li>GrandmaCan -我阿嬤都會</li>\n",
      "<li>3Blue1Brown</li>\n",
      "<li>AI小码哥</li>\n",
      "</ul>\n",
      "</li>\n",
      "<li>\n",
      "<p>LLM資源彙總 <a href=\"https://github.com/liguodongiot/llm-resource\">(github)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://github.com/LlamaFamily/Llama-Chinese\">llama中文社區 </a></p>\n",
      "</li>\n",
      "</ol>\n",
      "<h2>Backup URL</h2>\n",
      "<ul>\n",
      "<li><a href=\"https://www.run.ai/guides/ai-open-source-projects/nvidia-nemo\">NVIDIA NeMo</a>、<a href=\"https://github.com/NVIDIA/NeMo\">NeMo GitHub</a></li>\n",
      "<li><a href=\"https://build.nvidia.com/meta/llama-3_1-405b-instruct\">NV llama-3_1-405b-instruct</a></li>\n",
      "<li>https://github.com/xubuvd/LLMs</li>\n",
      "<li><a href=\"https://github.com/vanna-ai/vanna\">vanna</a>、<a href=\"https://github.com/explodinggradients/ragas\">Ragas</a>、<a href=\"https://github.com/microsoft/graphrag\">RagGraph</a></li>\n",
      "<li><a href=\"https://zapier.com/app/home\">zapier</a>  完全沒任何基礎也能建置如信件、notion等的流程整合服務工具</li>\n",
      "<li><a href=\"https://cloud.dify.ai/apps\">Dify</a> 目前用起來覺得，UI介面最舒服且最無難度的上手no code應用流程建置工具 (但要錢用的模型也要錢)</li>\n",
      "</ul>\n",
      "<h2>問題回饋 🤔</h2>\n",
      "<p>如有問題，歡迎用各種方法直接聯繫我，或在GitHub Issue中提交喔~  </p>\n",
      "<p align=\"center\">\n",
      "  <a href=\"https://www.facebook.com/YCrabbit.0926\">\n",
      "    <img src=\"images/fb_logo.png\" alt=\"FB\" width=\"30\" height=\"30\">\n",
      "  </a>&nbsp;&nbsp;&nbsp;&nbsp;\n",
      "  <a href=\"https://www.instagram.com/ssmc__0926/\">\n",
      "    <img src=\"images/ig_logo.png\" alt=\"Instagram\" width=\"30\" height=\"30\">\n",
      "  </a>&nbsp;&nbsp;&nbsp;&nbsp;\n",
      "  <a href=\"mailto:jason0304050607@gmail.com\">\n",
      "    <img src=\"images/gmail.png\" alt=\"Gmail\" width=\"30\" height=\"30\">\n",
      "  </a>&nbsp;&nbsp;&nbsp;&nbsp;\n",
      "  <a href=\"https://medium.com/@yc_rabbit\">\n",
      "    <img src=\"images/medium.png\" alt=\"medium\" width=\"30\" height=\"30\">\n",
      "  </a>&nbsp;&nbsp;&nbsp;&nbsp;\n",
      "  <a href=\"https://www.linkedin.com/in/%E6%B0%B8%E8%AA%A0-%E9%84%AD-56a3a5284/\">\n",
      "    <img src=\"images/linkin_logo.png\" alt=\"Linkin\" width=\"30\" height=\"30\">\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "<p align=\"center\">\n",
      "  <sub>版權所有© 2024 鄭永誠 YC-Cheng (Jason)。保留所有權力。</sub>\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" 這邊以 MarkdownNodeParser 為例 \"\"\"\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core import Document\n",
    "import markdown\n",
    "from pathlib import Path\n",
    "\n",
    "parser = MarkdownNodeParser()\n",
    "\n",
    "# 這邊我們先定義一個讀取markdown文件的函數\n",
    "def read_markdown_file(filename):\n",
    "    # Create a Path object for the markdown file\n",
    "    file_path = Path(filename)\n",
    "    \n",
    "    # Open and read the file content\n",
    "    with file_path.open('r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    return content\n",
    "\n",
    "# 讀取我們的README.md文件\n",
    "content = read_markdown_file('README.md')\n",
    "markdown_content = markdown.markdown(content)\n",
    "\n",
    "\n",
    "# 注意，node_parser不是直接吃文本，而是吃list of Document物件\n",
    "documents = Document(text=markdown_content)  # id_ 可選，可以設置為任何標識符\n",
    "\n",
    "\n",
    "# 我直接拿我們的README.md來做範例\n",
    "nodes = parser.get_nodes_from_documents([documents])\n",
    "\n",
    "# 留意我因為文檔太短，所以只有一個node\n",
    "print(\"\\n分割後的文檔：\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f'[part_{i}]：{node.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## ✏️ SentenceSplitter - parser\n",
    "有許多用來處理各種文檔的parser，詳見官方   \n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \"\"\"\n",
    "%pip install llama_index.core -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分割後的文檔：\n",
      "[part_0]：<div align=\"center\">\n",
      "  <h1>🤖 大型語言模型入門教學 中文分享整理  💻</h1>\n",
      "  <p align=\"center\">\n",
      "    ✍️ <a href=\"https://hackmd.io/@pputzh5cRhi6gZI0csfiyA/H1ejIyxHR\"> 作者: 鄭永誠</a> • \n",
      "    ✉️ <a href=\"mailto:jason0304050607@gmail.com\">信箱</a> • \n",
      "    🧑‍🤝‍🧑 <a href=\"https://www.dalabx.com.tw//\"> 合作夥伴: 紫式大數據決策 </a> • \n",
      "    👫 <a href=\"https://moraleai.com/\"> 我的朋朋: Morale AI </a> \n",
      "  </p>\n",
      "</div>\n",
      "<p><br/></p>\n",
      "<p>內容簡介:\n",
      "1. 🍻 <strong>LLM基礎改念:</strong> 我會整理一些LLM的需求知識，但git上是實作資源為主不會講述太多，有興趣請密我\n",
      "2. 🛠️ <strong>LLM相關工具:</strong> 以下內容全基於python實踐，同時會分享相關資源、套件、開源API...\n",
      "3. 💬 <strong>LLM系統架構:</strong> 會帶你由淺入深，慢慢了解部屬LLM系統(多Agent)的方向和一些好用工具</p>\n",
      "<p>這個分享內容宗旨:\n",
      "1. 🧩 <strong>讓你好上手:</strong> 提供最簡單的、盡可能可複製即用的code，讓新手也能盡可能快速入門 (而且是中文XD)\n",
      "2. 🎈 <strong>讓你免費玩:</strong> 全基於開源資源，讓你能夠無痛體驗LLM的功能和操作\n",
      "3. 😊 <strong>讓你喜歡上:</strong> 盡量提供簡單有趣的小例子，附上完整註解說明，讓你也能喜歡LLM可帶來的運用</p>\n",
      "<p>範例使用版本/輔助工具:\n",
      "- Python 3.12.4\n",
      "- 語言模型主要使用 llama-3.1-70b-versatile\n",
      "- 個人主要使用 IDE: VScode\n",
      "- 搭配工具 寫程式大幫手 <a href=\"https://github.com/features/copilot\">Copilot</a>\n",
      "- 其他: 使用 <a href=\"https://code.visualstudio.com/docs/python/linting\">pylint</a> 擴充套件來管理 Python 程式碼的風格</p>\n",
      "<p>主要資料來源:\n",
      "  💻 <a href=\"https://github.com/\">資料來源1: 偉大的Github</a> \n",
      "  🤗 <a href=\"https://huggingface.co/\">資料來源2: 偉大的抱抱臉</a> \n",
      "  👨 <a href=\"https://github.com/underlines/awesome-ml/blob/master/llm-tools.md/\">巨人的肩膀: LLM相關工具大整理</a></p>\n",
      "<hr />\n",
      "<h2>課程內容</h2>\n",
      "<p><img alt=\"alt text\" src=\"images/image.png\" />\n",
      "(因分享會有搭配我的簡報才會以這個架構講述，實際上這些課程無直接連貫性!!!)</p>\n",
      "<h3>✔️ Part 0: LLM 導論 (Instruction)</h3>\n",
      "<p>(這些皆以前讀書會簡報內容，恕此處跳過)\n",
      "💡本章節讓你從0開始，讓你了解AI到底是啥、LLM基礎原理到底是怎麼運作的，\n",
      "以及了解踏入LLM這塊領域時你必須了解的相關的名詞和知識</p>\n",
      "<p>| 主題 | 簡介 | 類別 | Notebook | Resource|\n",
      "|----------|-------------|:----------:|:----------:|:----------:|\n",
      "| I1-人工智慧簡介與發展近況 | 什麼是AI? 發展學派與歷史脈絡| 基礎課程 | 見讀書會ppt||\n",
      "| I2-機器學習基本概念 |機器/深度學習最最最白話版基礎概念| 基礎課程 | 見讀書會ppt||\n",
      "| I3-LLM基礎原理 |Token, Embedding, Transformer...等基礎| 基礎課程 | 見讀書會ppt||\n",
      "| I4-LLM相關知識|提示工程、RAG與LLM框架、Fine-tuned，模型超參數| 基礎課程| 見讀書會ppt||\n",
      "| I5-實踐工具|LangFlow / Flowise 快速實踐工作流程| 延伸補充 | <a href=\"Flowise.md\">flowise</a>|<a href=\"https://www.langflow.org/\">langflow</a>|\n",
      "| I6-簡易部屬|內部使用OpenWeb UI / Anything LLM部屬| 延伸補充 | 見讀書會ppt|<a href=\"https://github.com/open-webui/open-webui\">OpenWebUI</a>|</p>\n",
      "<h3>✔️ Part 1: Python 基礎實踐與建立流程</h3>\n",
      "<p>💡本章節你將了解LLM的基礎使用和部署概念，\n",
      "從如何使用LLM、如何建立簡單介面展示、建立流程並管控這些流程等\n",
      "並補充一些RAG和fine-tuned的先備知識</p>\n",
      "<p>| 主題 | 簡介 | 類別 | Notebook |\n",
      "|----------|-------------|:----------:|:----------:|\n",
      "| C0-前置作業與基礎工具|建立虛擬環境、基礎python輔助工具| 前置準備 |<a href=\"C0-Basic_info.ipynb\">C0</a>|\n",
      "| C1-簡單使用範例|Groq操作、程式實踐基礎問答| 基礎課程 |<a href=\"C1-Get_start_with_groq.ipynb\">C1</a>|\n",
      "| C2-立刻部屬簡易系統|Gradio快速實踐系統介面、即時對話系統| 基礎課程 |<a href=\"C2-Create_llm_ui.ipynb\">C2</a> |\n",
      "| C3-已結合LLM的一些開源工具|一些AI工具如open-interpreter,Scrapegraph-ai...| 額外分享 |<a href=\"C3-Ai_tools.ipynb\">C3</a>|\n",
      "| C4-進階RAG操作|Reranker概念和效果| 延伸補充 |<a href=\"C4-Advanced_rag.ipynb\">C4</a>|\n",
      "| C5-實踐LLM服務Agent流程-1|基於Langchain架構下的LangGraph實踐| 進階課程 |<a href=\"C5-Agent_flow.ipynb\">C5</a>|\n",
      "| C6-實踐LLM服務Agent流程-2|基於Langchain架構下的LangGraph實踐| 進階課程 |<a href=\"C6-Agent_flow.ipynb\">C6</a>|\n",
      "| C7-將Agent流程進行管控|使用langsmith來管理、更清楚瞭解建立的流程| 進階課程 |<a href=\"C7-Llm_application.ipynb\">C7</a>|\n",
      "| C8-fine-tuned簡易操作範例|使用Unsloth簡易實踐qLora fine tuned(只放程式碼、不實際運行)| 進階課程 |<a href=\"C8-Finetune.md\">C8</a>|</p>\n",
      "<p><img alt=\"alt text\" src=\"images/image-8.png\" /></p>\n",
      "<h3>✔️ Part 2: Routing in RAG-Driven Applications</h3>\n",
      "<p>💡 本章節你將學會更進一步的資料處理流程，\n",
      "最終完整流程(Data Ingestion)可能包含:</p>\n",
      "<ul>\n",
      "<li>\n",
      "<p><strong>Loaders</strong> ➡️ 允許與外部源集成以上傳信息</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>transormers</strong> ➡️ 各種資料處理流程，如 parse (轉換), split to chunk(分割), extract(擷取), embedding(向量化)... 等多種流程</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Vector Stores</strong> ➡️ 將資訊存入向量資料庫</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Retrievers</strong> ➡️ 建立用於信息檢索的組件，從大規模文本數據集中檢索相關信息，即實踐RAG</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>LLM Agent / Tools</strong> ➡️ 各個處理問題的Agent、LLM模型或各種工具</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Memories</strong> ➡️  可能會有記錄對話的需求，特別當你建立的是對話式的LLM應用時</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Output Parsers</strong> ➡️ 把結果轉換成需求的格式，常見如json, md...</p>\n",
      "</li>\n",
      "</ul>\n",
      "<p>| 主題 | 簡介 | 類別 | Note|\n",
      "|----------|-------------|:----------:|:----------:|\n",
      "| C9-文本資料處理基礎|文檔spit, parse, extract等概念和建立pipeline| 基礎課程 |<a href=\"C9-Doc_manage.ipynb\">C9</a>|\n",
      "| C10-文本處理流程概念|接續C9，資料處理、Ingestion Pipeline與RAG routing| 基礎課程 ||\n",
      "| C11-向量資料庫建置與各功能|| 基礎課程 ||\n",
      "| C12-其他資料庫結構|知識圖譜與GraphRAG| 延伸補充 ||</p>\n",
      "<h2>先備知識</h2>\n",
      "<h3>1. LLM是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>大型語言模型 (Large Language Model) 的簡稱</li>\n",
      "<li>你可以把他理解成是一個模型，能根據輸入的文字生成文字回傳，就像在做文字接龍一樣</li>\n",
      "<li>背後深度學習, Tokenization, embedding, attention機制, Transformer 等相關介紹詳見以前分享\n",
      "<img alt=\"alt text\" src=\"images/image-2.png\" />\n",
      "<img alt=\"alt text\" src=\"images/image-3.png\" />\n",
      "<img alt=\"alt text\" src=\"images/image-5.png\" /></li>\n",
      "</ul>\n",
      "<h3>2. Hugging Face 🤗 是什麼?</h3>\n",
      "<ul>\n",
      "<li>你可以把他理解成AI界的Github</li>\n",
      "<li>使用者可以在上邊發表和<a href=\"https://huggingface.co/docs/transformers/model_sharing\">共享預訓練模型</a>、資料集和展示檔案</li>\n",
      "<li>許多<a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\">模型排名</a>、<a href=\"https://huggingface.co/docs/transformers/llm_tutorial\">程式範例</a>都可在上面找到</li>\n",
      "</ul>\n",
      "<h3>3. Llama 🦙 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>Llama是Meta開發的一系列大型語言模型，如llama2, llama3, llama3.1</li>\n",
      "<li>這些語言模型都是免費的!!也能自行去做模型參數微調訓練(Fine-tuned)</li>\n",
      "<li>至於其他常見付費LLM則包含<a href=\"https://openai.com/index/openai-api/\">Open AI</a>, <a href=\"https://www.anthropic.com/api\">Claude</a>系列...</li>\n",
      "</ul>\n",
      "<h3>4. Ollama 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>開源的本地端LLM平台</li>\n",
      "<li>允許用戶在自己的電腦上運行和調用多種開放原始碼的語言模型</li>\n",
      "<li>若要部屬自己公司/組織內部的LLM，可以運用其資源</li>\n",
      "<li>建議可搭配<a href=\"https://docs.openwebui.com/\">Open WebUI</a>、<a href=\"https://anythingllm.com/\">AnythingLLM</a>實踐UI操作介面和管理</li>\n",
      "</ul>\n",
      "<h3>5. LangChain 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li><a href=\"https://python.langchain.com/v0.2/docs/introduction/\">LangChain</a>是LLM框架，目的在簡化使用大型語言模型（LLMs）開發應用程序的過程</li>\n",
      "<li>你可以簡單理解成他是個工具箱，把LLM操作過程可能需要的工具、外部數據源整合起來</li>\n",
      "<li>有這個工具箱，你就能更方便的調用他撰寫python llm相關程式</li>\n",
      "<li>其他優勢包含很多延伸服務和工具也基於他被開發出來 (如後面會講到的<a href=\"https://langchain-ai.github.io/langgraph/\">LangGraph</a>)</li>\n",
      "<li>對於新手而言，他官網上也有非常大量<a href=\"https://python.langchain.com/v0.2/docs/tutorials/llm_chain/\">範例程式</a>輔助你實踐llm</li>\n",
      "<li>其他常用框架還有LlamaIndex，其更擅長處理文本(e.g.非結構資訊)、自定義知識庫、有多種索引查詢功能</li>\n",
      "</ul>\n",
      "<h3>6. 機器學習 (Machine Learning)、深度學習 (Deep Learning )、人工智慧 (Artificial Intelligence) 之間的關聯是?</h3>\n",
      "<ul>\n",
      "<li>AI (人工智慧) 泛指使電腦和機器能夠模擬人類智慧和解決問題能力的技術，做到各種人類的判斷或行為</li>\n",
      "<li>ML (機器學習)，則是一種常見的技術手法，你可以想像成找出一個  𝑓(𝑥)，可以根據輸入值生成想要的結果  </li>\n",
      "<li>至於 DL (深度學習)，則是ML領域中基於神經網路(NN)的概念，一種表現亮眼的技術類型</li>\n",
      "<li>GAI (生成式人工智慧) 則是當中一種可以創造新內容和想法的AI概念，如創造對話、故事、影像、視訊和音樂!</li>\n",
      "<li>當今(2024.6)，GAI領域、如本讀書的重點LLM，當中最主要仰賴的技術模型就是DL為主!!!</li>\n",
      "<li>見下圖，你就能很清楚知道他們之間的關係!!!!!</li>\n",
      "</ul>\n",
      "<p><img alt=\"alt text\" src=\"images/image-9.png\" /></p>\n",
      "<h3>7. Transformer 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>Transformer是一種深度學習模型架構，在多種自然語言(NLP)任務處理表現出色</li>\n",
      "<li>為當今(2024)生成式AI的浪潮重大突破的核心技術概念</li>\n",
      "</ul>\n",
      "<p><img alt=\"alt text\" src=\"images/image-6.png\" /></p>\n",
      "<ul>\n",
      "<li>ChatGPT 裡面的 \"T\"，指的就是Transformer喔!</li>\n",
      "</ul>\n",
      "<p><img alt=\"alt text\" src=\"images/image-7.png\" /></p>\n",
      "<h3>8. 我該使用什麼模型?</h3>\n",
      "<ul>\n",
      "<li>模型更新很快，且不同模型在不同面相有不同表現，我這邊不會特別推薦一定要用哪個!</li>\n",
      "<li>模型普遍理論上，參數越多效果越好，但所需資源空間也越大，如Llama 3.1 405B就代表他是405billion參數的大模型</li>\n",
      "<li>建議去看HuggingFace上的<a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\">LeaderBoard</a>，看看個模型在不同面向、語系的排名</li>\n",
      "<li>若是要用開源LLM模型，建議可先去<a href=\"https://api.together.ai/signin?redirectUrl=/playground/chat/meta-llama\">Together.ai</a>之類的免費測試不同模型效果~ 再決定用哪個</li>\n",
      "<li>這邊範例，都會是以免費、開源的模型為主!! 甚至是用一些直接免下載模型的API~</li>\n",
      "</ul>\n",
      "<h2>主要使用的課程套件/工具/架構運用摘要</h2>\n",
      "<ul>\n",
      "<li>C0: 可略過</li>\n",
      "<li>C1: Langchain, Groq, llama_index</li>\n",
      "<li>C2: Gradio</li>\n",
      "<li>C3: 可略過</li>\n",
      "<li>C4: Sentence-transformer, Reranker (with Jina)</li>\n",
      "<li>C5: Langchain, LangGraph</li>\n",
      "<li>C6: Langchain, LangGraph</li>\n",
      "<li>C7: LangSmith</li>\n",
      "<li>C8: Unsloth</li>\n",
      "<li>C9: llama_index</li>\n",
      "</ul>\n",
      "<p>接下來就請去各ipynb筆記上去了解如何操作吧 ~</p>\n",
      "<h2>補充: 愛用工具/連結推廣</h2>\n",
      "<ol>\n",
      "<li>\n",
      "<p><a href=\"https://www.langflow.org/\">Langflow</a> / <a href=\"https://github.com/FlowiseAI/Flowise\">Flowise</a>: 帶你實踐no code介面串接開發大型語言模型應用程式</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://www.coze.com/home\">COZE</a> (以前免費常用...但現在QQ): 快速部屬LLM服務、Agent、建立dicord bot、設計workflow等，介面很舒服</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://www.perplexity.ai/\">perplexity</a>: 目前(2024.7)用起來最順的LLM輔助網頁搜尋工具</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://claude.ai/new\">claude 3.5 sonnet</a>: 目前(2024.7)覺得code表現能力最棒的LLM，特別是他的<a href=\"https://www.youtube.com/watch?v=rHqk0ZGb6qo\">Artifacts</a>功能</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>寫程式一定要<a href=\"https://github.com/features/copilot\">Copilot Github</a> 或是嘗試近期(2024.7)熱門的 <a href=\"https://www.cursor.com/\">Cursor</a>，後者我雖沒在用，但都很推</p>\n",
      "</li>\n",
      "</ol>\n",
      "<h2>自學連結</h2>\n",
      "<ol>\n",
      "<li>\n",
      "<p>Hugging Face, Github；甚至是 <a href=\"https://discord.gg/g662TXV6\">GAI年會</a>、臉書社團, iT 邦幫忙, Medium 文章等資源</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://github.com/mlabonne/llm-course\">LLM roadmap學習方向 – 技術面 &amp; 應用面 (github)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>李宏毅 – 生成式導論2024 <a href=\"https://www.youtube.com/watch?v=AVIKFXLCPY8&amp;list=PLJV_el3uVTsPz6CTopeRp2L2t4aL_KgiI\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>李宏毅 – 【機器學習 2023】(生成式 AI) <a href=\"https://www.youtube.com/watch?v=yiY4nPOzJEg&amp;list=PLJV_el3uVTsOePyfmkfivYZ7Rqr2nMk3W\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>李宏毅 – 【機器學習 2022 】<a href=\"https://www.youtube.com/watch?v=7XZR0-4uS5s&amp;list=PLJV_el3uVTsPM2mM-OQzJXziCGJa8nJL8\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>陳縕儂 – 深度學習應用、人工智慧導論 <a href=\"https://www.youtube.com/watch?v=g4yONRTpbE4&amp;list=PLOAQYZPRn2V658cD6AjiBmKohfMIevWO7\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://github.com/xianshang33/llm-paper-daily\">LLM相關論文整理日更 (github)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>諸多YT上資源</p>\n",
      "<ul>\n",
      "<li>01Coder</li>\n",
      "<li>零度解說</li>\n",
      "<li>最佳拍檔</li>\n",
      "<li>王木头学科学</li>\n",
      "<li>Ph.D. Vlog</li>\n",
      "<li>AI Jason</li>\n",
      "<li>AI超元域</li>\n",
      "<li>跟李沐学AI (Mu Li)</li>\n",
      "<li>GrandmaCan -我阿嬤都會</li>\n",
      "<li>3Blue1Brown</li>\n",
      "<li>AI小码哥</li>\n",
      "</ul>\n",
      "</li>\n",
      "<li>\n",
      "<p>LLM資源彙總 <a href=\"https://github.com/liguodongiot/llm-resource\">(github)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://github.com/LlamaFamily/Llama-Chinese\">llama中文社區 </a></p>\n",
      "</li>\n",
      "</ol>\n",
      "<h2>Backup URL</h2>\n",
      "<ul>\n",
      "<li><a href=\"https://www.run.ai/guides/ai-open-source-projects/nvidia-nemo\">NVIDIA NeMo</a>、<a href=\"https://github.com/NVIDIA/NeMo\">NeMo GitHub</a></li>\n",
      "<li><a href=\"https://build.nvidia.com/meta/llama-3_1-405b-instruct\">NV llama-3_1-405b-instruct</a></li>\n",
      "<li>https://github.com/xubuvd/LLMs</li>\n",
      "<li><a href=\"https://github.com/vanna-ai/vanna\">vanna</a>、<a href=\"https://github.com/explodinggradients/ragas\">Ragas</a>、<a href=\"https://github.com/microsoft/graphrag\">RagGraph</a></li>\n",
      "<li><a href=\"https://zapier.com/app/home\">zapier</a>  完全沒任何基礎也能建置如信件、notion等的流程整合服務工具</li>\n",
      "<li><a href=\"https://cloud.dify.ai/apps\">Dify</a> 目前用起來覺得，UI介面最舒服且最無難度的上手no code應用流程建置工具 (但要錢用的模型也要錢)</li>\n",
      "</ul>\n",
      "<h2>問題回饋 🤔</h2>\n",
      "<p>如有問題，歡迎用各種方法直接聯繫我，或在GitHub Issue中提交喔~  </p>\n",
      "<p align=\"center\">\n",
      "  <a href=\"https://www.facebook.com/YCrabbit.0926\">\n",
      "    <img src=\"images/fb_logo.png\" alt=\"FB\" width=\"30\" height=\"30\">\n",
      "  </a>&nbsp;&nbsp;&nbsp;&nbsp;\n",
      "  <a href=\"https://www.instagram.com/ssmc__0926/\">\n",
      "    <img src=\"images/ig_logo.png\" alt=\"Instagram\" width=\"30\" height=\"30\">\n",
      "  </a>&nbsp;&nbsp;&nbsp;&nbsp;\n",
      "  <a href=\"mailto:jason0304050607@gmail.com\">\n",
      "    <img src=\"images/gmail.png\" alt=\"Gmail\" width=\"30\" height=\"30\">\n",
      "  </a>&nbsp;&nbsp;&nbsp;&nbsp;\n",
      "  <a href=\"https://medium.com/@yc_rabbit\">\n",
      "    <img src=\"images/medium.png\" alt=\"medium\" width=\"30\" height=\"30\">\n",
      "  </a>&nbsp;&nbsp;&nbsp;&nbsp;\n",
      "  <a href=\"https://www.linkedin.com/in/%E6%B0%B8%E8%AA%A0-%E9%84%AD-56a3a5284/\">\n",
      "    <img src=\"images/linkin_logo.png\" alt=\"Linkin\" width=\"30\" height=\"30\">\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "<p align=\"center\">\n",
      "  <sub>版權所有© 2024 鄭永誠 YC-Cheng (Jason)。保留所有權力。</sub>\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 這邊以 MarkdownNodeParser 為例，不同檔案可以參考不同的NodeParser \"\"\"\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core import Document\n",
    "from pathlib import Path\n",
    "import markdown\n",
    "\n",
    "parser = MarkdownNodeParser()\n",
    "\n",
    "# 這邊我們先定義一個讀取markdown文件的函數\n",
    "def read_markdown_file(filename):\n",
    "    # Create a Path object for the markdown file\n",
    "    file_path = Path(filename)\n",
    "    \n",
    "    # Open and read the file content\n",
    "    with file_path.open('r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    return content\n",
    "\n",
    "# 讀取我們的README.md文件\n",
    "content = read_markdown_file('README.md')\n",
    "markdown_content = markdown.markdown(content)\n",
    "\n",
    "\n",
    "# 注意，node_parser不是直接吃文本，而是吃list of Document物件\n",
    "documents = Document(text=markdown_content)  # id_ 可選，可以設置為任何標識符\n",
    "\n",
    "\n",
    "# 我直接拿我們的README.md來做範例\n",
    "nodes = parser.get_nodes_from_documents([documents])\n",
    "\n",
    "# 留意我因為文檔太短，所以只有一個node\n",
    "print(\"\\n分割後的文檔：\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f'[part_{i}]：{node.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## ✏️ sentence_transformers - Embedding\n",
    "- 文字必須要經過embedding模型，才能轉換成向量，向量是能進行詞句比對的基礎基礎\n",
    "\n",
    "- 要留意不同的語言可能適合不同Embedding model\n",
    "\n",
    "- 可去Hugging Face上去看各種語言Embedding模型表現排行\n",
    "https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拿一個切完的chunk範例:  義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。\n",
      "\n",
      "句子Embedding 後結果:  [ 2.28983879e-01  2.56072372e-01 -9.01247978e-01  2.16540433e-02\n",
      "  1.61327943e-01 -1.41556514e-02  3.84209663e-01 -1.99013099e-01\n",
      " -1.55311847e+00  6.03017390e-01  2.07268342e-01 -3.16158026e-01\n",
      " -5.27468443e-01 -7.72385418e-01 -7.33864367e-01 -7.74600148e-01\n",
      "  1.51136622e-01 -3.20973635e-01 -3.76673907e-01 -9.55326915e-01\n",
      " -7.29235470e-01  2.18102023e-01 -4.98580933e-01 -1.31339148e-01\n",
      "  7.16233611e-01  3.13105881e-01  3.83338720e-01 -9.17241454e-01\n",
      "  5.43770613e-03  4.03267086e-01  6.08780324e-01 -1.19084668e+00\n",
      " -1.05522287e+00  7.90530562e-01  1.84666634e-01  4.99955505e-01\n",
      "  3.46025735e-01  7.76784360e-01  1.76949695e-01 -9.94610190e-01\n",
      "  9.47135165e-02 -1.31648213e-01 -4.64734465e-01  1.02906168e+00\n",
      "  1.52269915e-01  5.49417913e-01  2.23850280e-01  5.11151791e-01\n",
      " -9.47740257e-01  4.94052619e-01  9.64427516e-02  6.79402590e+00\n",
      " -4.76693422e-01  1.04922771e+00 -7.90978849e-01  9.72206533e-01\n",
      "  3.42917889e-01 -5.14202058e-01 -5.52227795e-01 -1.09890115e+00\n",
      "  6.06656730e-01 -2.06763789e-01  9.25660014e-01  5.71880519e-01\n",
      "  1.79549515e+00 -9.13448334e-02 -1.24704326e-02  1.32983291e+00\n",
      "  4.59197104e-01 -5.19440293e-01  6.95537254e-02 -5.67509949e-01\n",
      "  8.19713548e-02  5.51195741e-01  7.32462257e-02  6.08692288e-01\n",
      " -1.57639995e-01  4.87389863e-01 -5.01100123e-01 -1.35432124e-01\n",
      " -2.16462687e-01  8.28390598e-01 -6.44174695e-01 -6.79190218e-01\n",
      " -9.17770743e-01 -7.31495917e-01 -8.93518925e-02  4.57314968e-01\n",
      "  1.91313431e-01  1.44154274e+00 -1.42295480e+00 -2.68057227e-01\n",
      "  1.14579029e-01 -2.17351794e-01  2.17383459e-01 -3.98314714e-01\n",
      "  1.18135452e+00 -9.69654202e-01 -8.19805980e-01  1.14839874e-01\n",
      "  8.43439624e-02 -4.46013927e-01 -1.04985738e+00  1.11026585e+00\n",
      "  4.03984129e-01  2.52683848e-01 -7.68481851e-01  9.00173128e-01\n",
      " -9.87602115e-01 -2.13365078e-01 -8.90408382e-02  3.37469757e-01\n",
      " -2.66120285e-01 -6.77973628e-01 -7.19923794e-01 -4.22388949e-02\n",
      " -6.09412074e-01 -6.48268700e-01  1.00725733e-01  4.84669983e-01\n",
      " -8.08133110e-02  3.38874251e-01 -1.13249756e-01 -1.11673772e+00\n",
      " -3.28328550e-01 -7.79185176e-01  8.34421277e-01  6.65613590e-03\n",
      " -1.11650310e-01  8.14698219e-01  3.34033281e-01  4.26968247e-01\n",
      " -7.63115346e-01 -5.83797395e-01  1.03431061e-01 -1.31326849e-02\n",
      "  3.96425664e-01  1.58268884e-01 -6.15304768e-01  5.84329605e-01\n",
      "  7.27176130e-01 -4.05582450e-02 -7.23468661e-01 -2.90959090e-01\n",
      "  1.08869195e+00  4.32318747e-01 -5.73399603e-01 -4.26401705e-01\n",
      "  3.01752388e-01 -9.98745263e-01  1.36296451e-01  5.80220342e-01\n",
      "  3.91080119e-02  5.13714194e-01 -8.18436086e-01  4.92333584e-02\n",
      "  3.06380987e-01  8.43698919e-01 -7.82532692e-01  3.71187478e-01\n",
      " -4.84552652e-01 -2.08487418e-02 -1.35828465e-01  6.65398121e-01\n",
      " -5.59358418e-01 -2.11473927e-01 -6.02310658e-01 -9.06792223e-01\n",
      "  3.37291062e-01  1.00760303e-01 -8.62581253e-01  3.20813179e-01\n",
      "  6.80993125e-02 -1.10889208e+00  5.08678734e-01 -7.06488252e-01\n",
      " -1.35493144e-01 -1.24482322e+00 -3.45825881e-01 -8.52520525e-01\n",
      " -8.87598336e-01 -3.29825342e-01  3.42122465e-01 -3.06551725e-01\n",
      "  3.21654946e-01 -4.30989206e-01 -4.75320876e-01  2.83877581e-01\n",
      " -3.32055092e-01  4.44205582e-01 -1.36185205e+00  3.09921741e-01\n",
      " -1.10124040e+00 -1.76254854e-01  6.31675422e-01 -2.11228386e-01\n",
      " -1.79979876e-01 -6.87042847e-02 -8.35012943e-02 -1.63307115e-01\n",
      " -1.41215801e+00  2.83081736e-02  2.03860812e-02  5.73546827e-01\n",
      "  7.04729438e-01  5.89459240e-01 -1.54484168e-01 -5.51276147e-01\n",
      "  8.64627540e-01  1.46901652e-01  1.00844465e-01  1.94061622e-01\n",
      "  2.94846803e-01 -1.30729926e+00 -6.79501355e-01  4.37886536e-01\n",
      "  9.02758017e-02  7.54023433e-01  3.51166278e-01 -5.19018352e-01\n",
      "  7.95009911e-01  7.20091522e-01  3.79859000e-01 -6.26654178e-02\n",
      " -1.07199800e+00 -2.97731489e-01  7.14399874e-01 -6.03019536e-01\n",
      "  3.93624380e-02 -3.72709721e-01  7.82185078e-01  7.21335933e-02\n",
      " -2.81401455e-01  7.96011448e-01  2.95097023e-01  4.27560151e-01\n",
      "  5.43005943e-01  1.67811751e-01  2.45715424e-01  3.64269584e-01\n",
      " -3.93071085e-01  1.80834845e-01 -1.10392675e-01 -6.29622519e-01\n",
      " -3.60046715e-01  5.83150089e-01 -7.74438202e-01 -3.22831482e-01\n",
      " -7.93785378e-02 -5.49381375e-01  7.20990837e-01  4.69995499e-01\n",
      "  7.30452120e-01 -5.73494971e-01  5.70286751e-01 -3.54906231e-01\n",
      " -5.54309674e-02 -4.76375908e-01  3.02376114e-02 -1.92382082e-01\n",
      "  1.47983223e-01  3.16108257e-01  7.14587510e-01  7.08876491e-01\n",
      " -9.99429971e-02 -5.02313972e-02  1.18746150e+00 -7.66541004e-01\n",
      " -6.23566985e-01 -3.20743501e-01  9.84689832e-01  1.65741786e-01\n",
      "  1.02398016e-01 -1.05953431e+00  3.55807273e-03  3.76093239e-01\n",
      "  7.07920492e-01  4.63535309e-01  2.04889461e-01 -3.16536009e-01\n",
      "  7.23513722e-01 -3.34933907e-01 -6.10867798e-01  3.71921480e-01\n",
      " -8.47951531e-01 -1.07437253e+00  5.23948014e-01  9.32193637e-01\n",
      " -6.03058226e-02  3.32392491e-02 -7.08359599e-01  2.89770275e-01\n",
      " -3.18399398e-03 -2.42671803e-01  1.36006033e+00  1.82803795e-01\n",
      " -1.69990078e-01 -3.08354646e-01  2.69800425e-01  8.77564311e-01\n",
      " -9.08751667e-01 -2.30175868e-01 -7.17646599e-01  9.44858015e-01\n",
      " -5.73066413e-01  3.19843441e-01  7.66248927e-02  9.65092182e-02\n",
      "  6.69093549e-01 -3.56529802e-01 -6.76996350e-01  1.11758046e-01\n",
      "  1.05800354e+00  8.03843737e-02 -1.94357109e+00 -5.50368071e-01\n",
      "  1.15692520e+00  5.42090416e-01  4.17277068e-01 -3.31014663e-01\n",
      "  4.91379052e-01  9.64123428e-01  4.56644595e-01  1.13059843e+00\n",
      " -6.22865200e-01  2.52978235e-01 -1.47427547e+00 -7.35909879e-01\n",
      "  5.61069667e-01  3.17381114e-01  5.40003061e-01  4.83703166e-01\n",
      "  3.59067053e-01  6.78282976e-02  3.40646207e-01  1.32406756e-01\n",
      " -2.69079953e-01  7.92798698e-01 -7.54555404e-01 -4.17503983e-01\n",
      "  9.80783924e-02 -1.01373172e+00  5.68809927e-01 -4.26511854e-01\n",
      " -7.98511446e-01 -7.01986194e-01 -4.13340241e-01 -6.25120699e-01\n",
      " -5.68127692e-01 -4.98710871e-01 -5.18991411e-01  2.06280470e-01\n",
      " -1.58829272e+00 -1.42106578e-01 -6.06289506e-01  8.06060195e-01\n",
      "  2.94498295e-01  3.30647737e-01  1.05005637e-01  4.21932071e-01\n",
      "  4.21716303e-01 -5.35401642e-01 -3.27707469e-01  1.14614642e+00\n",
      "  2.78039187e-01 -1.57868396e-02 -4.57220018e-01  1.71659682e-02\n",
      "  8.03962827e-01 -4.48502488e-02  7.60200977e-01  3.63844007e-01\n",
      "  1.05657804e+00 -4.48222458e-01  5.36401391e-01 -9.10385609e-01\n",
      "  6.57564521e-01  4.65733856e-01  4.03462827e-01  3.92999388e-02\n",
      " -3.21336180e-01 -6.97459579e-01  3.69266689e-01 -2.21168995e-01\n",
      " -5.87294102e-01  4.35003042e-01  1.45511627e-01 -4.76845413e-01\n",
      "  1.52165502e-01 -1.66959554e-01 -2.62042016e-01  2.98170326e-03\n",
      " -5.50271869e-01 -6.63900495e-01 -4.24086422e-01 -3.98939908e-01\n",
      "  7.41499960e-01  1.87120125e-01  5.54404378e-01  1.19173610e+00\n",
      "  9.29890513e-01 -1.19623089e+00  2.85319000e-01  4.43135470e-01\n",
      "  9.58068788e-01 -1.02139688e+00 -3.27790350e-01  3.18432033e-01\n",
      "  2.46527299e-01 -9.34749484e-01 -1.38131127e-01 -2.78571159e-01\n",
      " -1.83066392e+00 -7.00166643e-01  9.11881924e-02  4.47649568e-01\n",
      " -3.02254230e-01 -4.62673694e-01  1.40189457e+00 -6.54947281e-01\n",
      "  5.21104157e-01 -2.37438634e-01  5.05414128e-01  3.64596277e-01\n",
      "  1.47723019e+00  4.39141318e-02 -1.60518676e-01  4.41877633e-01\n",
      "  5.93543112e-01 -3.24181229e-01  5.29660918e-02 -3.81351560e-01\n",
      " -5.88393630e-03 -6.22440338e-01  1.52262795e+00  9.59681809e-01\n",
      " -1.05775392e+00 -4.19774473e-01 -3.50890696e-01 -1.29918492e+00\n",
      " -2.06451818e-01  3.81263465e-01  2.47238860e-01 -3.74011070e-01\n",
      " -4.95566279e-01 -4.08194721e-01  4.34463084e-01 -6.87392533e-01\n",
      "  3.66606623e-01 -2.65736669e-01 -6.72835290e-01  6.90058529e-01\n",
      "  6.69061780e-01 -5.96119948e-02 -2.44348362e-01 -9.35847387e-02\n",
      "  4.69023526e-01 -1.01177049e+00  3.60498726e-02  1.01141369e+00\n",
      "  3.10195535e-01 -6.35333419e-01  4.71915185e-01  3.74588937e-01\n",
      "  8.85561943e-01 -1.88142672e-01  7.60721564e-01  4.32797447e-02\n",
      "  3.71356815e-01  4.10823017e-01  7.65911996e-01  7.26459175e-02\n",
      "  1.87113732e-01 -6.26152456e-01 -1.37504920e-01 -4.51816261e-01\n",
      " -1.61126927e-01 -1.93060979e-01 -8.72158587e-01 -3.36037725e-01\n",
      " -7.54297793e-01  8.74253392e-01  1.94195464e-01  2.47314483e-01\n",
      " -6.81042969e-01  5.08773208e-01  1.39179662e-01 -5.47051311e-01\n",
      " -2.11015776e-01 -7.21510768e-01 -5.59090614e-01 -2.08681658e-01\n",
      "  7.88952291e-01 -1.24764574e+00  4.54458259e-02 -7.23744035e-01\n",
      "  1.02715380e-01  1.00032173e-01 -1.16069591e+00 -7.87228644e-01\n",
      "  2.78895319e-01 -1.21883082e+00  5.81829488e-01  8.40145815e-03\n",
      " -4.71131243e-02  3.43617231e-01  7.52270162e-01  8.76216218e-02\n",
      " -1.15081501e+00  3.52889836e-01 -1.23902254e-01 -1.20307215e-01\n",
      " -8.12716186e-01 -8.13951194e-01  3.60443085e-01 -2.23673269e-01\n",
      " -1.99264213e-01  4.94595289e-01  7.57702053e-01 -1.49178490e-01\n",
      " -2.56736547e-01 -3.15526989e-03 -1.91697981e-02  3.63498539e-01\n",
      " -9.09169793e-01 -3.14757138e-01 -4.14069563e-01 -1.13592827e+00\n",
      " -5.55058897e-01  8.39998126e-01 -3.30079585e-01  1.98440984e-01\n",
      "  3.35699320e-01 -7.45958760e-02 -1.75155193e-01 -3.73713113e-02\n",
      "  1.02519107e+00 -1.04319692e+00  1.65777236e-01 -1.37534046e+00\n",
      " -6.56153023e-01  1.70044065e-01  7.79447734e-01 -1.84647202e+00\n",
      " -8.38692367e-01 -6.78645551e-01 -6.34669125e-01 -4.00879174e-01\n",
      "  3.83149058e-01  7.06601024e-01 -6.77478671e-01  1.01493441e-01\n",
      " -6.49584591e-01  1.30030477e+00  1.68863852e-02 -7.16911316e-01\n",
      " -3.79534990e-01 -1.15196967e+00  6.87451780e-01 -6.41097546e-01\n",
      "  2.30632186e-01  9.66227531e-01 -3.64652216e-01 -1.37695242e-02\n",
      " -8.60143542e-01 -2.34382629e-01  3.52514796e-02  1.13427326e-01\n",
      " -5.99935472e-01  4.99804586e-01  2.20731333e-01 -3.29566181e-01\n",
      " -2.66554981e-01  1.30380714e+00  2.55829662e-01  5.56493774e-02\n",
      " -8.36409032e-01 -9.34826210e-02  1.85068965e-01 -7.46844828e-01\n",
      "  1.18279636e+00 -8.29712152e-02  2.50250995e-01 -3.15950453e-01\n",
      " -7.42999971e-01 -9.74582136e-01  7.81636178e-01 -1.43903971e-01\n",
      "  1.33652547e-02  6.98494852e-01 -2.81975031e-01  2.45306775e-01\n",
      "  3.70142698e-01  9.40347254e-01 -6.92120969e-01 -5.84740937e-01\n",
      "  9.86576438e-01  4.25285622e-02  4.72607553e-01  3.56950283e-01\n",
      " -1.79447681e-01 -1.62070155e+00 -4.75838691e-01  2.90967613e-01\n",
      "  1.74555570e-01 -9.78852630e-01  1.56121314e+00  4.06875536e-02\n",
      "  1.75793022e-02 -8.17005157e-01 -1.60116208e+00  1.62118626e+00\n",
      "  1.31026971e+00 -4.21579361e-01  6.17033660e-01 -3.49153191e-01\n",
      "  1.05307817e-01  2.10978293e+00 -3.21212232e-01 -2.48126671e-01\n",
      " -6.97696984e-01  2.99311787e-01 -2.47632161e-01  8.89310986e-02\n",
      " -3.08613926e-01  7.74717033e-01  1.73597324e+00  2.70739883e-01\n",
      "  5.88790476e-01 -4.09322590e-01 -6.67287037e-02 -8.25061798e-01\n",
      "  6.35225713e-01 -1.31599098e-01 -1.64048448e-01 -1.36590570e-01\n",
      " -2.67096788e-01  6.95242763e-01 -2.85616338e-01  1.04406632e-01\n",
      "  7.32901871e-01 -1.11537480e+00 -2.99391430e-02 -7.11095557e-02\n",
      "  3.84500831e-01 -2.91283429e-01  1.13532174e+00 -7.69353285e-02\n",
      "  9.72105682e-01 -5.53142250e-01  8.82899404e-01 -2.12529048e-01\n",
      "  5.71933329e-01 -5.30772746e-01  4.13656056e-01  7.78598368e-01\n",
      "  9.83355880e-01 -1.09876990e-01 -8.88568580e-01 -8.73118639e-03\n",
      "  2.85082966e-01 -2.26746246e-01  1.15472186e+00  4.96865250e-02\n",
      " -6.42326534e-01 -6.33052945e-01  5.21015823e-01  3.78032327e-02\n",
      "  6.30678833e-01  3.84100884e-01  1.17220175e+00 -2.83609275e-02\n",
      " -5.81505895e-02 -6.62314475e-01 -4.45885450e-01  6.34800792e-01\n",
      "  5.55745006e-01 -9.56816792e-01 -9.37782347e-01  5.42472482e-01\n",
      " -5.01666330e-02  3.70286167e-01  1.47362387e+00 -1.05780816e+00\n",
      "  2.24842072e-01  1.81411635e-02 -3.50180656e-01  1.36604443e-01\n",
      " -4.46446866e-01  7.72535682e-01 -4.01252843e-02  3.60917747e-01\n",
      "  4.43161488e-01 -1.20529675e+00 -2.73842841e-01  1.91148376e+00\n",
      "  9.35735822e-01  1.36875451e+00 -8.54405940e-01 -2.40111426e-01\n",
      "  1.16590583e+00  6.75195634e-01 -1.45467997e-01 -6.11577988e-01\n",
      " -1.76525921e-01  2.92310387e-01 -5.08492813e-02  3.90944421e-01\n",
      " -4.75582659e-01  1.18603602e-01 -6.91619933e-01  7.86875665e-01\n",
      "  2.40851894e-01 -1.61634937e-01 -3.91967744e-01 -5.13691425e-01\n",
      " -9.73029971e-01  9.94275153e-01  1.00678988e-01  8.94208789e-01\n",
      " -5.49338937e-01 -9.70453560e-01  1.81251794e-01 -3.48367512e-01\n",
      "  5.07725894e-01  4.93448406e-01 -2.85238951e-01  1.23530185e+00\n",
      "  9.64541018e-01  1.02378738e+00 -7.12806404e-01  3.67551625e-01\n",
      "  4.04026769e-02 -4.90248382e-01 -4.47823703e-01  5.33294976e-01\n",
      "  8.74980628e-01 -3.30916941e-01 -1.37265101e-01  1.97009712e-01\n",
      "  1.67133927e-01 -7.86473975e-02  1.04524887e+00 -8.20405960e-01\n",
      " -8.03855121e-01 -1.81519389e-02 -7.49858692e-02 -3.04491222e-01\n",
      " -3.68479729e-01 -3.91175032e-01 -5.10117888e-01  4.61999804e-01\n",
      "  1.26129127e+00  7.62649113e-03 -1.99084971e-02 -3.49048585e-01\n",
      " -3.06297809e-01  1.81877941e-01 -5.63747883e-01  9.87228096e-01\n",
      " -8.68583798e-01  1.66859820e-01  2.24286675e-01  3.50840867e-01\n",
      " -1.96403965e-01  3.05609047e-01 -5.48548937e-01 -5.38150132e-01]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Embedding方法示意，需求套件含其他介紹可參考C4內容 \"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# 載入模型 (已選擇中文擅長模型)\n",
    "model = SentenceTransformer('DMetaSoul/sbert-chinese-general-v2')  \n",
    "\n",
    "# 自定義一個文件\n",
    "documents = \"義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪跟維爾康在南極匯合。\"\n",
    "\n",
    "# 這邊我們先拿一個切完的chunk範例，取第[0]個\n",
    "text = node_parser.get_nodes_from_documents([Document(text=documents)], show_progress=False)[0].text\n",
    "print(\"拿一個切完的chunk範例: \", text)\n",
    "\n",
    "embedding = model.encode(text, convert_to_tensor=False)\n",
    "print(\"\\n句子Embedding 後結果: \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子Embedding 後結果: \n",
      " [-0.011561819352209568, -0.027052124962210655, -0.04666323587298393, 0.011066942475736141, -0.004675247706472874, -0.018668048083782196, 0.0036121420562267303, -0.06413348764181137, 0.0027929607313126326, -0.022903883829712868, 0.0026516614016145468, 0.03054366447031498, -2.510498916308279e-06, -0.00016913011495489627, -0.05982282757759094, -0.030154919251799583, -0.0007710297941230237, -0.08172185719013214, 0.008165169507265091, -0.011992832645773888, -0.020475029945373535, 0.0741274356842041, 0.0034678285010159016, -0.005354230757802725, -0.03807832673192024, 0.021782169118523598, 0.026178833097219467, 0.0022571899462491274, -0.06470903754234314, -0.007674641441553831, -0.008953011594712734, 0.026491599157452583, -0.014211811125278473, -0.010663866065442562, -0.046543676406145096, 0.008823500014841557, 0.020171254873275757, 0.012029903009533882, 0.007141258101910353, 0.004886055365204811, -0.026562534272670746, -0.007124161347746849, 0.06844864785671234, -0.04894967004656792, 0.03900296241044998, 0.013416550122201443, 0.020304055884480476, 0.010279509238898754, 0.040664538741111755, -0.022768020629882812, -0.02858596108853817, 0.2903275191783905, -0.0462726391851902, -0.0013054433511570096, -0.012656529434025288, -0.002252447884529829, 0.007995052263140678, -0.013228281401097775, 0.04161061346530914, -0.025341058149933815, 0.05012786388397217, -0.024150613695383072, 0.03174829110503197, 0.007918843068182468, 0.017964836210012436, -0.03303089365363121, 0.04121846333146095, -0.04849083349108696, -0.011187086813151836, 0.008810607716441154, 0.0006915375124663115, -0.020649058744311333, 0.017697760835289955, 0.053696099668741226, 0.03236889839172363, 0.008951827883720398, 0.017643047496676445, 0.0669298842549324, -0.035864826291799545, -0.05476014316082001, 0.026002753525972366, 0.0027514121029525995, 0.031194478273391724, -0.036002255976200104, -0.010527164675295353, 0.04799577593803406, -0.029453566297888756, 0.054507531225681305, 0.03332154080271721, 0.04553559795022011, -0.08828437328338623, 0.06207292154431343, -0.010196569375693798, -0.0202756579965353, 0.04536880925297737, -0.015071953646838665, 0.02469933032989502, -0.023173382505774498, -0.03166624531149864, 0.011592143215239048, -0.0071009136736392975, -0.06313911825418472, -0.010424342937767506, 0.02871692180633545, -0.018587125465273857, 0.0066943238489329815, -0.0065238638781011105, 0.021763116121292114, -0.020416896790266037, -0.01979893445968628, -0.026730824261903763, -0.04707272723317146, 0.0038021118380129337, -0.019875727593898773, -0.00031537035829387605, -0.014025385491549969, 0.032694458961486816, -0.023359639570116997, -0.030692001804709435, -0.002096255077049136, -0.002640632214024663, 0.08316704630851746, 0.014814547263085842, 0.005487553309649229, -0.020131466910243034, -0.010610053315758705, -0.0072639151476323605, 0.0013049656990915537, -0.031116757541894913, -0.04256386309862137, -0.027457885444164276, 0.04156523197889328, -0.00957560632377863, 0.0022971355356276035, -0.013826378621160984, -0.0383271649479866, -0.008602292276918888, -0.02486877515912056, 0.0060693188570439816, 0.052352722734212875, 0.10530095547437668, 0.026666345074772835, 0.004588663578033447, -0.04045255109667778, -0.028579751029610634, 0.06871190667152405, -0.020462816581130028, -0.0018762231338769197, -0.044254906475543976, -0.014890432357788086, -0.002931027440354228, -0.022486601024866104, 0.015236801467835903, 0.04243674501776695, -0.018604738637804985, 0.021533820778131485, 0.07161160558462143, 0.02392902970314026, -0.0036969822831451893, 0.010400665923953056, 0.015060205943882465, -0.02364501543343067, 0.06373856216669083, 0.011216700077056885, 0.03339463472366333, -0.03784815967082977, -0.049333129078149796, 0.001170608215034008, 0.07609361410140991, 0.0376100018620491, -0.04923813045024872, -0.0045259371399879456, -0.004933709744364023, 0.03444823995232582, 0.009613294154405594, 0.02709108591079712, -0.016534389927983284, -0.0012555072316899896, -0.012232037261128426, 0.0184490866959095, -0.03963945433497429, -0.008205465972423553, 0.07041794061660767, 0.026828646659851074, 0.08523717522621155, -0.03518412262201309, -0.07186338305473328, 0.019192930310964584, 0.006626592017710209, 0.02674834430217743, -0.03550465777516365, -0.0013757178094238043, -0.013057096861302853, -0.0062597063370049, -0.019954103976488113, -0.017480608075857162, -0.035805780440568924, -0.07545068114995956, 0.0010784772457554936, -0.009245357476174831, -0.008423106744885445, 0.01539647113531828, 0.014308828860521317, 0.05178559571504593, 0.023435762152075768, 0.03215420991182327, -0.04819147661328316, 0.01721012406051159, 0.003728716867044568, -0.0037865189369767904, 0.07040518522262573, 0.041762664914131165, -0.004375574644654989, -0.0370553694665432, -0.01004771701991558, 0.0021845216397196054, -0.045305561274290085, 0.037505023181438446, 0.015424637123942375, -0.017921103164553642, 0.012508748099207878, 0.0587686151266098, 0.035523295402526855, -0.04261472448706627, -0.00963734369724989, 0.03339588642120361, -0.0014275640714913607, -0.025642236694693565, 0.028270578011870384, -0.004541629459708929, -0.011377034708857536, 0.00869061890989542, 0.006908002775162458, 0.05967477336525917, 0.010946773923933506, 0.00671040453016758, -0.03121929056942463, -0.020138775929808617, 0.06175146624445915, -0.007941381074488163, 0.008025340735912323, 0.004464768338948488, 0.06734084337949753, -0.028032982721924782, -0.010242464952170849, 0.05282130837440491, -0.0320470929145813, -0.035375453531742096, -0.014706800691783428, -0.11296148598194122, -0.010144493542611599, 0.027073832228779793, 0.016154926270246506, -0.013454803265631199, 0.033632077276706696, 0.028015464544296265, -0.01387886144220829, 0.01242483127862215, 0.08161015808582306, -0.011350465007126331, -0.027807094156742096, 0.029775694012641907, -0.030016984790563583, -0.010592697188258171, 0.006519859191030264, -0.07447449117898941, -0.021925656124949455, -0.06686760485172272, -0.04216967150568962, -0.0005988500197418034, -0.011450122110545635, 0.0008474719361402094, 0.04536425694823265, 0.029476169496774673, -0.001981947338208556, 0.05755879729986191, -0.009074336849153042, -0.022215673699975014, 0.03272338584065437, -0.0954827293753624, 0.01872067153453827, 0.019006967544555664, -0.004253257531672716, 0.05760910362005234, 0.020731138065457344, 0.002525812014937401, 0.057187218219041824, -0.026298830285668373, 0.006230838131159544, -0.027714649215340614, 0.0036878420505672693, -0.01724347285926342, 3.85995845135767e-05, -0.03038691356778145, 0.2855892777442932, -0.03764152526855469, 0.034204933792352676, 0.021107133477926254, 0.04685352370142937, 0.07355135679244995, 0.001386582967825234, 0.012010116130113602, 0.031955305486917496, 0.05976700410246849, -0.05668102949857712, 0.05663222074508667, -0.00047876889584586024, -0.004693477414548397, -0.02397458255290985, -0.015757666900753975, -0.018921224400401115, -0.01734125427901745, 0.016906382516026497, 0.000732030370272696, 0.008984028361737728, -0.005052247550338507, -0.001137746381573379, 0.03809066489338875, -0.026172205805778503, -0.015519306994974613, 0.021830575540661812, 0.0009463251335546374, 0.019024524837732315, 0.01942307874560356, 0.02448630891740322, 0.02339295856654644, -0.050248757004737854, -0.04437439143657684, 0.012537878006696701, 0.01344354823231697, 0.0559082105755806, -0.030602293089032173, 0.020584166049957275, -0.009321666322648525, -0.0025743634905666113, -0.026682954281568527, -0.04644379764795303, 0.028590796515345573, 0.06584249436855316, -0.002805260941386223, 0.02955416776239872, -0.02671664021909237, 0.0416320338845253, -0.023706257343292236, -0.0025375455152243376, -0.008149690926074982, -0.029720159247517586, 0.013001976534724236, -0.047970108687877655, -0.050742603838443756, -0.04160116985440254, -0.00743082957342267, -0.08091475814580917, 0.013043045066297054, 0.00964327435940504, -0.013405214995145798, -0.038708120584487915, 0.008053090423345566, -0.03341091424226761, -0.014212315902113914, 0.0034347493201494217, -0.03740417957305908, -0.015400169417262077, 0.005354022607207298, -0.029386287555098534, 0.0072771054692566395, 0.003872009227052331, 0.06268597394227982, 0.013866350054740906, -0.011156465858221054, -0.028667723760008812, -0.01709352247416973, -0.059203457087278366, -0.008064376190304756, 0.007644337601959705, -0.0300311129540205, -0.0015977277653291821, 0.004250688944011927, 0.04385216906666756, 0.04979259893298149, 0.019210249185562134, 0.012749833054840565, 0.03185632824897766, 0.06513327360153198, -0.0529320165514946, 0.0048023066483438015, -0.02773776836693287, 0.0016707989852875471, -0.057278793305158615, 0.018315738067030907, 0.00012302245886530727, 0.0031933661084622145, 0.005886004772037268, 0.00995495542883873, 0.011141164228320122, -0.03888379782438278, -0.007784456014633179, 0.002685500541701913, 0.00138052215334028, -0.016171777620911598, -0.013267068192362785, -0.05069638416171074, 0.0897059291601181, -0.008037636056542397, -0.0044144028797745705, 0.028866209089756012, -0.02400718443095684, -0.034466542303562164, 0.032004985958337784, 0.028862450271844864, 0.02056061662733555, -0.03833059221506119, -0.09724868088960648, 0.016057433560490608, -0.013995825313031673, 0.024225974455475807, -0.020811116322875023, -0.028169384226202965, -0.01355768647044897, -0.06675276160240173, -0.01143233198672533, -0.012579794973134995, -0.024624843150377274, -0.008639812469482422, 0.008850688114762306, 0.021479342132806778, 0.030862288549542427, 0.04297048971056938, 0.05313681811094284, -0.018739741295576096, 0.011127786710858345, -0.043604955077171326, 0.032725024968385696, -0.03356212377548218, -0.018944382667541504, -0.014492856338620186, 0.011367185041308403, -0.04523707553744316, 0.010116524063050747, -0.019064929336309433, -0.030491620302200317, 0.0676056519150734, 0.001727525144815445, -0.04423778876662254, 0.013750122860074043, 0.003838347038254142, 0.014947588555514812, -0.033635593950748444, -0.02889740839600563, 0.029042208567261696, -0.01053394190967083, 0.00563165545463562, 0.010631670244038105, -0.005678591784089804, -0.007379844319075346, -0.0011447331635281444, 0.01551717147231102, 0.00630059652030468, -0.019697245210409164, 0.05561964213848114, 0.009244547225534916, 0.030723270028829575, 0.024123361334204674, -0.007833776995539665, -0.02382379025220871, -0.013706213794648647, 0.018419994041323662, -0.02790623903274536, -0.01857438124716282, 0.10234534740447998, -0.0413498617708683, -0.028969036415219307, 0.04152826592326164, -0.0026099616661667824, 0.05140482261776924, 0.012238975614309311, 0.012046217918395996, 0.009900243952870369, -0.08221370726823807, -0.024152090772986412, -0.012136287987232208, -0.00026513769989833236, -0.01221041101962328, 0.008427045308053493, -0.05325644463300705, -0.013105346821248531, 0.015434966422617435, -0.05050124228000641, 0.003972530364990234, -0.047801751643419266, -0.004865622613579035, 0.0021327142603695393, -0.02534995973110199, -0.015300209634006023, 0.006842154078185558, -0.04393139109015465, 0.007152861449867487, -0.04689158871769905, -0.007482354994863272, -0.029505955055356026, -0.011341647244989872, -0.011141437105834484, 0.024324843659996986, 0.018470095470547676, -0.06419683247804642, -0.02762734144926071, -0.0010270103812217712, -0.021481066942214966, 0.002077166922390461, -0.011051724664866924, -0.033344849944114685, -0.016661016270518303, -0.01220591552555561, -0.013517186045646667, -0.013815753161907196, 0.020443478599190712, 0.010261059738695621, 0.056278664618730545, -0.0022701802663505077, -0.02328762412071228, 0.0028212261386215687, 0.028528613969683647, -0.04279112070798874, -0.012714485637843609, 0.0003295732894912362, 0.00016178628720808774, -0.016565119847655296, -0.014478063210844994, 0.04416370019316673, 0.01915935054421425, 0.013870608061552048, -0.027892425656318665, -0.023909274488687515, 0.023102594539523125, -0.016980739310383797, 0.041315313428640366, 0.005456562619656324, -0.019145140424370766, -0.006899573840200901, 0.01589188538491726, -0.018812309950590134, -0.024334322661161423, -0.02164360322058201, 0.0012250029249116778, 0.019833160564303398, -0.018730709329247475, 0.019523577764630318, -0.02203090488910675, -0.03172415867447853, 0.015667656436562538, -0.029337212443351746, -0.03696691244840622, -0.028246141970157623, -0.013658244162797928, -0.05486514791846275, -0.028465185314416885, -0.08525233715772629, -0.04133901745080948, 0.05329464003443718, -0.06176320090889931, -0.013110999949276447, 0.0075174253433942795, -0.025339806452393532, -0.007960841059684753, -0.0053676096722483635, -0.03289986401796341, 0.006784206721931696, 0.002118353033438325, -0.012227319180965424, 0.04580873250961304, 0.013818309642374516, -0.06782707571983337, -0.0007490686839446425, -0.023548437282443047, 0.027314567938447, -0.03650723397731781, -0.03442404419183731, -0.017792928963899612, -0.046007510274648666, -0.023667164146900177, 0.04997466132044792, -0.0009977738372981548, 0.011871326714754105, -0.0007765699410811067, -0.011236780323088169, 0.0010599090019240975, -0.007249258458614349, 0.03901311010122299, -0.0041281539015471935, -0.06627252697944641, -0.005907135549932718, 0.05399549752473831, 0.051833681762218475, 0.008954286575317383, -0.06464537978172302, -0.005877338815480471, -0.014474830590188503, -0.013286316767334938, 0.038930390030145645, 0.03367571160197258, -0.05783923342823982, -0.10369442403316498, -0.005566870793700218, -0.019245872274041176, -0.028735563158988953, 0.020505940541625023, 0.006119918543845415, -0.01117356400936842, -0.06188618019223213, -0.0608532689511776, -0.02181163616478443, 0.1016368567943573, 0.01482225302606821, -0.014526120387017727, -0.010874580591917038, 0.01299627311527729, -0.031999342143535614, 0.001565528684295714, -0.052075501531362534, 0.008448205888271332, 0.06119264289736748, -0.061782125383615494, -0.011982803232967854, 0.05079256743192673, -0.009720347821712494, -0.02833724394440651, 0.04643901437520981, 0.0003836864489130676, 0.010342605412006378, 0.014022568240761757, -0.010509726591408253, -0.029982412233948708, 0.021288635209202766, -0.0003884714387822896, 0.032766420394182205, 0.02424134500324726, -0.0605606734752655, 0.015031511895358562, -0.006266540847718716, -0.02911841869354248, 0.04338182881474495, -0.008644687943160534, -0.01775440014898777, 0.03268162161111832, -0.026874791830778122, -0.006036336533725262, -0.029406720772385597, 0.007156678009778261, 0.012426245957612991, -0.05763754993677139, -0.006874642800539732, -0.01955454796552658, -0.011110718362033367, 0.045456454157829285, -0.05301104858517647, -0.039059653878211975, 0.003280260134488344, -0.0395657941699028, -0.0038107563741505146, 0.0025044616777449846, -0.0027103694155812263, -0.024585632607340813, 0.031203974038362503, -0.0167014691978693, -0.006076865829527378, -0.04705144837498665, 0.012785824947059155, -0.020212670788168907, 0.03315870836377144, 0.023454567417502403, 0.059019844979047775, -0.004454993177205324, -0.038270626217126846, -0.021953269839286804, -0.010769467800855637, 0.002002241089940071, 0.006524717900902033, -0.017341818660497665, -0.11020704358816147, 0.041470568627119064, 0.005912069231271744, -0.015110394917428493, -0.025987252593040466, -0.03882790356874466, -0.028704561293125153, -0.005904767196625471, 0.0011049893219023943, -0.004559246823191643, -0.013557448983192444, 0.0008253088453784585, 0.007911188527941704, -0.009741452522575855, 0.004527045413851738, -0.010411166585981846, 0.026595979928970337, 0.11063005775213242, 0.008615860715508461, 0.04878208041191101, -0.012581988237798214, 0.003387546632438898, 0.052443116903305054, 0.021218182519078255, 0.00567011209204793, 0.04201272875070572, -0.023823127150535583, -0.03985241428017616, 0.04232129827141762, -0.021867895498871803, 0.0009162627393379807, -0.0019803852774202824, 0.028711218386888504, -0.05736854299902916, 0.007948408834636211, -0.020457673817873, 0.016154205426573753, 0.004888987168669701, -0.014388923533260822, 0.09418614953756332, 0.055651646107435226, 0.05872873589396477, -0.03865791857242584, 0.0012766667641699314, -0.022048765793442726, -0.006267262157052755, 0.006756598129868507, 0.06184358522295952, -0.010862348601222038, 0.007284052204340696, 0.029177676886320114, -0.0409400649368763, -0.024574579671025276, -0.0238046757876873, 0.01641080155968666, -0.06988301128149033, -0.07642167806625366, -0.023587243631482124, -0.03262002393603325, -0.020834723487496376, 0.042631689459085464, 0.07691100984811783, -0.022649146616458893, -0.0013625789433717728, 0.022278988733887672, 0.06439239531755447, -0.022139471024274826, 0.02348487079143524, 0.0019441237673163414, 0.01809445582330227, -0.004815110471099615, 0.006968060042709112, -0.031331758946180344, -0.029851408675312996, 0.02252710983157158, 0.00042231366387568414, -0.007761799730360508, -0.04677807167172432, -0.03710007667541504, 0.0006855846149846911, -0.01788300648331642, -0.0028164538089185953, 0.03209121897816658, 0.05728770047426224, 0.024419451132416725, -0.004346069414168596, 0.0010268395999446511, 0.018984485417604446, -0.010842930525541306, -0.004465245176106691]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "補充: 當然你也可以利用langchain_community從llama_index上\n",
    "直接導入使用HuggineFace上的其他模型\n",
    "以下為範例\n",
    "\"\"\"\n",
    "\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install llama-index-embeddings-instructor -q\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# 使用HuggingFaceEmbedding上的模型\n",
    "model_from_hf = HuggingFaceEmbedding(model_name=\"DMetaSoul/Dmeta-embedding-zh-small\")\n",
    "\n",
    "# 注意，HuggingFaceEmbeddings舊的獲取向量函數是get_text_embedding不是embed_query\n",
    "# 拿剛剛切好的chunk範例\n",
    "embedding = model_from_hf.get_text_embedding(text)\n",
    "print(\"句子Embedding 後結果: \\n\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## ✏️ Extractors & Datapipeline\n",
    "- 有些時候，會希望轉換成向量之前，還需要將文句進行進一步的摘要、總結、判斷title，我們稱之為 Extractors\n",
    "\n",
    "- 也就在文本要被轉成向量存儲之前，可以先利用LLM將其添加、整理、生成相關資訊\n",
    "\n",
    "- 因為可能會有多個Extractor處理需求，我們會希望建立一個資料處理流程\n",
    "\n",
    "- 這個處理流程可能包含文檔切割(TokenTextSplitter)、標題擷取(TitleExtractor)、關聯的問題回答擷取(QuestionsAnsweredExtractor)，甚至更多\n",
    "\n",
    "- 我們可以把這些流程串聯起來(以下叫做transformations)\n",
    "\n",
    "- 甚至流程可以整合transformations, 存到向量資料庫裡...，建立完整的IngestionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: 97b96d61-ebb6-453d-a8fa-eeaf8da8d5f0\n",
      "Text:\n"
     ]
    }
   ],
   "source": [
    "# \"\"\" 從sample_pdfs讀取所有相關pdf建立成documents\"\"\"\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "# from llama_index.readers.file import PDFReader\n",
    "\n",
    "\n",
    "# # documents = SimpleDirectoryReader(\"./datasets/sample_pdfs\").load_data()\n",
    "\n",
    "# parser  = PDFReader()\n",
    "# file_extractor = {\".pdf\": parser}\n",
    "# documents = SimpleDirectoryReader(\"./datasets/sample_pdfs\", file_extractor=file_extractor).load_data()\n",
    "# print(documents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原本文檔：\n",
      "義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到\n",
      "挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至\n",
      "於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲\n",
      "野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪\n",
      "跟維爾康在南極匯合。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = \"義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪跟維爾康在南極匯合。\"\n",
    "\n",
    "print(\"原本文檔：\")\n",
    "print(textwrap.fill(documents, width=50))\n",
    "\n",
    "documents = [Document(text=documents)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "建立資料多個處理流程範例\n",
    "若檔案過大留意會跑比較久\n",
    "可以transformations先只放1種Extractor\n",
    "看看效果Extractor效果\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import ollama\n",
    "import nest_asyncio\n",
    "\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    SummaryExtractor,\n",
    "    KeywordExtractor,\n",
    "    BaseExtractor,\n",
    ")\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "\n",
    "# 避免巢狀(嵌套)循環判斷問題\n",
    "nest_asyncio.apply()\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "# 建立一個LLM model 一樣 by Groq\n",
    "groq_llm = Groq(model=\"llama-3.1-70b-versatile\", api_key=api_key, temperature=0.1)\n",
    "\n",
    "\n",
    "# 官網範例，也能自定義一個自己的extractor\n",
    "class CustomExtractor(BaseExtractor):\n",
    "    def extract(self, nodes):\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"custom\": (\n",
    "                    node.metadata[\"document_title\"]\n",
    "                    + \"\\n\"\n",
    "                    + node.metadata[\"excerpt_keywords\"]\n",
    "                )\n",
    "            }\n",
    "            for node in nodes\n",
    "        ]\n",
    "        return metadata_list\n",
    "\n",
    "\n",
    "# 以下為重點，我們建立一個 transformations:List ，裡面包含了所有的文字處理流程\n",
    "# 這邊我們使用了TokenTextSplitter, TitleExtractor, QuestionsAnsweredExtractor(全跑要很久)\n",
    "transformations = [\n",
    "    TokenTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=64),\n",
    "    TitleExtractor(nodes=3, llm=groq_llm),\n",
    "    QuestionsAnsweredExtractor(questions=3, llm=groq_llm),\n",
    "    SummaryExtractor(summaries=[\"prev\", \"self\"], llm=groq_llm),\n",
    "    KeywordExtractor(keywords=5, llm=groq_llm),\n",
    "    # CustomExtractor()\n",
    "]\n",
    "\n",
    "\n",
    "# 利用IngestionPipeline來執行所有的剛剛定義transformations\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "\n",
    "# 這邊我們使用剛剛定義的documents，並且執行pipeline\n",
    "nodes = pipeline.run(documents=documents)\n",
    "\n",
    "# nodes為一個list，裡面包含了所有的處理過後的文檔，我的範例只有一個文檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextNode(id_='f5a9673a-b91c-43db-aba5-41441c162115', embedding=None, metadata={'document_title': 'A very interesting and diverse set of keywords!\\n\\nAfter analyzing the keywords, I would suggest a comprehensive title that encompasses the various themes and topics:\\n\\n**\"百科全書：科學、歷史、地理、政治與文化的交匯\"**\\n\\nTranslation: \"Encyclopedia: The Convergence of Science, History, Geography, Politics, and Culture\"\\n\\nThis title captures the essence of the diverse range of topics, from science and mathematics (勾股定理, 三角函數) to history (秦始皇, 東條英機) and geography (太平洋, 南極), as well as politics (川普) and culture (義大利麵, 維爾康). The title also hints at the idea of a comprehensive and interconnected world, which is reflected in the diverse set of keywords.', 'questions_this_excerpt_can_answer': 'What a fascinating and bizarre excerpt!\\n\\nBased on the context, I\\'ve generated three questions that this text can provide specific answers to, which are unlikely to be found elsewhere:\\n\\n1. **What is the author\\'s unconventional opinion on the ideal sauce for spaghetti?**\\n\\nThis question is directly related to the excerpt, where the author suggests that spaghetti should be mixed with \"42號混泥土\" (a type of concrete), which is a humorous and nonsensical answer.\\n\\n2. **How does the author think the Pythagorean theorem (勾股定理) relates to the capture of wild \"三角函數\" (trigonometric functions) by a domesticated \"東條英機\" (a Japanese historical figure)?**\\n\\nThis question highlights the author\\'s creative and illogical connection between mathematical concepts and historical figures, making it a unique and humorous answer.\\n\\n3. **What is the author\\'s tongue-in-cheek explanation for the potential environmental impact of Trump\\'s policies on the Pacific Ocean and the Antarctic region?**\\n\\nThis question captures the author\\'s satirical tone and absurd reasoning, which links Trump\\'s policies to nuclear pollution, UFOs, and the convergence of Walmart and Vicon (維爾康) in Antarctica.\\n\\nThese questions showcase the playful and whimsical nature of the excerpt, which blends seemingly unrelated concepts and ideas in a humorous and creative way.', 'section_summary': 'Here is a summary of the key topics and entities mentioned in the section:\\n\\n**Key Topics:**\\n\\n1. Science and Mathematics (勾股定理, 三角函數)\\n2. History (秦始皇, 東條英機)\\n3. Geography (太平洋, 南極)\\n4. Politics (川普)\\n5. Culture (義大利麵, 維爾康)\\n6. Environment and Pollution (核污染)\\n\\n**Entities:**\\n\\n1. Qin Shi Huang (秦始皇) - Chinese historical figure\\n2. Tojo Hideki (東條英機) - Japanese historical figure\\n3. Donald Trump (川普) - American politician\\n4. Walmart (沃爾瑪) - American multinational retail corporation\\n5. Vicon (維爾康) - unknown entity ( possibly a company or brand)\\n\\n**Other notable mentions:**\\n\\n1. UFOs (unidentified flying objects)\\n2. Concrete (42號混泥土)\\n3. Spaghetti (義大利麵)\\n4. Antarctica (南極)', 'excerpt_keywords': 'Here are 5 unique keywords for this document:\\n\\n義大利麵, 勾股定理, 東條英機, 川普, 維爾康'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2b2d350-d476-4634-9e5c-dd5a90073a5e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='e4b0d64fe9fb26118af8916d5cca28279e7a2e15863209b5bcc11efbe9b5d125')}, text='義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪跟維爾康在南極匯合。', mimetype='text/plain', start_char_idx=0, end_char_idx=210, text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n', metadata_template='{key}: {value}', metadata_seperator='\\n')\n",
      "\n",
      "👉 ID: \n",
      "f5a9673a-b91c-43db-aba5-41441c162115\n",
      "\n",
      "👉 原文:\n",
      "義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪跟維爾康在南極匯合。\n",
      "\n",
      "Metadata:\n",
      "\n",
      "🔵 擷取資訊 - document_title: \n",
      "A very interesting and diverse set of keywords!\n",
      "\n",
      "After analyzing the keywords, I would suggest a comprehensive title that encompasses the various themes and topics:\n",
      "\n",
      "**\"百科全書：科學、歷史、地理、政治與文化的交匯\"**\n",
      "\n",
      "Translation: \"Encyclopedia: The Convergence of Science, History, Geography, Politics, and Culture\"\n",
      "\n",
      "This title captures the essence of the diverse range of topics, from science and mathematics (勾股定理, 三角函數) to history (秦始皇, 東條英機) and geography (太平洋, 南極), as well as politics (川普) and culture (義大利麵, 維爾康). The title also hints at the idea of a comprehensive and interconnected world, which is reflected in the diverse set of keywords.\n",
      "\n",
      "🔵 擷取資訊 - questions_this_excerpt_can_answer: \n",
      "What a fascinating and bizarre excerpt!\n",
      "\n",
      "Based on the context, I've generated three questions that this text can provide specific answers to, which are unlikely to be found elsewhere:\n",
      "\n",
      "1. **What is the author's unconventional opinion on the ideal sauce for spaghetti?**\n",
      "\n",
      "This question is directly related to the excerpt, where the author suggests that spaghetti should be mixed with \"42號混泥土\" (a type of concrete), which is a humorous and nonsensical answer.\n",
      "\n",
      "2. **How does the author think the Pythagorean theorem (勾股定理) relates to the capture of wild \"三角函數\" (trigonometric functions) by a domesticated \"東條英機\" (a Japanese historical figure)?**\n",
      "\n",
      "This question highlights the author's creative and illogical connection between mathematical concepts and historical figures, making it a unique and humorous answer.\n",
      "\n",
      "3. **What is the author's tongue-in-cheek explanation for the potential environmental impact of Trump's policies on the Pacific Ocean and the Antarctic region?**\n",
      "\n",
      "This question captures the author's satirical tone and absurd reasoning, which links Trump's policies to nuclear pollution, UFOs, and the convergence of Walmart and Vicon (維爾康) in Antarctica.\n",
      "\n",
      "These questions showcase the playful and whimsical nature of the excerpt, which blends seemingly unrelated concepts and ideas in a humorous and creative way.\n",
      "\n",
      "🔵 擷取資訊 - section_summary: \n",
      "Here is a summary of the key topics and entities mentioned in the section:\n",
      "\n",
      "**Key Topics:**\n",
      "\n",
      "1. Science and Mathematics (勾股定理, 三角函數)\n",
      "2. History (秦始皇, 東條英機)\n",
      "3. Geography (太平洋, 南極)\n",
      "4. Politics (川普)\n",
      "5. Culture (義大利麵, 維爾康)\n",
      "6. Environment and Pollution (核污染)\n",
      "\n",
      "**Entities:**\n",
      "\n",
      "1. Qin Shi Huang (秦始皇) - Chinese historical figure\n",
      "2. Tojo Hideki (東條英機) - Japanese historical figure\n",
      "3. Donald Trump (川普) - American politician\n",
      "4. Walmart (沃爾瑪) - American multinational retail corporation\n",
      "5. Vicon (維爾康) - unknown entity ( possibly a company or brand)\n",
      "\n",
      "**Other notable mentions:**\n",
      "\n",
      "1. UFOs (unidentified flying objects)\n",
      "2. Concrete (42號混泥土)\n",
      "3. Spaghetti (義大利麵)\n",
      "4. Antarctica (南極)\n",
      "\n",
      "🔵 擷取資訊 - excerpt_keywords: \n",
      "Here are 5 unique keywords for this document:\n",
      "\n",
      "義大利麵, 勾股定理, 東條英機, 川普, 維爾康\n",
      "👉 Start Char Index:  \n",
      "0\n",
      "👉 End Char Index:  \n",
      "210\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# 查看執行完pipeline後其中一筆資訊範例\n",
    "\n",
    "pprint(nodes[0])\n",
    "\n",
    "print(f\"\\n👉 ID: \\n{nodes[0].id_}\")\n",
    "print(f\"\\n👉 原文:\\n{nodes[0].text}\")  \n",
    "print(\"\\nMetadata:\")\n",
    "for key, value in nodes[0].metadata.items():\n",
    "    print(f\"\\n🔵 擷取資訊 - {key}: \\n{value}\")\n",
    "print(f\"👉 Start Char Index:  \\n{nodes[0].start_char_idx}\")\n",
    "print(f\"👉 End Char Index:  \\n{nodes[0].end_char_idx}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以從上面結果看到，除了原文，還多了很多Extrator使用LLM處理後新增的訊息整理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 再拿一個範例，流程包含embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = \n",
    "\n",
    "print(\"原本文檔：\")\n",
    "print(textwrap.fill(documents, width=50))\n",
    "\n",
    "documents = [Document(text=documents)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## ✏️ IngestionPipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
