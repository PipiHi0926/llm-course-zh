{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:small; color:gray;\"> Author: 鄭永誠, Year: 2024 </p>\n",
    "\n",
    "# 一些文檔處理工具基於 llama_index\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基礎改念 (複習):\n",
    "- **token / tokenization** ➡️ token是LLM在處理文字時的最小單位，可以理解就像是一個個單字，留意不同LLM會用[不同詞彙表](https://huggingface.co/docs/transformers/en/tokenizer_summary)切token\n",
    "\n",
    "- **embedding** ➡️ 把這些token轉成一組高維向量，而這個向量用來表示這個句子的涵義，要留意不同模型能吃的token大小有異\n",
    "\n",
    "- **chunk** ➡️ 分塊，當資料量太大時，我們會將其切成一個一個分塊(chunks)\n",
    "\n",
    "- **parse / parsing** ➡️ 把句子解析、轉換成更好理解的格式，像是各類文檔語法結構調整、抽取訊息等，像是從html, json...轉成更好使用的格式\n",
    "\n",
    "- **extractor** ➡️ 文本在存儲之前，我們可以透過LLM先一步去從中識別和提取特定的信息，如關鍵字識別、主題建模、摘要、建立相關問題...\n",
    "\n",
    "- **pipeline** ➡️ 上面講了很多處理的流程，我們可以將其串在一起，建立所謂的資料處理流程(pipeline)，此處會講基於llamaindex的實踐方法，(下面統稱transormers)\n",
    "\n",
    "\n",
    "## 最終完整流程(Data Ingestion)可能包含\n",
    "\n",
    "- **Loaders** ➡️ 允許與外部源集成以上傳信息\n",
    "\n",
    "- **transormers** ➡️ 資料處理流程，如 parse, split to chunk, extract, embedding... 等多種流程\n",
    "\n",
    "- **Vector Stores** ➡️ 將資訊存入向量資料庫\n",
    "\n",
    "- **Retrievers** ➡️ 用於信息檢索的組件，從大規模文本數據集中檢索相關信息\n",
    "\n",
    "- **LLM Agent / Tools** ➡️ 各個處理問題的Agent、LLM模型或各種工具\n",
    "\n",
    "- **Memories** ➡️  記錄對話\n",
    "\n",
    "- **Output Parsers** ➡️ 把結果轉換成需求的格式，如json...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## ✏️ IngestionPipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \"\"\"\n",
    "%pip install llama_index.core -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分割後的文檔：\n",
      "[part_0]：<div align=\"center\">\n",
      "  <h1>🤖 大型語言模型入門教學 中文分享整理  💻</h1>\n",
      "  <p align=\"center\">\n",
      "    ✍️ <a href=\"https://hackmd.io/@pputzh5cRhi6gZI0csfiyA/H1ejIyxHR\"> 作者: 鄭永誠</a> • \n",
      "    ✉️ <a href=\"mailto:jason0304050607@gmail.com\">信箱</a> • \n",
      "    🧑‍🤝‍🧑 <a href=\"https://www.dalabx.com.tw//\"> 合作夥伴: 紫式大數據決策 </a> • \n",
      "    👫 <a href=\"https://moraleai.com/\"> 我的朋朋: Morale AI </a> \n",
      "  </p>\n",
      "</div>\n",
      "<p><br/></p>\n",
      "<p>內容簡介:\n",
      "1. 🍻 <strong>LLM基礎改念:</strong> 我會整理一些LLM的需求知識，但git上是實作資源為主不會講述太多，有興趣請密我\n",
      "2. 🛠️ <strong>LLM相關工具:</strong> 以下內容全基於python實踐，同時會分享相關資源、套件、開源API...\n",
      "3. 💬 <strong>LLM系統架構:</strong> 會帶你由淺入深，慢慢了解部屬LLM系統(多Agent)的方向和一些好用工具</p>\n",
      "<p>這個分享內容宗旨:\n",
      "1. 🧩 <strong>讓你好上手:</strong> 提供最簡單的、盡可能可複製即用的code，讓新手也能盡可能快速入門 (而且是中文XD)\n",
      "2. 🎈 <strong>讓你免費玩:</strong> 全基於開源資源，讓你能夠無痛體驗LLM的功能和操作\n",
      "3. 😊 <strong>讓你喜歡上:</strong> 盡量提供簡單有趣的小例子，讓你也能喜歡LLM可帶來的運用</p>\n",
      "<p>範例使用版本/輔助工具:\n",
      "- Python 3.12.4\n",
      "- 語言模型主要使用 llama-3.1-70b-versatile\n",
      "- 個人主要使用 IDE: VScode\n",
      "- 搭配工具 寫程式大幫手 <a href=\"https://github.com/features/copilot\">Copilot</a>\n",
      "- 其他: 使用 <a href=\"https://code.visualstudio.com/docs/python/linting\">pylint</a> 擴充套件來管理 Python 程式碼的風格</p>\n",
      "<p>主要資料來源:\n",
      "  💻 <a href=\"https://github.com/\">資料來源1: 偉大的Github</a> \n",
      "  🤗 <a href=\"https://huggingface.co/\">資料來源2: 偉大的抱抱臉</a> \n",
      "  👨 <a href=\"https://github.com/underlines/awesome-ml/blob/master/llm-tools.md/\">巨人的肩膀: LLM相關工具大整理</a></p>\n",
      "<hr />\n",
      "<h2>課程內容</h2>\n",
      "<p><img alt=\"alt text\" src=\"images/image.png\" />\n",
      "(因分享會有搭配我的簡報才會以這個架構講述，實際上這些課程無直接連貫性!!!)</p>\n",
      "<h3>✔️ Part 0: LLM 導論 (Instruction)</h3>\n",
      "<p>(這些皆以前課程內容，恕此處跳過)</p>\n",
      "<p>| 主題 | 簡介 | 類別 | Notebook | Resource|\n",
      "|----------|-------------|----------|----------|----------|\n",
      "| I1-人工智慧簡介與發展近況 | 什麼是AI? 發展學派與歷史脈絡| 基礎課程 | 見讀書會ppt||\n",
      "| I2-機器學習基本概念 |機器/深度學習最最最白話版基礎概念| 基礎課程 | 見讀書會ppt||\n",
      "| I3-LLM基礎原理 |Token, Embedding, Transformer...等基礎| 基礎課程 | 見讀書會ppt||\n",
      "| I4-LLM相關知識|提示工程、RAG與LLM框架、Fine-tuned，模型超參數| 基礎課程| 見讀書會ppt||\n",
      "| I5-實踐工具|LangFlow / Flowise 快速實踐工作流程| 延伸補充 | 見讀書會ppt|<a href=\"https://www.langflow.org/\">langflow</a>|\n",
      "| I6-簡易部屬|內部使用OpenWeb UI / Anything LLM部屬| 延伸補充 | 見讀書會ppt|<a href=\"https://github.com/open-webui/open-webui\">OpenWebUI</a>|</p>\n",
      "<h3>✔️ Part 1: Python 基礎實踐與建立流程</h3>\n",
      "<p>| 主題 | 簡介 | 類別 | Notebook |\n",
      "|----------|-------------|----------|----------|\n",
      "| C0-前置作業與基礎工具|建立虛擬環境、基礎python輔助工具| 前置準備 |<a href=\"C0-Basic_info.ipynb\">C0</a>|\n",
      "| C1-簡單使用範例|Groq操作、程式實踐基礎問答| 基礎課程 |<a href=\"C1-Get_start_with_groq.ipynb\">C1</a>|\n",
      "| C2-立刻部屬簡易系統|Gradio快速實踐系統介面、即時對話系統| 基礎課程 |<a href=\"C2-Create_llm_ui.ipynb\">C2</a> |\n",
      "| C3-已結合LLM的一些開源工具|一些AI工具如open-interpreter,Scrapegraph-ai...| 額外分享 |<a href=\"C3-Ai_tools.ipynb\">C3</a>|\n",
      "| C4-進階RAG操作|Reranker概念和效果| 延伸補充 |<a href=\"C4-Advanced_rag.ipynb\">C4</a>|\n",
      "| C5-實踐LLM服務Agent流程-1|基於Langchain架構下的LangGraph實踐| 進階課程 |<a href=\"C5-Agent_flow.ipynb\">C5</a>|\n",
      "| C6-實踐LLM服務Agent流程-2|基於Langchain架構下的LangGraph實踐| 進階課程 |<a href=\"C6-Agent_flow.ipynb\">C6</a>|\n",
      "| C7-將Agent流程進行管控|使用langsmith來管理、更清楚瞭解建立的流程| 進階課程 |<a href=\"C7-Llm_application.ipynb\">C7</a>|\n",
      "| C8-fine-tuned簡易操作範例|使用Unsloth簡易實踐qLora fine tuned(只放程式碼、不實際運行)| 進階課程 ||</p>\n",
      "<p><img alt=\"alt text\" src=\"images/image-8.png\" /></p>\n",
      "<h3>✔️ Part 2: Routing in RAG-Driven Applications</h3>\n",
      "<p>| 主題 | 簡介 | 類別 | Note|\n",
      "|----------|-------------|----------|----------|\n",
      "| C9-文本處理|文檔切割與解析概念| 基礎課程 |<a href=\"C9-Doc_manage.ipynb\">C9</a>|\n",
      "| C10-文本處理流程概念|資料處理、Ingestion Pipeline與RAG routing| 基礎課程 ||\n",
      "| C12-向量資料庫建置與各功能|| 基礎課程 ||\n",
      "| C13-其他資料庫結構|知識圖譜與GraphRAG| 延伸補充 ||</p>\n",
      "<h2>先備知識</h2>\n",
      "<h3>1. LLM是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>大型語言模型 (Large Language Model) 的簡稱</li>\n",
      "<li>你可以把他理解成是一個模型，能根據輸入的文字生成文字回傳，就像在做文字接龍一樣</li>\n",
      "<li>背後深度學習, Tokenization, embedding, attention機制, Transformer 等相關介紹詳見以前分享\n",
      "<img alt=\"alt text\" src=\"images/image-2.png\" />\n",
      "<img alt=\"alt text\" src=\"images/image-3.png\" />\n",
      "<img alt=\"alt text\" src=\"images/image-5.png\" /></li>\n",
      "</ul>\n",
      "<h3>2. Hugging Face 🤗 是什麼?</h3>\n",
      "<ul>\n",
      "<li>你可以把他理解成AI界的Github</li>\n",
      "<li>使用者可以在上邊發表和<a href=\"https://huggingface.co/docs/transformers/model_sharing\">共享預訓練模型</a>、資料集和展示檔案</li>\n",
      "<li>許多<a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\">模型排名</a>、<a href=\"https://huggingface.co/docs/transformers/llm_tutorial\">程式範例</a>都可在上面找到</li>\n",
      "</ul>\n",
      "<h3>3. Llama 🦙 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>Llama是Meta開發的一系列大型語言模型，如llama2, llama3, llama3.1</li>\n",
      "<li>這些語言模型都是免費的!!也能自行去做模型參數微調訓練(Fine-tuned)</li>\n",
      "<li>至於其他常見付費LLM則包含<a href=\"https://openai.com/index/openai-api/\">Open AI</a>, <a href=\"https://www.anthropic.com/api\">Claude</a>系列...</li>\n",
      "</ul>\n",
      "<h3>4. Ollama 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>開源的本地端LLM平台</li>\n",
      "<li>允許用戶在自己的電腦上運行和調用多種開放原始碼的語言模型</li>\n",
      "<li>若要部屬自己公司/組織內部的LLM，可以運用其資源</li>\n",
      "<li>建議可搭配<a href=\"https://docs.openwebui.com/\">Open WebUI</a>、<a href=\"https://anythingllm.com/\">AnythingLLM</a>實踐UI操作介面和管理</li>\n",
      "</ul>\n",
      "<h3>5. LangChain 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li><a href=\"https://python.langchain.com/v0.2/docs/introduction/\">LangChain</a>是LLM框架，目的在簡化使用大型語言模型（LLMs）開發應用程序的過程</li>\n",
      "<li>你可以簡單理解成他是個工具箱，把LLM操作過程可能需要的工具、外部數據源整合起來</li>\n",
      "<li>有這個工具箱，你就能更方便的調用他撰寫python llm相關程式</li>\n",
      "<li>其他優勢包含很多延伸服務和工具也基於他被開發出來 (如後面會講到的<a href=\"https://langchain-ai.github.io/langgraph/\">LangGraph</a>)</li>\n",
      "<li>對於新手而言，他官網上也有非常大量<a href=\"https://python.langchain.com/v0.2/docs/tutorials/llm_chain/\">範例程式</a>輔助你實踐llm</li>\n",
      "<li>其他常用框架還有LlamaIndex，其更擅長處理文本(e.g.非結構資訊)、自定義知識庫、有多種索引查詢功能</li>\n",
      "</ul>\n",
      "<h3>6. 機器學習 (Machine Learning)、深度學習 (Deep Learning )、人工智慧 (Artificial Intelligence) 之間的關聯是?</h3>\n",
      "<ul>\n",
      "<li>AI (人工智慧) 泛指使電腦和機器能夠模擬人類智慧和解決問題能力的技術，做到各種人類的判斷或行為</li>\n",
      "<li>ML (機器學習)，則是一種常見的技術手法，你可以想像成找出一個  𝑓(𝑥)，可以根據輸入值生成想要的結果  </li>\n",
      "<li>至於 DL (深度學習)，則是ML領域中基於神經網路(NN)的概念，一種表現亮眼的技術類型</li>\n",
      "<li>GAI (生成式人工智慧) 則是當中一種可以創造新內容和想法的AI概念，如創造對話、故事、影像、視訊和音樂!</li>\n",
      "<li>當今(2024.6)，GAI領域、如本讀書的重點LLM，當中最主要仰賴的技術模型就是DL為主!!!</li>\n",
      "<li>見下圖，你就能很清楚知道他們之間的關係!!!!!</li>\n",
      "</ul>\n",
      "<p><img alt=\"alt text\" src=\"images/image-9.png\" /></p>\n",
      "<h3>7. Transformer 是什麼 ?</h3>\n",
      "<ul>\n",
      "<li>Transformer是一種深度學習模型架構，在多種自然語言(NLP)任務處理表現出色</li>\n",
      "<li>為當今(2024)生成式AI的浪潮重大突破的核心技術概念</li>\n",
      "</ul>\n",
      "<p><img alt=\"alt text\" src=\"images/image-6.png\" /></p>\n",
      "<ul>\n",
      "<li>ChatGPT 裡面的 \"T\"，指的就是Transformer喔!</li>\n",
      "</ul>\n",
      "<p><img alt=\"alt text\" src=\"images/image-7.png\" /></p>\n",
      "<h2>主要使用的課程套件/工具/架構運用摘要</h2>\n",
      "<ul>\n",
      "<li>C0: 可略過</li>\n",
      "<li>C1: Langchain, Groq, llama_index</li>\n",
      "<li>C2: Gradio</li>\n",
      "<li>C3: 可略過</li>\n",
      "<li>C4: Sentence-transformer, Reranker (with Jina)</li>\n",
      "<li>C5: Langchain, LangGraph</li>\n",
      "<li>C6: Langchain, LangGraph</li>\n",
      "<li>C7: LangSmith</li>\n",
      "<li>C8: Unsloth</li>\n",
      "<li>C9: llama_index</li>\n",
      "</ul>\n",
      "<p>接下來就請去各ipynb筆記上去了解如何操作吧 ~</p>\n",
      "<h2>補充: 愛用工具/連結推廣</h2>\n",
      "<ol>\n",
      "<li>\n",
      "<p><a href=\"https://www.langflow.org/\">Langflow</a> / <a href=\"https://github.com/FlowiseAI/Flowise\">Flowise</a>: 帶你實踐no code介面串接開發大型語言模型應用程式</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://www.coze.com/home\">COZE</a> (以前免費常用...但現在QQ): 快速部屬LLM服務、Agent、建立dicord bot、設計workflow等，介面很舒服</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://www.perplexity.ai/\">perplexity</a>: 目前(2024.7)用起來最順的LLM輔助網頁搜尋工具</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://claude.ai/new\">claude 3.5 sonnet</a>: 目前(2024.7)覺得code表現能力最棒的LLM，特別是他的<a href=\"https://www.youtube.com/watch?v=rHqk0ZGb6qo\">Artifacts</a>功能</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>寫程式一定要<a href=\"https://github.com/features/copilot\">Copilot Github</a> 或是嘗試近期(2024.7)熱門的 <a href=\"https://www.cursor.com/\">Cursor</a>，後者我雖沒在用，但都很推</p>\n",
      "</li>\n",
      "</ol>\n",
      "<h2>自學連結</h2>\n",
      "<ol>\n",
      "<li>\n",
      "<p>Hugging Face, Github；甚至是 <a href=\"https://discord.gg/g662TXV6\">GAI年會</a>、臉書社團, iT 邦幫忙, Medium 文章等資源</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://github.com/mlabonne/llm-course\">LLM roadmap學習方向 – 技術面 &amp; 應用面 (github)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>李宏毅 – 生成式導論2024 <a href=\"https://www.youtube.com/watch?v=AVIKFXLCPY8&amp;list=PLJV_el3uVTsPz6CTopeRp2L2t4aL_KgiI\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>李宏毅 – 【機器學習 2023】(生成式 AI) <a href=\"https://www.youtube.com/watch?v=yiY4nPOzJEg&amp;list=PLJV_el3uVTsOePyfmkfivYZ7Rqr2nMk3W\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>李宏毅 – 【機器學習 2022 】<a href=\"https://www.youtube.com/watch?v=7XZR0-4uS5s&amp;list=PLJV_el3uVTsPM2mM-OQzJXziCGJa8nJL8\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>陳縕儂 – 深度學習應用、人工智慧導論 <a href=\"https://www.youtube.com/watch?v=g4yONRTpbE4&amp;list=PLOAQYZPRn2V658cD6AjiBmKohfMIevWO7\">(YT)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://github.com/xianshang33/llm-paper-daily\">LLM相關論文整理日更 (github)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>諸多YT上資源</p>\n",
      "<ul>\n",
      "<li>01Coder</li>\n",
      "<li>零度解說</li>\n",
      "<li>最佳拍檔</li>\n",
      "<li>王木头学科学</li>\n",
      "<li>Ph.D. Vlog</li>\n",
      "<li>AI Jason</li>\n",
      "<li>AI超元域</li>\n",
      "<li>跟李沐学AI (Mu Li)</li>\n",
      "<li>GrandmaCan -我阿嬤都會</li>\n",
      "<li>3Blue1Brown</li>\n",
      "<li>AI小码哥</li>\n",
      "</ul>\n",
      "</li>\n",
      "<li>\n",
      "<p>LLM資源彙總 <a href=\"https://github.com/liguodongiot/llm-resource\">(github)</a></p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://github.com/LlamaFamily/Llama-Chinese\">llama中文社區 </a></p>\n",
      "</li>\n",
      "</ol>\n",
      "<h2>Backup URL</h2>\n",
      "<ul>\n",
      "<li><a href=\"https://www.run.ai/guides/ai-open-source-projects/nvidia-nemo\">NVIDIA NeMo</a>、<a href=\"https://github.com/NVIDIA/NeMo\">NeMo GitHub</a></li>\n",
      "<li><a href=\"https://build.nvidia.com/meta/llama-3_1-405b-instruct\">NV llama-3_1-405b-instruct</a></li>\n",
      "<li>https://github.com/xubuvd/LLMs</li>\n",
      "<li><a href=\"https://github.com/vanna-ai/vanna\">vanna</a>、<a href=\"https://github.com/explodinggradients/ragas\">Ragas</a>、<a href=\"https://github.com/microsoft/graphrag\">RagGraph</a></li>\n",
      "<li><a href=\"https://zapier.com/app/home\">zapier</a>  完全沒任何基礎也能建置如信件、notion等的流程整合服務工具</li>\n",
      "<li><a href=\"https://cloud.dify.ai/apps\">Dify</a> 目前用起來覺得，UI介面最舒服且最無難度的上手no code應用流程建置工具 (但要錢用的模型也要錢)</li>\n",
      "</ul>\n",
      "<h3>🤔 問題回饋</h3>\n",
      "<p>如有問題，歡迎直接聯繫我，或在GitHub Issue中提交喔~\n",
      "<img alt=\"alt text\" src=\"ig_logo.png\" />\n",
      "<a href=\"https://www.instagram.com/yourprofile\">\n",
      "    <img src=\"images/ig-icon.png\" alt=\"Instagram\" style=\"width: 30px; height: 30px;\">\n",
      "</a></p>\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 這邊以 MarkdownNodeParser 為例，可以參考其他的NodeParser \"\"\"\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core import Document\n",
    "import markdown\n",
    "\n",
    "parser = MarkdownNodeParser()\n",
    "\n",
    "# 這邊我們先定義一個讀取markdown文件的函數\n",
    "def read_markdown_file(filename):\n",
    "    # Create a Path object for the markdown file\n",
    "    file_path = Path(filename)\n",
    "    \n",
    "    # Open and read the file content\n",
    "    with file_path.open('r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    return content\n",
    "\n",
    "# 讀取我們的README.md文件\n",
    "content = read_markdown_file('README.md')\n",
    "markdown_content = markdown.markdown(content)\n",
    "\n",
    "\n",
    "# 注意，node_parser不是直接吃文本，而是吃list of Document物件\n",
    "documents = Document(text=markdown_content)  # id_ 可選，可以設置為任何標識符\n",
    "\n",
    "\n",
    "# 我直接拿我們的README.md來做範例\n",
    "nodes = parser.get_nodes_from_documents([documents])\n",
    "\n",
    "# 留意我因為文檔太短，所以只有一個node\n",
    "print(\"\\n分割後的文檔：\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f'[part_{i}]：{node.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Name: llama-index\n",
      "Version: 0.10.64\n",
      "Summary: Interface between LLMs and your data\n",
      "Home-page: https://llamaindex.ai\n",
      "Author: Jerry Liu\n",
      "Author-email: jerry@llamaindex.ai\n",
      "License: MIT\n",
      "Location: c:\\Users\\PipiHi\\Desktop\\KM\\llm-course-zh\\.venv\\Lib\\site-packages\n",
      "Requires: llama-index-agent-openai, llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-legacy, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 安裝llamaindex \"\"\"\n",
    "# %pip uninstall llama_index -q\n",
    "%pip install llama-index -q\n",
    "%pip show llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-core -q\n",
    "%pip install llama-index-llms-openai -q\n",
    "%pip install llama-index-llms-replicate -q\n",
    "%pip install llama-index-embeddings-huggingface -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_example = [\n",
    "    {\n",
    "        'title': 'Alice', \n",
    "        'content': 'Alice is very adaptable and can handle unexpected challenges with ease.', \n",
    "        'job': ['Engineer', 'Designer']\n",
    "    }, \n",
    "    {\n",
    "        'title': 'Bob',\n",
    "        'content': 'Bob is a natural leader with strong communication skills.',\n",
    "        'job': ['Teacher', 'Writer']\n",
    "    }, \n",
    "    {\n",
    "        'title': 'Charlie',\n",
    "        'content': 'Charlie is a charismatic individual who easily connects with others', \n",
    "        'job': ['Doctor', 'Researcher']\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## ✏️ SentenceSplitter - chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: llama-index\n",
      "Version: 0.10.64\n",
      "Summary: Interface between LLMs and your data\n",
      "Home-page: https://llamaindex.ai\n",
      "Author: Jerry Liu\n",
      "Author-email: jerry@llamaindex.ai\n",
      "License: MIT\n",
      "Location: c:\\Users\\PipiHi\\Desktop\\KM\\llm-course-zh\\.venv\\Lib\\site-packages\n",
      "Requires: llama-index-agent-openai, llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-legacy, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原本文檔：\n",
      "義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到\n",
      "挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至\n",
      "於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲\n",
      "野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪\n",
      "跟維爾康在南極匯合。\n",
      "\n",
      "分割後的文檔：\n",
      "[part_0]：義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。\n",
      "[part_1]：你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至於對整個太平洋，和充電器的核污染。再或者說透\n",
      "[part_2]：者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈\n",
      "[part_3]：是否有沈澱物，都不會影響到沃爾瑪跟維爾康在南極匯合。\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 網路上llama_index舊版的是用 llama_index.XXXX ，新版用llama_index.core.XXXX \"\"\"\n",
    "import textwrap\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "\n",
    "# 自定義一個文件\n",
    "documents = \"義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。你往裡砸的時候，一瞬間他就會產生大量的高能蛋白，俗稱UFO，會嚴重影響經濟的發展，以至於對整個太平洋，和充電器的核污染。再或者說透過這勾股定理很容易推斷出人工飼養的東條英機，他是可以捕獲野生的三角函數，所以說不管這秦始皇的切面是否具有放射性，川普的N次方是否有沈澱物，都不會影響到沃爾瑪跟維爾康在南極匯合。\"\n",
    "\n",
    "print(\"原本文檔：\")\n",
    "print(textwrap.fill(documents, width=50))\n",
    "\n",
    "node_parser  = SentenceSplitter(\n",
    "    chunk_size=100, # 設定每個chunk的大小\n",
    "    chunk_overlap=5, # 設定chunk之間的重疊大小\n",
    "    tokenizer= None, # 設定分詞器\n",
    "    paragraph_separator=\"\\n\\n\", # 設定段落之間的分隔符號\n",
    "    separator=\" \", # 用於拆分句子的預設的分隔字元\n",
    "    secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?' # 用於分割句子的備份正規表示式\n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(\n",
    "    [Document(text=documents)], show_progress=False\n",
    ")\n",
    "print(\"\\n分割後的文檔：\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f'[part_{i}]：{node.text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## ✏️ SentenceSplitter - parser\n",
    "有許多用來處理各種文檔的parser，詳見官方   \n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/\n",
    "\"\"\" \"\"\"\n",
    "%pip install llama_index.core -q\n",
    "\"\"\" 這邊以 MarkdownNodeParser 為例，可以參考其他的NodeParser \"\"\"\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core import Document\n",
    "import markdown\n",
    "\n",
    "parser = MarkdownNodeParser()\n",
    "\n",
    "# 這邊我們先定義一個讀取markdown文件的函數\n",
    "def read_markdown_file(filename):\n",
    "    # Create a Path object for the markdown file\n",
    "    file_path = Path(filename)\n",
    "    \n",
    "    # Open and read the file content\n",
    "    with file_path.open('r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    return content\n",
    "\n",
    "# 讀取我們的README.md文件\n",
    "content = read_markdown_file('README.md')\n",
    "markdown_content = markdown.markdown(content)\n",
    "\n",
    "\n",
    "# 注意，node_parser不是直接吃文本，而是吃list of Document物件\n",
    "documents = Document(text=markdown_content)  # id_ 可選，可以設置為任何標識符\n",
    "\n",
    "\n",
    "# 我直接拿我們的README.md來做範例\n",
    "nodes = parser.get_nodes_from_documents([documents])\n",
    "\n",
    "# 留意我因為文檔太短，所以只有一個node\n",
    "print(\"\\n分割後的文檔：\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f'[part_{i}]：{node.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## ✏️ SentenceSplitter - parser\n",
    "有許多用來處理各種文檔的parser，詳見官方   \n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \"\"\"\n",
    "%pip install llama_index.core -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 讀取我們的README.md文件\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mread_markdown_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mREADME.md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m markdown_content \u001b[38;5;241m=\u001b[39m markdown\u001b[38;5;241m.\u001b[39mmarkdown(content)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 注意，node_parser不是直接吃文本，而是吃list of Document物件\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m, in \u001b[0;36mread_markdown_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_markdown_file\u001b[39m(filename):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Create a Path object for the markdown file\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(filename)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Open and read the file content\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" 這邊以 MarkdownNodeParser 為例，可以參考其他的NodeParser \"\"\"\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core import Document\n",
    "import markdown\n",
    "\n",
    "parser = MarkdownNodeParser()\n",
    "\n",
    "# 這邊我們先定義一個讀取markdown文件的函數\n",
    "def read_markdown_file(filename):\n",
    "    # Create a Path object for the markdown file\n",
    "    file_path = Path(filename)\n",
    "    \n",
    "    # Open and read the file content\n",
    "    with file_path.open('r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    return content\n",
    "\n",
    "# 讀取我們的README.md文件\n",
    "content = read_markdown_file('README.md')\n",
    "markdown_content = markdown.markdown(content)\n",
    "\n",
    "\n",
    "# 注意，node_parser不是直接吃文本，而是吃list of Document物件\n",
    "documents = Document(text=markdown_content)  # id_ 可選，可以設置為任何標識符\n",
    "\n",
    "\n",
    "# 我直接拿我們的README.md來做範例\n",
    "nodes = parser.get_nodes_from_documents([documents])\n",
    "\n",
    "# 留意我因為文檔太短，所以只有一個node\n",
    "print(\"\\n分割後的文檔：\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f'[part_{i}]：{node.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## ✏️ sentence_transformers - Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拿一個切完的chunk範例:  義大利麵應拌什麼好?  我個人認為義大利麵就應該拌42號混泥土，因為這個螺絲釘的長度很容易直接影響到挖掘機的扭矩。\n",
      "\n",
      "句子Embedding 後結果:  [ 2.28983879e-01  2.56072372e-01 -9.01247978e-01  2.16540433e-02\n",
      "  1.61327943e-01 -1.41556514e-02  3.84209663e-01 -1.99013099e-01\n",
      " -1.55311847e+00  6.03017390e-01  2.07268342e-01 -3.16158026e-01\n",
      " -5.27468443e-01 -7.72385418e-01 -7.33864367e-01 -7.74600148e-01\n",
      "  1.51136622e-01 -3.20973635e-01 -3.76673907e-01 -9.55326915e-01\n",
      " -7.29235470e-01  2.18102023e-01 -4.98580933e-01 -1.31339148e-01\n",
      "  7.16233611e-01  3.13105881e-01  3.83338720e-01 -9.17241454e-01\n",
      "  5.43770613e-03  4.03267086e-01  6.08780324e-01 -1.19084668e+00\n",
      " -1.05522287e+00  7.90530562e-01  1.84666634e-01  4.99955505e-01\n",
      "  3.46025735e-01  7.76784360e-01  1.76949695e-01 -9.94610190e-01\n",
      "  9.47135165e-02 -1.31648213e-01 -4.64734465e-01  1.02906168e+00\n",
      "  1.52269915e-01  5.49417913e-01  2.23850280e-01  5.11151791e-01\n",
      " -9.47740257e-01  4.94052619e-01  9.64427516e-02  6.79402590e+00\n",
      " -4.76693422e-01  1.04922771e+00 -7.90978849e-01  9.72206533e-01\n",
      "  3.42917889e-01 -5.14202058e-01 -5.52227795e-01 -1.09890115e+00\n",
      "  6.06656730e-01 -2.06763789e-01  9.25660014e-01  5.71880519e-01\n",
      "  1.79549515e+00 -9.13448334e-02 -1.24704326e-02  1.32983291e+00\n",
      "  4.59197104e-01 -5.19440293e-01  6.95537254e-02 -5.67509949e-01\n",
      "  8.19713548e-02  5.51195741e-01  7.32462257e-02  6.08692288e-01\n",
      " -1.57639995e-01  4.87389863e-01 -5.01100123e-01 -1.35432124e-01\n",
      " -2.16462687e-01  8.28390598e-01 -6.44174695e-01 -6.79190218e-01\n",
      " -9.17770743e-01 -7.31495917e-01 -8.93518925e-02  4.57314968e-01\n",
      "  1.91313431e-01  1.44154274e+00 -1.42295480e+00 -2.68057227e-01\n",
      "  1.14579029e-01 -2.17351794e-01  2.17383459e-01 -3.98314714e-01\n",
      "  1.18135452e+00 -9.69654202e-01 -8.19805980e-01  1.14839874e-01\n",
      "  8.43439624e-02 -4.46013927e-01 -1.04985738e+00  1.11026585e+00\n",
      "  4.03984129e-01  2.52683848e-01 -7.68481851e-01  9.00173128e-01\n",
      " -9.87602115e-01 -2.13365078e-01 -8.90408382e-02  3.37469757e-01\n",
      " -2.66120285e-01 -6.77973628e-01 -7.19923794e-01 -4.22388949e-02\n",
      " -6.09412074e-01 -6.48268700e-01  1.00725733e-01  4.84669983e-01\n",
      " -8.08133110e-02  3.38874251e-01 -1.13249756e-01 -1.11673772e+00\n",
      " -3.28328550e-01 -7.79185176e-01  8.34421277e-01  6.65613590e-03\n",
      " -1.11650310e-01  8.14698219e-01  3.34033281e-01  4.26968247e-01\n",
      " -7.63115346e-01 -5.83797395e-01  1.03431061e-01 -1.31326849e-02\n",
      "  3.96425664e-01  1.58268884e-01 -6.15304768e-01  5.84329605e-01\n",
      "  7.27176130e-01 -4.05582450e-02 -7.23468661e-01 -2.90959090e-01\n",
      "  1.08869195e+00  4.32318747e-01 -5.73399603e-01 -4.26401705e-01\n",
      "  3.01752388e-01 -9.98745263e-01  1.36296451e-01  5.80220342e-01\n",
      "  3.91080119e-02  5.13714194e-01 -8.18436086e-01  4.92333584e-02\n",
      "  3.06380987e-01  8.43698919e-01 -7.82532692e-01  3.71187478e-01\n",
      " -4.84552652e-01 -2.08487418e-02 -1.35828465e-01  6.65398121e-01\n",
      " -5.59358418e-01 -2.11473927e-01 -6.02310658e-01 -9.06792223e-01\n",
      "  3.37291062e-01  1.00760303e-01 -8.62581253e-01  3.20813179e-01\n",
      "  6.80993125e-02 -1.10889208e+00  5.08678734e-01 -7.06488252e-01\n",
      " -1.35493144e-01 -1.24482322e+00 -3.45825881e-01 -8.52520525e-01\n",
      " -8.87598336e-01 -3.29825342e-01  3.42122465e-01 -3.06551725e-01\n",
      "  3.21654946e-01 -4.30989206e-01 -4.75320876e-01  2.83877581e-01\n",
      " -3.32055092e-01  4.44205582e-01 -1.36185205e+00  3.09921741e-01\n",
      " -1.10124040e+00 -1.76254854e-01  6.31675422e-01 -2.11228386e-01\n",
      " -1.79979876e-01 -6.87042847e-02 -8.35012943e-02 -1.63307115e-01\n",
      " -1.41215801e+00  2.83081736e-02  2.03860812e-02  5.73546827e-01\n",
      "  7.04729438e-01  5.89459240e-01 -1.54484168e-01 -5.51276147e-01\n",
      "  8.64627540e-01  1.46901652e-01  1.00844465e-01  1.94061622e-01\n",
      "  2.94846803e-01 -1.30729926e+00 -6.79501355e-01  4.37886536e-01\n",
      "  9.02758017e-02  7.54023433e-01  3.51166278e-01 -5.19018352e-01\n",
      "  7.95009911e-01  7.20091522e-01  3.79859000e-01 -6.26654178e-02\n",
      " -1.07199800e+00 -2.97731489e-01  7.14399874e-01 -6.03019536e-01\n",
      "  3.93624380e-02 -3.72709721e-01  7.82185078e-01  7.21335933e-02\n",
      " -2.81401455e-01  7.96011448e-01  2.95097023e-01  4.27560151e-01\n",
      "  5.43005943e-01  1.67811751e-01  2.45715424e-01  3.64269584e-01\n",
      " -3.93071085e-01  1.80834845e-01 -1.10392675e-01 -6.29622519e-01\n",
      " -3.60046715e-01  5.83150089e-01 -7.74438202e-01 -3.22831482e-01\n",
      " -7.93785378e-02 -5.49381375e-01  7.20990837e-01  4.69995499e-01\n",
      "  7.30452120e-01 -5.73494971e-01  5.70286751e-01 -3.54906231e-01\n",
      " -5.54309674e-02 -4.76375908e-01  3.02376114e-02 -1.92382082e-01\n",
      "  1.47983223e-01  3.16108257e-01  7.14587510e-01  7.08876491e-01\n",
      " -9.99429971e-02 -5.02313972e-02  1.18746150e+00 -7.66541004e-01\n",
      " -6.23566985e-01 -3.20743501e-01  9.84689832e-01  1.65741786e-01\n",
      "  1.02398016e-01 -1.05953431e+00  3.55807273e-03  3.76093239e-01\n",
      "  7.07920492e-01  4.63535309e-01  2.04889461e-01 -3.16536009e-01\n",
      "  7.23513722e-01 -3.34933907e-01 -6.10867798e-01  3.71921480e-01\n",
      " -8.47951531e-01 -1.07437253e+00  5.23948014e-01  9.32193637e-01\n",
      " -6.03058226e-02  3.32392491e-02 -7.08359599e-01  2.89770275e-01\n",
      " -3.18399398e-03 -2.42671803e-01  1.36006033e+00  1.82803795e-01\n",
      " -1.69990078e-01 -3.08354646e-01  2.69800425e-01  8.77564311e-01\n",
      " -9.08751667e-01 -2.30175868e-01 -7.17646599e-01  9.44858015e-01\n",
      " -5.73066413e-01  3.19843441e-01  7.66248927e-02  9.65092182e-02\n",
      "  6.69093549e-01 -3.56529802e-01 -6.76996350e-01  1.11758046e-01\n",
      "  1.05800354e+00  8.03843737e-02 -1.94357109e+00 -5.50368071e-01\n",
      "  1.15692520e+00  5.42090416e-01  4.17277068e-01 -3.31014663e-01\n",
      "  4.91379052e-01  9.64123428e-01  4.56644595e-01  1.13059843e+00\n",
      " -6.22865200e-01  2.52978235e-01 -1.47427547e+00 -7.35909879e-01\n",
      "  5.61069667e-01  3.17381114e-01  5.40003061e-01  4.83703166e-01\n",
      "  3.59067053e-01  6.78282976e-02  3.40646207e-01  1.32406756e-01\n",
      " -2.69079953e-01  7.92798698e-01 -7.54555404e-01 -4.17503983e-01\n",
      "  9.80783924e-02 -1.01373172e+00  5.68809927e-01 -4.26511854e-01\n",
      " -7.98511446e-01 -7.01986194e-01 -4.13340241e-01 -6.25120699e-01\n",
      " -5.68127692e-01 -4.98710871e-01 -5.18991411e-01  2.06280470e-01\n",
      " -1.58829272e+00 -1.42106578e-01 -6.06289506e-01  8.06060195e-01\n",
      "  2.94498295e-01  3.30647737e-01  1.05005637e-01  4.21932071e-01\n",
      "  4.21716303e-01 -5.35401642e-01 -3.27707469e-01  1.14614642e+00\n",
      "  2.78039187e-01 -1.57868396e-02 -4.57220018e-01  1.71659682e-02\n",
      "  8.03962827e-01 -4.48502488e-02  7.60200977e-01  3.63844007e-01\n",
      "  1.05657804e+00 -4.48222458e-01  5.36401391e-01 -9.10385609e-01\n",
      "  6.57564521e-01  4.65733856e-01  4.03462827e-01  3.92999388e-02\n",
      " -3.21336180e-01 -6.97459579e-01  3.69266689e-01 -2.21168995e-01\n",
      " -5.87294102e-01  4.35003042e-01  1.45511627e-01 -4.76845413e-01\n",
      "  1.52165502e-01 -1.66959554e-01 -2.62042016e-01  2.98170326e-03\n",
      " -5.50271869e-01 -6.63900495e-01 -4.24086422e-01 -3.98939908e-01\n",
      "  7.41499960e-01  1.87120125e-01  5.54404378e-01  1.19173610e+00\n",
      "  9.29890513e-01 -1.19623089e+00  2.85319000e-01  4.43135470e-01\n",
      "  9.58068788e-01 -1.02139688e+00 -3.27790350e-01  3.18432033e-01\n",
      "  2.46527299e-01 -9.34749484e-01 -1.38131127e-01 -2.78571159e-01\n",
      " -1.83066392e+00 -7.00166643e-01  9.11881924e-02  4.47649568e-01\n",
      " -3.02254230e-01 -4.62673694e-01  1.40189457e+00 -6.54947281e-01\n",
      "  5.21104157e-01 -2.37438634e-01  5.05414128e-01  3.64596277e-01\n",
      "  1.47723019e+00  4.39141318e-02 -1.60518676e-01  4.41877633e-01\n",
      "  5.93543112e-01 -3.24181229e-01  5.29660918e-02 -3.81351560e-01\n",
      " -5.88393630e-03 -6.22440338e-01  1.52262795e+00  9.59681809e-01\n",
      " -1.05775392e+00 -4.19774473e-01 -3.50890696e-01 -1.29918492e+00\n",
      " -2.06451818e-01  3.81263465e-01  2.47238860e-01 -3.74011070e-01\n",
      " -4.95566279e-01 -4.08194721e-01  4.34463084e-01 -6.87392533e-01\n",
      "  3.66606623e-01 -2.65736669e-01 -6.72835290e-01  6.90058529e-01\n",
      "  6.69061780e-01 -5.96119948e-02 -2.44348362e-01 -9.35847387e-02\n",
      "  4.69023526e-01 -1.01177049e+00  3.60498726e-02  1.01141369e+00\n",
      "  3.10195535e-01 -6.35333419e-01  4.71915185e-01  3.74588937e-01\n",
      "  8.85561943e-01 -1.88142672e-01  7.60721564e-01  4.32797447e-02\n",
      "  3.71356815e-01  4.10823017e-01  7.65911996e-01  7.26459175e-02\n",
      "  1.87113732e-01 -6.26152456e-01 -1.37504920e-01 -4.51816261e-01\n",
      " -1.61126927e-01 -1.93060979e-01 -8.72158587e-01 -3.36037725e-01\n",
      " -7.54297793e-01  8.74253392e-01  1.94195464e-01  2.47314483e-01\n",
      " -6.81042969e-01  5.08773208e-01  1.39179662e-01 -5.47051311e-01\n",
      " -2.11015776e-01 -7.21510768e-01 -5.59090614e-01 -2.08681658e-01\n",
      "  7.88952291e-01 -1.24764574e+00  4.54458259e-02 -7.23744035e-01\n",
      "  1.02715380e-01  1.00032173e-01 -1.16069591e+00 -7.87228644e-01\n",
      "  2.78895319e-01 -1.21883082e+00  5.81829488e-01  8.40145815e-03\n",
      " -4.71131243e-02  3.43617231e-01  7.52270162e-01  8.76216218e-02\n",
      " -1.15081501e+00  3.52889836e-01 -1.23902254e-01 -1.20307215e-01\n",
      " -8.12716186e-01 -8.13951194e-01  3.60443085e-01 -2.23673269e-01\n",
      " -1.99264213e-01  4.94595289e-01  7.57702053e-01 -1.49178490e-01\n",
      " -2.56736547e-01 -3.15526989e-03 -1.91697981e-02  3.63498539e-01\n",
      " -9.09169793e-01 -3.14757138e-01 -4.14069563e-01 -1.13592827e+00\n",
      " -5.55058897e-01  8.39998126e-01 -3.30079585e-01  1.98440984e-01\n",
      "  3.35699320e-01 -7.45958760e-02 -1.75155193e-01 -3.73713113e-02\n",
      "  1.02519107e+00 -1.04319692e+00  1.65777236e-01 -1.37534046e+00\n",
      " -6.56153023e-01  1.70044065e-01  7.79447734e-01 -1.84647202e+00\n",
      " -8.38692367e-01 -6.78645551e-01 -6.34669125e-01 -4.00879174e-01\n",
      "  3.83149058e-01  7.06601024e-01 -6.77478671e-01  1.01493441e-01\n",
      " -6.49584591e-01  1.30030477e+00  1.68863852e-02 -7.16911316e-01\n",
      " -3.79534990e-01 -1.15196967e+00  6.87451780e-01 -6.41097546e-01\n",
      "  2.30632186e-01  9.66227531e-01 -3.64652216e-01 -1.37695242e-02\n",
      " -8.60143542e-01 -2.34382629e-01  3.52514796e-02  1.13427326e-01\n",
      " -5.99935472e-01  4.99804586e-01  2.20731333e-01 -3.29566181e-01\n",
      " -2.66554981e-01  1.30380714e+00  2.55829662e-01  5.56493774e-02\n",
      " -8.36409032e-01 -9.34826210e-02  1.85068965e-01 -7.46844828e-01\n",
      "  1.18279636e+00 -8.29712152e-02  2.50250995e-01 -3.15950453e-01\n",
      " -7.42999971e-01 -9.74582136e-01  7.81636178e-01 -1.43903971e-01\n",
      "  1.33652547e-02  6.98494852e-01 -2.81975031e-01  2.45306775e-01\n",
      "  3.70142698e-01  9.40347254e-01 -6.92120969e-01 -5.84740937e-01\n",
      "  9.86576438e-01  4.25285622e-02  4.72607553e-01  3.56950283e-01\n",
      " -1.79447681e-01 -1.62070155e+00 -4.75838691e-01  2.90967613e-01\n",
      "  1.74555570e-01 -9.78852630e-01  1.56121314e+00  4.06875536e-02\n",
      "  1.75793022e-02 -8.17005157e-01 -1.60116208e+00  1.62118626e+00\n",
      "  1.31026971e+00 -4.21579361e-01  6.17033660e-01 -3.49153191e-01\n",
      "  1.05307817e-01  2.10978293e+00 -3.21212232e-01 -2.48126671e-01\n",
      " -6.97696984e-01  2.99311787e-01 -2.47632161e-01  8.89310986e-02\n",
      " -3.08613926e-01  7.74717033e-01  1.73597324e+00  2.70739883e-01\n",
      "  5.88790476e-01 -4.09322590e-01 -6.67287037e-02 -8.25061798e-01\n",
      "  6.35225713e-01 -1.31599098e-01 -1.64048448e-01 -1.36590570e-01\n",
      " -2.67096788e-01  6.95242763e-01 -2.85616338e-01  1.04406632e-01\n",
      "  7.32901871e-01 -1.11537480e+00 -2.99391430e-02 -7.11095557e-02\n",
      "  3.84500831e-01 -2.91283429e-01  1.13532174e+00 -7.69353285e-02\n",
      "  9.72105682e-01 -5.53142250e-01  8.82899404e-01 -2.12529048e-01\n",
      "  5.71933329e-01 -5.30772746e-01  4.13656056e-01  7.78598368e-01\n",
      "  9.83355880e-01 -1.09876990e-01 -8.88568580e-01 -8.73118639e-03\n",
      "  2.85082966e-01 -2.26746246e-01  1.15472186e+00  4.96865250e-02\n",
      " -6.42326534e-01 -6.33052945e-01  5.21015823e-01  3.78032327e-02\n",
      "  6.30678833e-01  3.84100884e-01  1.17220175e+00 -2.83609275e-02\n",
      " -5.81505895e-02 -6.62314475e-01 -4.45885450e-01  6.34800792e-01\n",
      "  5.55745006e-01 -9.56816792e-01 -9.37782347e-01  5.42472482e-01\n",
      " -5.01666330e-02  3.70286167e-01  1.47362387e+00 -1.05780816e+00\n",
      "  2.24842072e-01  1.81411635e-02 -3.50180656e-01  1.36604443e-01\n",
      " -4.46446866e-01  7.72535682e-01 -4.01252843e-02  3.60917747e-01\n",
      "  4.43161488e-01 -1.20529675e+00 -2.73842841e-01  1.91148376e+00\n",
      "  9.35735822e-01  1.36875451e+00 -8.54405940e-01 -2.40111426e-01\n",
      "  1.16590583e+00  6.75195634e-01 -1.45467997e-01 -6.11577988e-01\n",
      " -1.76525921e-01  2.92310387e-01 -5.08492813e-02  3.90944421e-01\n",
      " -4.75582659e-01  1.18603602e-01 -6.91619933e-01  7.86875665e-01\n",
      "  2.40851894e-01 -1.61634937e-01 -3.91967744e-01 -5.13691425e-01\n",
      " -9.73029971e-01  9.94275153e-01  1.00678988e-01  8.94208789e-01\n",
      " -5.49338937e-01 -9.70453560e-01  1.81251794e-01 -3.48367512e-01\n",
      "  5.07725894e-01  4.93448406e-01 -2.85238951e-01  1.23530185e+00\n",
      "  9.64541018e-01  1.02378738e+00 -7.12806404e-01  3.67551625e-01\n",
      "  4.04026769e-02 -4.90248382e-01 -4.47823703e-01  5.33294976e-01\n",
      "  8.74980628e-01 -3.30916941e-01 -1.37265101e-01  1.97009712e-01\n",
      "  1.67133927e-01 -7.86473975e-02  1.04524887e+00 -8.20405960e-01\n",
      " -8.03855121e-01 -1.81519389e-02 -7.49858692e-02 -3.04491222e-01\n",
      " -3.68479729e-01 -3.91175032e-01 -5.10117888e-01  4.61999804e-01\n",
      "  1.26129127e+00  7.62649113e-03 -1.99084971e-02 -3.49048585e-01\n",
      " -3.06297809e-01  1.81877941e-01 -5.63747883e-01  9.87228096e-01\n",
      " -8.68583798e-01  1.66859820e-01  2.24286675e-01  3.50840867e-01\n",
      " -1.96403965e-01  3.05609047e-01 -5.48548937e-01 -5.38150132e-01]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Embedding方法示意，可參考C4內容 \"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# 載入模型 (已選擇中文擅長模型)\n",
    "model = SentenceTransformer('DMetaSoul/sbert-chinese-general-v2')  \n",
    "\n",
    "text = node_parser.get_nodes_from_documents([Document(text=documents)], show_progress=False)[0].text\n",
    "print(\"拿一個切完的chunk範例: \", text)\n",
    "\n",
    "embedding = model.encode(text, convert_to_tensor=False)\n",
    "print(\"\\n句子Embedding 後結果: \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子Embedding 後結果: \n",
      " [-0.011561818420886993, -0.027052123099565506, -0.046663232147693634, 0.011066941544413567, -0.004675247240811586, -0.018668046221137047, 0.0036121418233960867, -0.06413348019123077, 0.002792960498481989, -0.02290388196706772, 0.002651661168783903, 0.03054366260766983, -2.5104986889346037e-06, -0.00016913010040298104, -0.059822823852300644, -0.030154917389154434, -0.0007710297359153628, -0.08172184973955154, 0.008165168575942516, -0.011992831714451313, -0.020475028082728386, 0.0741274282336235, 0.003467828268185258, -0.0053542302921414375, -0.038078323006629944, 0.02178216725587845, 0.026178831234574318, 0.0022571897134184837, -0.06470903009176254, -0.007674640975892544, -0.00895301066339016, 0.026491597294807434, -0.014211810193955898, -0.010663865134119987, -0.0465436726808548, 0.008823499083518982, 0.020171253010630608, 0.012029902078211308, 0.007141257636249065, 0.004886054899543524, -0.026562532410025597, -0.007124160882085562, 0.06844864040613174, -0.04894966632127762, 0.03900295868515968, 0.013416549190878868, 0.020304054021835327, 0.01027950830757618, 0.04066453501582146, -0.022768018767237663, -0.02858595922589302, 0.2903274893760681, -0.0462726354598999, -0.0013054432347416878, -0.012656528502702713, -0.0022524476516991854, 0.007995051331818104, -0.0132282804697752, 0.041610609740018845, -0.025341056287288666, 0.05012786015868187, -0.024150611832737923, 0.03174828737974167, 0.007918842136859894, 0.017964834347367287, -0.03303088992834091, 0.041218459606170654, -0.04849082976579666, -0.011187085881829262, 0.00881060678511858, 0.0006915374542586505, -0.020649056881666183, 0.017697758972644806, 0.05369609594345093, 0.032368894666433334, 0.008951826952397823, 0.017643045634031296, 0.0669298768043518, -0.03586482256650925, -0.05476013943552971, 0.026002751663327217, 0.002751411870121956, 0.031194476410746574, -0.036002252250909805, -0.010527163743972778, 0.04799577221274376, -0.029453564435243607, 0.054507527500391006, 0.03332153707742691, 0.04553559422492981, -0.08828436583280563, 0.06207291781902313, -0.010196568444371223, -0.020275656133890152, 0.04536880552768707, -0.01507195271551609, 0.02469932846724987, -0.02317338064312935, -0.031666241586208344, 0.011592142283916473, -0.00710091320797801, -0.06313911080360413, -0.010424342006444931, 0.0287169199436903, -0.018587123602628708, 0.006694323383271694, -0.006523863412439823, 0.021763114258646965, -0.020416894927620888, -0.01979893259704113, -0.026730822399258614, -0.047072723507881165, 0.00380211160518229, -0.019875725731253624, -0.0003153703291900456, -0.014025384560227394, 0.03269445523619652, -0.023359637707471848, -0.030691999942064285, -0.0020962548442184925, -0.0026406319811940193, 0.08316703885793686, 0.014814546331763268, 0.005487552843987942, -0.020131465047597885, -0.01061005238443613, -0.007263914681971073, 0.0013049655826762319, -0.031116755679249763, -0.04256385937333107, -0.027457883581519127, 0.04156522825360298, -0.009575605392456055, 0.00229713530279696, -0.01382637768983841, -0.038327161222696304, -0.008602291345596313, -0.02486877329647541, 0.006069318391382694, 0.05235271900892258, 0.10530094802379608, 0.026666343212127686, 0.00458866311237216, -0.04045254737138748, -0.028579749166965485, 0.06871189922094345, -0.02046281471848488, -0.001876223017461598, -0.04425490275025368, -0.014890431426465511, -0.0029310272075235844, -0.022486599162220955, 0.015236800536513329, 0.042436741292476654, -0.018604736775159836, 0.021533818915486336, 0.07161159813404083, 0.02392902784049511, -0.0036969820503145456, 0.010400664992630482, 0.01506020501255989, -0.023645013570785522, 0.06373855471611023, 0.01121669914573431, 0.03339463099837303, -0.037848155945539474, -0.0493331253528595, 0.0011706080986186862, 0.07609360665082932, 0.037609998136758804, -0.04923812672495842, -0.004525936674326658, -0.004933709278702736, 0.03444823622703552, 0.00961329322308302, 0.02709108404815197, -0.016534388065338135, -0.0012555071152746677, -0.012232036329805851, 0.01844908483326435, -0.03963945060968399, -0.008205465041100979, 0.07041793316602707, 0.026828644797205925, 0.08523716777563095, -0.035184118896722794, -0.07186337560415268, 0.019192928448319435, 0.006626591552048922, 0.02674834243953228, -0.03550465404987335, -0.0013757176930084825, -0.013057095929980278, -0.006259705871343613, -0.019954102113842964, -0.017480606213212013, -0.035805776715278625, -0.07545067369937897, 0.0010784771293401718, -0.009245356544852257, -0.00842310581356287, 0.015396470203995705, 0.014308827929198742, 0.05178559198975563, 0.02343576028943062, 0.032154206186532974, -0.04819147288799286, 0.01721012219786644, 0.0037287166342139244, -0.0037865187041461468, 0.07040517777204514, 0.041762661188840866, -0.004375574178993702, -0.0370553657412529, -0.010047716088593006, 0.002184521406888962, -0.045305557548999786, 0.03750501945614815, 0.0154246361926198, -0.017921101301908493, 0.012508747167885303, 0.058768611401319504, 0.03552329167723656, -0.04261472076177597, -0.009637342765927315, 0.033395882695913315, -0.0014275639550760388, -0.025642234832048416, 0.028270576149225235, -0.004541628994047642, -0.011377033777534962, 0.008690617978572845, 0.006908002309501171, 0.05967476963996887, 0.010946772992610931, 0.006710404064506292, -0.03121928870677948, -0.020138774067163467, 0.061751462519168854, -0.007941380143165588, 0.008025339804589748, 0.004464767873287201, 0.06734083592891693, -0.028032980859279633, -0.010242464020848274, 0.05282130464911461, -0.032047089189291, -0.0353754498064518, -0.014706799760460854, -0.11296147853136063, -0.010144492611289024, 0.027073830366134644, 0.016154924407601357, -0.013454802334308624, 0.0336320735514164, 0.028015462681651115, -0.013878860510885715, 0.012424830347299576, 0.08161015063524246, -0.011350464075803757, -0.027807092294096947, 0.029775692149996758, -0.030016982927918434, -0.010592696256935596, 0.006519858725368977, -0.07447448372840881, -0.021925654262304306, -0.06686759740114212, -0.04216966778039932, -0.0005988499615341425, -0.01145012117922306, 0.0008474718779325485, 0.04536425322294235, 0.029476167634129524, -0.0019819471053779125, 0.05755879357457161, -0.009074335917830467, -0.022215671837329865, 0.032723382115364075, -0.0954827219247818, 0.01872066967189312, 0.019006965681910515, -0.004253257066011429, 0.05760909989476204, 0.020731136202812195, 0.002525811782106757, 0.057187214493751526, -0.026298828423023224, 0.006230837665498257, -0.027714647352695465, 0.0036878418177366257, -0.01724347099661827, 3.8599580875597894e-05, -0.0303869117051363, 0.2855892479419708, -0.03764152154326439, 0.03420493006706238, 0.021107131615281105, 0.04685351997613907, 0.07355134934186935, 0.0013865828514099121, 0.012010115198791027, 0.0319553017616272, 0.05976700037717819, -0.05668102577328682, 0.05663221701979637, -0.0004787688667420298, -0.00469347694888711, -0.023974580690264702, -0.015757665038108826, -0.018921222537755966, -0.0173412524163723, 0.016906380653381348, 0.0007320303120650351, 0.008984027430415154, -0.005052247084677219, -0.0011377462651580572, 0.03809066116809845, -0.026172203943133354, -0.015519306063652039, 0.021830573678016663, 0.0009463250753469765, 0.019024522975087166, 0.019423076882958412, 0.024486307054758072, 0.02339295670390129, -0.050248753279447556, -0.044374387711286545, 0.012537877075374126, 0.013443547300994396, 0.0559082068502903, -0.030602291226387024, 0.020584164187312126, -0.00932166539132595, -0.0025743632577359676, -0.026682952418923378, -0.046443793922662735, 0.028590794652700424, 0.06584248691797256, -0.002805260708555579, 0.02955416589975357, -0.02671663835644722, 0.041632030159235, -0.023706255480647087, -0.002537545282393694, -0.008149689994752407, -0.029720157384872437, 0.013001975603401661, -0.04797010496258736, -0.05074260011315346, -0.041601166129112244, -0.007430829107761383, -0.08091475069522858, 0.01304304413497448, 0.009643273428082466, -0.013405214063823223, -0.03870811685919762, 0.008053089492022991, -0.03341091051697731, -0.01421231497079134, 0.003434749087318778, -0.037404175847768784, -0.015400168485939503, 0.005354022141546011, -0.029386285692453384, 0.007277105003595352, 0.0038720089942216873, 0.06268596649169922, 0.013866349123418331, -0.01115646492689848, -0.028667721897363663, -0.017093520611524582, -0.05920345336198807, -0.008064375258982182, 0.007644337136298418, -0.03003111109137535, -0.0015977276489138603, 0.004250688478350639, 0.04385216534137726, 0.04979259520769119, 0.019210247322916985, 0.01274983212351799, 0.03185632452368736, 0.06513326615095139, -0.0529320128262043, 0.004802306182682514, -0.02773776650428772, 0.0016707988688722253, -0.05727878957986832, 0.018315736204385757, 0.00012302244431339204, 0.003193365875631571, 0.00588600430637598, 0.009954954497516155, 0.011141163296997547, -0.038883794099092484, -0.007784455548971891, 0.0026855003088712692, 0.0013805220369249582, -0.01617177575826645, -0.01326706726104021, -0.05069638043642044, 0.0897059217095375, -0.008037635125219822, -0.004414402414113283, 0.028866207227110863, -0.02400718256831169, -0.034466538578271866, 0.032004982233047485, 0.028862448409199715, 0.0205606147646904, -0.03833058848977089, -0.09724867343902588, 0.01605743169784546, -0.013995824381709099, 0.024225972592830658, -0.020811114460229874, -0.028169382363557816, -0.013557685539126396, -0.06675275415182114, -0.011432331055402756, -0.01257979404181242, -0.024624841287732124, -0.008639811538159847, 0.008850687183439732, 0.02147934027016163, 0.030862286686897278, 0.04297048598527908, 0.05313681438565254, -0.018739739432930946, 0.01112778577953577, -0.04360495135188103, 0.0327250212430954, -0.03356212005019188, -0.018944380804896355, -0.014492855407297611, 0.011367184109985828, -0.04523707181215286, 0.010116523131728172, -0.019064927473664284, -0.030491618439555168, 0.0676056444644928, 0.0017275250284001231, -0.044237785041332245, 0.013750121928751469, 0.003838346805423498, 0.014947587624192238, -0.033635590225458145, -0.02889740653336048, 0.029042206704616547, -0.010533940978348255, 0.005631654988974333, 0.01063166931271553, -0.005678591318428516, -0.007379843853414059, -0.0011447330471128225, 0.015517170540988445, 0.0063005960546433926, -0.019697243347764015, 0.05561963841319084, 0.009244546294212341, 0.030723268166184425, 0.024123359471559525, -0.00783377606421709, -0.02382378838956356, -0.013706212863326073, 0.018419992178678513, -0.027906237170100212, -0.01857437938451767, 0.10234533995389938, -0.041349858045578, -0.028969034552574158, 0.041528262197971344, -0.0026099614333361387, 0.05140481889247894, 0.012238974682986736, 0.012046216987073421, 0.009900243021547794, -0.08221369981765747, -0.024152088910341263, -0.012136287055909634, -0.0002651376707945019, -0.012210410088300705, 0.008427044376730919, -0.05325644090771675, -0.013105345889925957, 0.01543496549129486, -0.05050123855471611, 0.003972529899328947, -0.04780174791812897, -0.0048656221479177475, 0.0021327140275388956, -0.02534995786845684, -0.015300208702683449, 0.006842153612524271, -0.04393138736486435, 0.0071528609842062, -0.04689158499240875, -0.007482354529201984, -0.029505953192710876, -0.011341646313667297, -0.01114143617451191, 0.024324841797351837, 0.018470093607902527, -0.06419682502746582, -0.027627339586615562, -0.0010270102648064494, -0.021481065079569817, 0.0020771666895598173, -0.01105172373354435, -0.03334484621882439, -0.016661014407873154, -0.012205914594233036, -0.013517185114324093, -0.013815752230584621, 0.020443476736545563, 0.010261058807373047, 0.05627866089344025, -0.002270180033519864, -0.02328762225806713, 0.002821225905790925, 0.028528612107038498, -0.04279111698269844, -0.012714484706521034, 0.00032957326038740575, 0.00016178627265617251, -0.016565117985010147, -0.014478062279522419, 0.044163696467876434, 0.0191593486815691, 0.013870607130229473, -0.027892423793673515, -0.023909272626042366, 0.023102592676877975, -0.016980737447738647, 0.04131530970335007, 0.005456562153995037, -0.019145138561725616, -0.006899573374539614, 0.01589188352227211, -0.018812308087944984, -0.024334320798516273, -0.02164360135793686, 0.001225002808496356, 0.01983315870165825, -0.018730707466602325, 0.01952357590198517, -0.0220309030264616, -0.03172415494918823, 0.01566765457391739, -0.029337210580706596, -0.03696690872311592, -0.028246140107512474, -0.013658243231475353, -0.054865144193172455, -0.028465183451771736, -0.08525232970714569, -0.04133901372551918, 0.05329463630914688, -0.06176319718360901, -0.013110999017953873, 0.007517424877732992, -0.025339804589748383, -0.007960840128362179, -0.005367609206587076, -0.03289986029267311, 0.006784206256270409, 0.0021183528006076813, -0.012227318249642849, 0.04580872878432274, 0.013818308711051941, -0.06782706826925278, -0.0007490686257369816, -0.023548435419797897, 0.02731456607580185, -0.03650723025202751, -0.03442404046654701, -0.017792927101254463, -0.04600750654935837, -0.023667162284255028, 0.04997465759515762, -0.000997773720882833, 0.01187132578343153, -0.0007765698828734457, -0.011236779391765594, 0.0010599088855087757, -0.007249257992953062, 0.039013106375932693, -0.004128153435885906, -0.06627251952886581, -0.005907135084271431, 0.05399549379944801, 0.05183367803692818, 0.008954285643994808, -0.06464537233114243, -0.005877338349819183, -0.014474829658865929, -0.013286315836012363, 0.03893038630485535, 0.03367570787668228, -0.057839229702949524, -0.10369441658258438, -0.005566870328038931, -0.019245870411396027, -0.028735561296343803, 0.020505938678979874, 0.006119918078184128, -0.011173563078045845, -0.061886176466941833, -0.0608532652258873, -0.021811634302139282, 0.1016368493437767, 0.014822252094745636, -0.014526119455695152, -0.010874579660594463, 0.012996272183954716, -0.031999338418245316, 0.001565528567880392, -0.052075497806072235, 0.008448204956948757, 0.06119263917207718, -0.061782121658325195, -0.011982802301645279, 0.05079256370663643, -0.00972034689038992, -0.02833724208176136, 0.04643901064991951, 0.0003836864198092371, 0.010342604480683804, 0.014022567309439182, -0.010509725660085678, -0.02998241037130356, 0.021288633346557617, -0.00038847140967845917, 0.03276641666889191, 0.024241343140602112, -0.060560669749975204, 0.015031510964035988, -0.006266540382057428, -0.02911841683089733, 0.04338182508945465, -0.00864468701183796, -0.01775439828634262, 0.03268161788582802, -0.026874789968132973, -0.006036336068063974, -0.029406718909740448, 0.007156677544116974, 0.012426245026290417, -0.057637546211481094, -0.006874642334878445, -0.01955454610288143, -0.011110717430710793, 0.045456450432538986, -0.05301104485988617, -0.03905965015292168, 0.0032802599016577005, -0.0395657904446125, -0.003810756141319871, 0.002504461444914341, -0.0027103691827505827, -0.024585630744695663, 0.031203972175717354, -0.01670146733522415, -0.006076865363866091, -0.04705144464969635, 0.01278582401573658, -0.020212668925523758, 0.03315870463848114, 0.023454565554857254, 0.05901984125375748, -0.004454992711544037, -0.03827062249183655, -0.021953267976641655, -0.010769466869533062, 0.0020022408571094275, 0.0065247174352407455, -0.017341816797852516, -0.11020703613758087, 0.041470564901828766, 0.0059120687656104565, -0.015110393986105919, -0.025987250730395317, -0.03882789984345436, -0.028704559430480003, -0.005904766730964184, 0.0011049892054870725, -0.0045592463575303555, -0.01355744805186987, 0.0008253087871707976, 0.00791118759661913, -0.00974145159125328, 0.004527044948190451, -0.010411165654659271, 0.026595978066325188, 0.11063005030155182, 0.008615859784185886, 0.04878207668662071, -0.01258198730647564, 0.0033875463996082544, 0.052443113178014755, 0.021218180656433105, 0.0056701116263866425, 0.04201272502541542, -0.023823125287890434, -0.039852410554885864, 0.04232129454612732, -0.021867893636226654, 0.0009162626811303198, -0.0019803850445896387, 0.028711216524243355, -0.05736853927373886, 0.007948407903313637, -0.020457671955227852, 0.016154203563928604, 0.004888986703008413, -0.014388922601938248, 0.09418614208698273, 0.05565164238214493, 0.05872873216867447, -0.038657914847135544, 0.0012766666477546096, -0.022048763930797577, -0.006267261691391468, 0.00675659766420722, 0.06184358149766922, -0.010862347669899464, 0.007284051738679409, 0.029177675023674965, -0.040940061211586, -0.024574577808380127, -0.023804673925042152, 0.01641079969704151, -0.06988300383090973, -0.07642167061567307, -0.023587241768836975, -0.03262002021074295, -0.020834721624851227, 0.042631685733795166, 0.07691100239753723, -0.022649144753813744, -0.001362578826956451, 0.022278986871242523, 0.06439238786697388, -0.022139469161629677, 0.023484868928790092, 0.0019441236509010196, 0.01809445396065712, -0.004815110005438328, 0.006968059577047825, -0.031331755220890045, -0.029851406812667847, 0.02252710796892643, 0.0004223136347718537, -0.007761799264699221, -0.04677806794643402, -0.03710007295012474, 0.0006855845567770302, -0.017883004620671272, -0.0028164535760879517, 0.03209121525287628, 0.05728769674897194, 0.024419449269771576, -0.004346068948507309, 0.0010268394835293293, 0.018984483554959297, -0.010842929594218731, -0.004465244710445404]\n"
     ]
    }
   ],
   "source": [
    "# 補充: 當然你也可以利用langchain_community或langchain_community 使用HuggineFace上的其他模型，例如：\n",
    "\n",
    "# %pip install llama-index-embeddings-huggingface -q\n",
    "# %pip install llama-index-embeddings-instructor -q\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# 使用HuggingFaceEmbedding上的模型\n",
    "model_from_hf = HuggingFaceEmbeddings(model_name=\"DMetaSoul/Dmeta-embedding-zh-small\")\n",
    "\n",
    "# 注意，HuggingFaceEmbeddings舊的獲取向量函數是get_text_embedding不是embed_query\n",
    "embedding = model_from_hf.embed_query(text)\n",
    "print(\"句子Embedding 後結果: \\n\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## ✏️ Extractors & Datapipeline\n",
    "- 有些時候，會希望轉換成向量之前，還需要將文句進行進一步的摘要、總結，我們稱之為 Extractors\n",
    "\n",
    "- 同時，我們會希望建立一個資料處理流程\n",
    "\n",
    "- 這個處理流程可能包含文檔切割(TokenTextSplitter)、標題擷取(TitleExtractor)、關聯的問題回答擷取(QuestionsAnsweredExtractor)，甚至更多\n",
    "\n",
    "- 我們可以把這些流程串聯起來(以下叫做transformations)\n",
    "\n",
    "- 甚至流程可以整合transformations, 存到向量資料庫裡...，建立完整的IngestionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 從sample_pdfs讀取所有相關pdf建立成documents\"\"\"\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./datasets/sample_pdfs\").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PipiHi\\AppData\\Local\\Temp\\ipykernel_45076\\2061471635.py:20: RuntimeWarning: coroutine 'Dispatcher.span.<locals>.async_wrapper' was never awaited\n",
      "  result = extractor.aextract(documents)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "from llama_index.core.extractors import metadata_extractors\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "nest_asyncio.apply()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# 建立一個LLM model 一樣 by Groq\n",
    "groq_llm = Groq(model=\"llama-3.1-70b-versatile\", api_key=api_key)\n",
    "\n",
    "# 使用metadata_extractors上的函數\n",
    "extractor = metadata_extractors.SummaryExtractor(\n",
    "                    llm=groq_llm, \n",
    "                    summaries=['self'], # 預設是['self']\n",
    "                    # prompt_template=\"\" # 可不填入，使用預設的prompt，也可自定義\n",
    "                    )\n",
    "\n",
    "result = extractor.aextract(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.82it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.64it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.79it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.80it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.73it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.97it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.68it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.00it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.69it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.80it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.42it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.94it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.26it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.20it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.61it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.87it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.89it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.96it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.81it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.03it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.09it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.78it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.71it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.64s/it]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.79it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.79s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.50it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.78it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.69it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.61it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.64it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.80it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.78it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.64it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.80it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.27it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.99it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00,  7.74it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.21it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  4.00it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.23it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.77it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.90it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.17it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.13it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.55it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.54it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.23it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.97it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.51it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.26it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.70it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.45it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.89it/s]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.44it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.26it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.35it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.36it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.95it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.78it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.79it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.12it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.30it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.29it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.16it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.25it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.21it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.32it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.96it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.01s/it]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.40it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.40it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.23it/s]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.10s/it]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.90it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.72it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.42it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.95it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.12it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.07it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.34it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.61it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.19it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.30it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.78it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.95it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.57it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.13it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.15it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.44it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.16it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.95it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.14it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.96it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.41it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.35it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.19it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.44it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.40it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.45it/s]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.08s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.12it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.33it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.30it/s]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.02s/it]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.33it/s]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.05s/it]\n",
      "  3%|▎         | 14/550 [00:11<07:14,  1.23it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6571274248335562 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 493ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      "  4%|▍         | 23/550 [00:18<06:53,  1.28it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.39046811436586293 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 514ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 13%|█▎        | 74/550 [00:56<05:39,  1.40it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.34105290989391024 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 594ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 17%|█▋        | 94/550 [01:12<06:07,  1.24it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.3802685899966364 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 305ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 17%|█▋        | 96/550 [01:15<07:42,  1.02s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8847213240250925 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 353ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 27%|██▋       | 148/550 [01:55<06:13,  1.08it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7150637699386843 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 500ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 28%|██▊       | 153/550 [01:58<05:14,  1.26it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.10402824882121564 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 407.999999ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 31%|███       | 171/550 [02:12<04:55,  1.28it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.1076244486890322 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 154.999999ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 39%|███▊      | 213/550 [02:45<04:33,  1.23it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.491545137480228 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 325ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 39%|███▉      | 216/550 [02:48<05:26,  1.02it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.43971622419931367 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 544.999999ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 40%|████      | 222/550 [02:52<03:34,  1.53it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7798300523422081 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 358ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 41%|████      | 225/550 [02:55<04:06,  1.32it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.47577537478227394 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 511ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 41%|████▏     | 228/550 [02:57<04:19,  1.24it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.005559965276144552 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 507ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 47%|████▋     | 258/550 [03:21<03:57,  1.23it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8497393365049964 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 335ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 53%|█████▎    | 289/550 [03:44<02:30,  1.74it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.11625974198364908 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 465.999999ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 57%|█████▋    | 313/550 [04:04<03:24,  1.16it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.14765047159012434 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 88.999999ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 61%|██████▏   | 338/550 [04:24<02:24,  1.47it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8497082685500111 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 372ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 84%|████████▍ | 461/550 [06:12<01:09,  1.29it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.39276193750152577 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 509ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 87%|████████▋ | 480/550 [06:27<00:48,  1.43it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7303872089932204 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 62.999999ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 88%|████████▊ | 482/550 [06:29<00:56,  1.21it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.11557442677128726 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 81ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 89%|████████▊ | 487/550 [06:33<00:48,  1.31it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.4521559370261502 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 272ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 90%|█████████ | 496/550 [06:40<00:46,  1.17it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8543227644605673 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 303ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 93%|█████████▎| 509/550 [06:50<00:25,  1.62it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.09280404980652979 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 3ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 94%|█████████▍| 517/550 [06:56<00:23,  1.39it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.15226396945682108 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 396.999999ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 95%|█████████▌| 523/550 [07:01<00:19,  1.37it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6818865256914454 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 19.999999ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      " 97%|█████████▋| 536/550 [07:12<00:12,  1.15it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7945431445696937 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j4vjgyssemfswan46515bc62` on requests per minute (RPM): Limit 100, Used 100, Requested 1. Please try again in 206.999999ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}.\n",
      "100%|██████████| 550/550 [07:24<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    BaseExtractor,\n",
    ")\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# 建立一個TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=512, chunk_overlap=128\n",
    ")\n",
    "\n",
    "\n",
    "# 建立一個LLM model 一樣 by Groq\n",
    "groq_llm = Groq(model=\"llama-3.1-70b-versatile\", api_key=api_key)\n",
    "\n",
    "# 官網範例，也能自定義一個自己的extractor\n",
    "class CustomExtractor(BaseExtractor):\n",
    "    def extract(self, nodes):\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"custom\": (\n",
    "                    node.metadata[\"document_title\"]\n",
    "                    + \"\\n\"\n",
    "                    + node.metadata[\"excerpt_keywords\"]\n",
    "                )\n",
    "            }\n",
    "            for node in nodes\n",
    "        ]\n",
    "        return metadata_list\n",
    "\n",
    "\n",
    "# 以下為重點，我們建立一個 transformations:List ，裡面包含了所有的文字處理流程\n",
    "# 這邊我們使用了TokenTextSplitter, TitleExtractor, QuestionsAnsweredExtractor\n",
    "transformations = [\n",
    "    TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128),\n",
    "    TitleExtractor(nodes=5, llm=groq_llm),\n",
    "    QuestionsAnsweredExtractor(questions=3, llm=groq_llm),\n",
    "    # EntityExtractor(prediction_threshold=0.5),\n",
    "    # SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "    # KeywordExtractor(keywords=10, llm=llm),\n",
    "    # CustomExtractor()\n",
    "]\n",
    "\n",
    "\n",
    "# 利用IngestionPipeline來執行所有的剛剛定義transformations\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "\n",
    "# 這邊我們使用剛剛定義的documents，並且執行pipeline\n",
    "nodes = pipeline.run(documents=documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '2', 'file_name': 'Deep-Learning-with-PyTorch.pdf'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看執行完pipeline後其中一筆資訊範例\n",
    "nodes[1].metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM sees:\n",
      " [Excerpt from document]\n",
      "page_label: 9\n",
      "Excerpt:\n",
      "-----\n",
      "in the years since the library’s release, it has grown into one of the most prominent deep learning tools for a broad range of applications. PyTorch provides a core data structure, the Tensor, a multidimensional array that has many similarities with NumPy arrays. From that foundation, a laundry list of fea-tures was built to make it easy to get a project up and running, or to design and train investigation into a new neural network architecture. Tensors accelerate mathematical operations (assuming that the appropriate combination of hardware and software is present), and PyTorch has packages for distributed training, worker processes for effi-cient data loading, and an extensive library of common deep learning functions. As Python is for programming, PyTorch is both an excellent introduction to deep learning and a tool usable in professional contexts for real-world, high-level work. W e  b e l i e v e  t h a t  P y T o r c h  s h o u l d  b e  t h e  f i r s t  d e e p  l e a r n i n g  l i b r a r y  y o u  l e a r n . Whether it should be the last is a decision that we’ll leave to you.1.2 What is this book?This book is intended to be a starting point for software engineers, data scientists, and motivated students who are fluent in Python and want to become comfortable using PyTorch to build deep learning projects. To that end, we take a hands-on approach; we encourage you to keep your computer at the ready so that you can play with the examples and take them a step further.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "\n",
    "#\n",
    "for node in nodes:\n",
    "    node.metadata = {\n",
    "        k: node.metadata[k]\n",
    "        for k in node.metadata\n",
    "        if k in [\"page_label\", \"file_name\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# 取一筆資料範例展示\n",
    "print(\n",
    "    \"LLM sees:\\n\",\n",
    "    (nodes)[12].get_content(metadata_mode=MetadataMode.LLM),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## ✏️ IngestionPipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
