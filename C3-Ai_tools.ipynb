{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:small; color:gray;\"> Author: 鄭永誠, Year: 2024 </p>\n",
    "\n",
    "# C3 - 基於LLM發展出來的的AI工具\n",
    "----------\n",
    "\n",
    "若你的開發系統想要結合更多資訊，如找新聞、爬指定網頁等，可多留意現在也有很多AI整合的工具，  \n",
    "很多是別人寫好的可以參考，我這邊就只是隨便拿幾個別人寫好的自己拿來使用範例\n",
    "\n",
    "主要是要強調，善用資源的重要性!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## # databonsai \n",
    "https://github.com/databonsai/databonsai  \n",
    "\n",
    "使用 LLM 執行資料清理任務的 Python 函式庫\n",
    "\n",
    "注意: 這裡的官方實作需要使用openAI，所以我自己定義了一個新的使用groq的程式函示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\PipiHi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Install dependencies \"\"\"\n",
    "%pip install databonsai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 我自定義了一個groq_provider，用來封裝Groq的API，並且支援重試機制。 \"\"\"\n",
    "import os\n",
    "from typing import Optional\n",
    "from groq import Groq\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from functools import wraps\n",
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from databonsai.llm_providers import LLMProvider\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# class LLMProvider(ABC):\n",
    "#     @abstractmethod\n",
    "#     def generate(self, system_prompt: str, user_prompt: str, max_tokens: int = 1000) -> str:\n",
    "#         pass\n",
    "\n",
    "class GroqProvider(LLMProvider):\n",
    "    \"\"\"\n",
    "    A provider class to interact with Groq's API.\n",
    "    Supports exponential backoff retries for handling large datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        multiplier: int = 1,\n",
    "        min_wait: int = 1,\n",
    "        max_wait: int = 30,\n",
    "        max_tries: int = 5,\n",
    "        model: str = \"llama-3.1-70b-versatile\",\n",
    "        temperature: float = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the GroqProvider with an API key and retry parameters.\n",
    "\n",
    "        Parameters:\n",
    "        api_key (str): Groq API key.\n",
    "        multiplier (int): The multiplier for the exponential backoff in retries.\n",
    "        min_wait (int): The minimum wait time between retries.\n",
    "        max_wait (int): The maximum wait time between retries.\n",
    "        max_tries (int): The maximum number of attempts before giving up.\n",
    "        model (str): The default model to use for text generation.\n",
    "        temperature (float): The temperature parameter for text generation.\n",
    "        \"\"\"\n",
    "        # Provider related configs\n",
    "        if api_key:\n",
    "            self.api_key = api_key\n",
    "        else:\n",
    "            self.api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "            if not self.api_key:\n",
    "                raise ValueError(\"Groq API key not provided.\")\n",
    "        self.model = model\n",
    "        self.client = Groq(api_key=self.api_key)\n",
    "        self.temperature = temperature\n",
    "        self.input_tokens = 0\n",
    "        self.output_tokens = 0\n",
    "\n",
    "        # Retry related configs\n",
    "        self.multiplier = multiplier\n",
    "        self.min_wait = min_wait\n",
    "        self.max_wait = max_wait\n",
    "        self.max_tries = max_tries\n",
    "\n",
    "    def retry_with_exponential_backoff(method):\n",
    "        \"\"\"\n",
    "        Decorator to apply retry logic with exponential backoff to an instance method.\n",
    "        \"\"\"\n",
    "        @wraps(method)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            retry_decorator = retry(\n",
    "                wait=wait_exponential(\n",
    "                    multiplier=self.multiplier, min=self.min_wait, max=self.max_wait\n",
    "                ),\n",
    "                stop=stop_after_attempt(self.max_tries),\n",
    "            )\n",
    "            return retry_decorator(method)(self, *args, **kwargs)\n",
    "        return wrapper\n",
    "\n",
    "    @retry_with_exponential_backoff\n",
    "    def generate(\n",
    "        self, system_prompt: str, user_prompt: str, max_tokens: int = 1000\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generates a text completion using Groq's API, with a given system and user prompt.\n",
    "        This method is decorated with retry logic to handle temporary failures.\n",
    "\n",
    "        Parameters:\n",
    "        system_prompt (str): The system prompt to provide context or instructions for the generation.\n",
    "        user_prompt (str): The user's prompt, based on which the text completion is generated.\n",
    "        max_tokens (int): The maximum number of tokens to generate in the response.\n",
    "\n",
    "        Returns:\n",
    "        str: The generated text completion.\n",
    "        \"\"\"\n",
    "        if not system_prompt:\n",
    "            raise ValueError(\"System prompt is required.\")\n",
    "        if not user_prompt:\n",
    "            raise ValueError(\"User prompt is required.\")\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            # Groq API doesn't provide token usage information, so we can't update input_tokens and output_tokens\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error occurred during generation: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" databonsai 資料分類操作範例，建立分類器 \"\"\"\n",
    "from databonsai.categorize import MultiCategorizer, BaseCategorizer\n",
    "\n",
    "# 這兩個是官方寫好的，但此處範例我用自己寫的\n",
    "from databonsai.llm_providers import OpenAIProvider, AnthropicProvider\n",
    "\n",
    "# 使用自己定義的GroqProvider\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "provider = GroqProvider(api_key=api_key)\n",
    "\n",
    "#  定義類型\n",
    "categories = {\n",
    "    \"傲嬌\": \"傲嬌指的是表面上高傲，但內心其實溫柔體貼，經常口是心非的一種性格。\",\n",
    "    \"高冷\": \"那些性格冷淡、難以接近，且在情感上表現得理智克制、不輕易顯露情感的女性角色。\",\n",
    "    \"小惡魔\": \"表面上可愛甜美，卻喜歡調皮捉弄他人，以玩弄別人的感情或挑逗行為來展現魅力的女性角色\",\n",
    "    \"元氣\": \"充滿活力、樂觀開朗，經常是團隊中的精神支柱，性格開朗外向。\",\n",
    "    \"天然呆\": \"天真無邪，有些迷糊，性格溫柔可愛，對周圍的一切都沒有戒心。\",\n",
    "    \"大小姐\": \"出身高貴、優雅端莊，很有品味和教養，不太了解世俗價值觀\",\n",
    "}\n",
    "\n",
    "# 提供範例資料\n",
    "few_shot_examples = [\n",
    "    {\"example\": \"哼，我才不關心你呢，只是剛好路過而已！\", \"response\": \"傲嬌\"},\n",
    "    {\"example\": \"請不要隨便靠近我，我不喜歡被打擾。\", \"response\": \"高冷\"},\n",
    "    {\"example\": \"嘿嘿，不知道這次你能不能猜到我在想什麼呢？\", \"response\": \"小惡魔\"},\n",
    "    {\"example\": \"早安！今天也要充滿幹勁地加油喔！\", \"response\": \"元氣\"},\n",
    "    {\"example\": \"咦？剛才你說什麼？抱歉我沒聽清楚。\", \"response\": \"天然呆\"},\n",
    "    {\"example\": \"哼，這種小事根本不值得我出手，不過就勉為其難幫你吧。\", \"response\": \"大小姐\"}\n",
    "]\n",
    "\n",
    "# 將資料進行分類\n",
    "categorizer = BaseCategorizer(\n",
    "    categories=categories,\n",
    "    llm_provider=provider,\n",
    "    examples = few_shot_examples,\n",
    "    #strict = False # Default true, set to False to allow for categories not in the provided dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這句話應該是出自: 傲嬌\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 範例問題展示 \"\"\"\n",
    "category = categorizer.categorize(\"這樣的問題你也敢問我？真是沒眼光\")\n",
    "print(\"這句話應該是出自:\" , category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## # ScrapeGraphAI\n",
    "\n",
    "透過輸入問題/指令，自己撈取網頁下對應的需求資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\PipiHi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\~andas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\PipiHi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "milvus-model 0.2.0 requires protobuf==3.20.0, but you have protobuf 4.25.4 which is incompatible.\n",
      "pymilvus 2.4.1 requires grpcio<=1.60.0,>=1.49.1, but you have grpcio 1.65.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\PipiHi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scrapegraphai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 透過ScrapeGraphAI的SmartScraperGraph來擷取網頁資料 \"\"\"\n",
    "\n",
    "import json\n",
    "from scrapegraphai.graphs import SmartScraperMultiGraph\n",
    "\n",
    "\n",
    "# 我這邊搭配使用groq提供的免費LLM api\n",
    "groq_key = \"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\"\n",
    "graph_config = {\n",
    "    \"llm\": {\"model\": \"groq/gemma-7b-it\", \"api_key\": groq_key, \"temperature\": 0.5},\n",
    "    \"verbose\": True,\n",
    "    \"headless\": False,\n",
    "    \"embeddings\": {\"model\": \"ollama/nomic-embed-text\"},\n",
    "}\n",
    "\n",
    "# 官網範例\n",
    "multiple_search_graph = SmartScraperMultiGraph(\n",
    "    prompt=\"簡禎富擔任過哪些職務?\",\n",
    "    source=[\n",
    "        f\"https://sites.google.com/gapp.nthu.edu.tw/2023/%E7%B0%A1%E7%A6%8E%E5%AF%8C%E5%89%AF%E6%A0%A1%E9%95%B7/\",\n",
    "    ],\n",
    "    schema=None,\n",
    "    config=graph_config,\n",
    ")\n",
    "\n",
    "result = multiple_search_graph.run()\n",
    "# print(result)\n",
    "print(json.dumps(result, indent=4, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
